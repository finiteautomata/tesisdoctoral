
%%
%% Deep Learning
%%
%%

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}

@inproceedings{glorot2011domain,
  title={Domain adaptation for large-scale sentiment classification: A deep learning approach},
  author={Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  booktitle={ICML},
  year={2011}
}

@article{rosenblatt1958perceptron,
  title={The perceptron: a probabilistic model for information storage and organization in the brain.},
  author={Rosenblatt, Frank},
  journal={Psychological review},
  volume={65},
  number={6},
  pages={386},
  year={1958},
  publisher={American Psychological Association}
}

@article{minsky1969perceptrons,
  title={Perceptrons.},
  author={Minsky, Marvin and Papert, Seymour},
  year={1969},
  publisher={MIT Press}
}

@article{elman1990finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}
%%
%%
%% Intro
%%

@article{conte2012manifesto,
  title={Manifesto of computational social science},
  author={Conte, Rosaria and Gilbert, Nigel and Bonelli, Giulia and Cioffi-Revilla, Claudio and Deffuant, Guillaume and Kertesz, Janos and Loreto, Vittorio and Moat, Suzy and Nadal, J-P and Sanchez, Anxo and others},
  journal={The European Physical Journal Special Topics},
  volume={214},
  number={1},
  pages={325--346},
  year={2012},
  publisher={Springer}
}

@book{hovy2020text,
  title={Text Analysis in Python for Social Scientists: Discovery and Exploration},
  author={Hovy, Dirk},
  year={2020},
  publisher={Cambridge University Press}
}

@article{edelmann2020computational,
  title={Computational social science and sociology},
  author={Edelmann, Achim and Wolff, Tom and Montagne, Danielle and Bail, Christopher A},
  journal={Annual Review of Sociology},
  volume={46},
  pages={61--81},
  year={2020},
  publisher={Annual Reviews}
}

%%%
%%%
%%%
@inproceedings{arora17,
	author = {Sanjeev Arora and Yingyu Liang and Tengyu Ma},
	title = {A Simple but Tough-to-Beat Baseline for Sentence Embeddings},
	booktitle = {International Conference on Learning Representations},
	year = {2017}
}



@inproceedings{bird2004nltk,
  title={NLTK: the natural language toolkit},
  author={Bird, Steven and Loper, Edward},
  booktitle={Proceedings of the ACL 2004 on Interactive poster and demonstration sessions},
  pages={31},
  year={2004},
  organization={Association for Computational Linguistics}
}


@article{bojanowski16,
  title={Enriching word vectors with subword information},
  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1607.04606},
  year={2016}
}


@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inproceedings{cumbreras2016overview,
  title={Overview of TASS 2016.},
  author={Cumbreras, Miguel {\'A}ngel Garc{\'\i}a and Villena-Rom{\'a}n, Julio and C{\'a}mara, Eugenio Mart{\'\i}nez and D{\'\i}az-Galiano, Manuel Carlos and Mart{\'\i}n-Valdivia, Maria Teresa and L{\'o}pez, Luis Alfonso Urena},
  booktitle={TASS@ SEPLN},
  pages={13--21},
  year={2016}
}
@misc { 10045_81394,
   title = {Overview of TASS 2018: Opinions, Health and Emotions},
   author = {Martínez Cámara, Eugenio AND Almeida-Cruz, Yudivian AND Díaz Galiano, Manuel Carlos AND Estévez-Velarde, Suilan AND García Cumbreras, Miguel Ángel AND García Vega, Manuel AND Gutiérrez, Yoan AND Montejo Ráez, Arturo AND Montoyo, Andres AND Muñoz, Rafael AND Piad-Morffis, Alejandro AND Villena Román, Julio},
   year = {2018-09}
}


@inproceedings{overview_tass2018,
	author		= "Mart\'{i}nez-C\'{a}mara, Eugenio and Almeida Cruz, Yudivi\'{a}n and D\'{i}az-Galiano, Manuel C. and Est\'{e}vez Velarde, Suilan and Garc\'{i}a-Cumbreras, Miguel \'{A}. and Garc\'{i}a-Vega, Manuel and Guti\'{e}rrez V\'{a}zquez, Yoan and Montejo R\'{a}ez, Arturo and Montoyo Guijarro, Andr\'{e} and Mu\~{n}oz Guillena, Rafael and Piad Morffis, Alejandro and Villena-Rom\'{a}n, Julio",
	title		= "Overview of {TASS} 2018: Opinions, Health and Emotions",
	booktitle	= "Proceedings of {TASS} 2018: Workshop on Semantic Analysis at SEPLN ({TASS} 2018)",
	editor		= "Mart\'{i}nez-C\'{a}mara, Eugenio and Almeida Cruz, Yudivi\'{a}n and D\'{i}az-Galiano, Manuel C. and Est\'{e}vez Velarde, Suilan and Garc\'{i}a-Cumbreras, Miguel \'{A}. and Garc\'{i}a-Vega, Manuel and Guti\'{e}rrez V\'{a}zquez, Yoan and Montejo R\'{a}ez, Arturo and Montoyo Guijarro, Andr\'{e} and Mu\~{n}oz Guillena, Rafael and Piad Morffis, Alejandro and Villena-Rom\'{a}n, Julio",
	volume		= "2172",
	series		= "CEUR Workshop Proceedings",
	address	= "Sevilla, Spain",
	publisher	= "CEUR-WS",
	year		= "2018",
	month		= "September",
}

@inproceedings{atalaya_tass2018,
  author    = {Franco M. Luque and
               Juan Manuel P{\'{e}}rez},
  title     = {Atalaya at {TASS} 2018: Sentiment Analysis with Tweet Embeddings and
               Data Augmentation},
  booktitle = {Proceedings of {TASS} 2018: Workshop on Semantic Analysis at SEPLN,
               TASS@SEPLN 2018, co-located with 34nd {SEPLN} Conference {(SEPLN}
               2018), Sevilla, Spain, September 18th, 2018.},
  pages     = {29--35},
  year      = {2018},
  url       = {http://ceur-ws.org/Vol-2172/p1\_atalaya\_tass2018.pdf},
  timestamp = {Wed, 19 Sep 2018 18:11:15 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/sepln/LuqueP18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{garcia2020overview,
  title={Overview of TASS 2020: introducing emotion detection},
  author={Garc{\'\i}a-Vegaa, Manuel and D{\'\i}az-Galianoa, Manuel Carlos and Garc{\'\i}a-Cumbrerasa, Miguel {\'A} and del Arcoa, Flor Miriam Plaza and Montejo-R{\'a}eza, Arturo and Jim{\'e}nez-Zafraa, Salud Mar{\'\i}a and C{\'a}marab, Eugenio Mart{\'\i}nez and Aguilarc, C{\'e}sar Antonio and Antonio, Marco and Cabezudod, Sobrevilla and others},
  year={2020}
}

%%
%%
%% Surveys
%%
%%
%%

@inproceedings{schmidt2017survey,
  title={A survey on hate speech detection using natural language processing},
  author={Schmidt, Anna and Wiegand, Michael},
  booktitle={Proceedings of the fifth international workshop on natural language processing for social media},
  pages={1--10},
  year={2017}
}

@article{fortuna2018survey,
  title={A survey on automatic detection of hate speech in text},
  author={Fortuna, Paula and Nunes, S{\'e}rgio},
  journal={ACM Computing Surveys (CSUR)},
  volume={51},
  number={4},
  pages={1--30},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@article{poletto2021resources,
  title={Resources and benchmark corpora for hate speech detection: a systematic review},
  author={Poletto, Fabio and Basile, Valerio and Sanguinetti, Manuela and Bosco, Cristina and Patti, Viviana},
  journal={Language Resources and Evaluation},
  volume={55},
  number={2},
  pages={477--523},
  year={2021},
  publisher={Springer}
}

%%
%%
%% ELMo
%%
%%
%%

@InProceedings{che-EtAl:2018:K18-2,
  author    = {Che, Wanxiang  and  Liu, Yijia  and  Wang, Yuxuan  and  Zheng, Bo  and  Liu, Ting},
  title     = {Towards Better {UD} Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation},
  booktitle = {Proceedings of the {CoNLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies},
  month     = {October},
  year      = {2018},
  address   = {Brussels, Belgium},
  publisher = {Association for Computational Linguistics},
  pages     = {55--64},
  url       = {http://www.aclweb.org/anthology/K18-2005}
}

@article{peters2018,
  author    = {Matthew E. Peters and
               Mark Neumann and
               Mohit Iyyer and
               Matt Gardner and
               Christopher Clark and
               Kenton Lee and
               Luke Zettlemoyer},
  title     = {Deep contextualized word representations},
  journal   = {CoRR},
  volume    = {abs/1802.05365},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.05365},
  archivePrefix = {arXiv},
  eprint    = {1802.05365},
  timestamp = {Mon, 13 Aug 2018 16:48:54 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1802-05365},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{cawley2010over,
  title={On over-fitting in model selection and subsequent selection bias in performance evaluation},
  author={Cawley, Gavin C and Talbot, Nicola LC},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={Jul},
  pages={2079--2107},
  year={2010}
}

@inproceedings{warner2012detecting,
  title={Detecting hate speech on the world wide web},
  author={Warner, William and Hirschberg, Julia},
  booktitle={Proceedings of the Second Workshop on Language in Social Media},
  pages={19--26},
  year={2012},
  organization={Association for Computational Linguistics}
}


@book{levy2001cyberculture,
  title={Cyberculture},
  author={L{\'e}vy, Pierre},
  volume={4},
  year={2001},
  publisher={U of Minnesota Press}
}

@book{rheingold1993virtual,
  title={The virtual community: Finding commection in a computerized world},
  author={Rheingold, Howard},
  year={1993},
  publisher={Addison-Wesley Longman Publishing Co., Inc.}
}
@inproceedings{waseem2016hateful,
  title={Hateful symbols or hateful people? predictive features for hate speech detection on twitter},
  author={Waseem, Zeerak and Hovy, Dirk},
  booktitle={Proceedings of the NAACL student research workshop},
  pages={88--93},
  year={2016}
}

 @inproceedings{waseem-2016-racist,
    title = "Are You a Racist or Am {I} Seeing Things? Annotator Influence on Hate Speech Detection on {T}witter",
    author = "Waseem, Zeerak",
    booktitle = "Proceedings of the First Workshop on {NLP} and Computational Social Science",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-5618",
    doi = "10.18653/v1/W16-5618",
    pages = "138--142",
}

%%
%%
%% Twitter
%%
%%

@article{pfeffer2018tampering,
  title={Tampering with Twitter’s sample API},
  author={Pfeffer, J{\"u}rgen and Mayer, Katja and Morstatter, Fred},
  journal={EPJ Data Science},
  volume={7},
  number={1},
  pages={50},
  year={2018},
  publisher={Springer Berlin Heidelberg}
}

%%
%%
%% Hate speech + techniques
%%
%%


@inproceedings{badjatiya2017deep,
  title={Deep learning for hate speech detection in tweets},
  author={Badjatiya, Pinkesh and Gupta, Shashank and Gupta, Manish and Varma, Vasudeva},
  booktitle={Proceedings of the 26th International Conference on World Wide Web Companion},
  pages={759--760},
  year={2017},
  organization={International World Wide Web Conferences Steering Committee}
}

@inproceedings{greevy2004classifying,
  title={Classifying racist texts using a support vector machine},
  author={Greevy, Edel and Smeaton, Alan F},
  booktitle={Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={468--469},
  year={2004},
  organization={ACM}
}

@inproceedings{zhang2018detecting,
  title={Detecting hate speech on Twitter using a convolution-GRU based deep neural network},
  author={Zhang, Ziqi and Robinson, David and Tepper, Jonathan},
  booktitle={European Semantic Web Conference},
  pages={745--760},
  year={2018},
  organization={Springer}
}

@article{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{kettrey2014staking,
  title={Staking territory in the “World White Web” an exploration of the roles of overt and color-blind racism in maintaining racial boundaries on a popular web site},
  author={Kettrey, Heather Hensman and Laster, Whitney Nicole},
  journal={Social Currents},
  volume={1},
  number={3},
  pages={257--274},
  year={2014},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{adams2005white,
  title={White supremacists, oppositional culture and the World Wide Web},
  author={Adams, Josh and Roscigno, Vincent J},
  journal={Social Forces},
  volume={84},
  number={2},
  pages={759--778},
  year={2005},
  publisher={The University of North Carolina Press}
}

@article{filipovic2007blogging,
  title={Blogging while female: How internet misogyny parallels real-world harassment},
  author={Filipovic, Jill},
  journal={Yale JL \& Feminism},
  volume={19},
  pages={295},
  year={2007},
  publisher={HeinOnline}
}

@article{mantilla2013gendertrolling,
  title={Gendertrolling: Misogyny adapts to new media},
  author={Mantilla, Karla},
  journal={Feminist Studies},
  volume={39},
  number={2},
  pages={563--570},
  year={2013},
  publisher={JSTOR}
}

@article{burnap2015cyber,
  title={Cyber hate speech on twitter: An application of machine classification and statistical modeling for policy and decision making},
  author={Burnap, Pete and Williams, Matthew L},
  journal={Policy \& Internet},
  volume={7},
  number={2},
  pages={223--242},
  year={2015},
  publisher={Wiley Online Library}
}

@article{thelwall2008social,
  title={Social networks, gender, and friending: An analysis of MySpace member profiles},
  author={Thelwall, Mike},
  journal={Journal of the American Society for Information Science and Technology},
  volume={59},
  number={8},
  pages={1321--1330},
  year={2008},
  publisher={Wiley Online Library}
}
@inproceedings{pak2010twitter,
  title={Twitter as a corpus for sentiment analysis and opinion mining.},
  author={Pak, Alexander and Paroubek, Patrick},
  booktitle={LREC},
  volume={10},
  number={2010},
  pages={1320--1326},
  year={2010}
}


@inproceedings{mccann2017learned,
  title={Learned in translation: Contextualized word vectors},
  author={McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6294--6305},
  year={2017}
}

@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}

@INPROCEEDINGS{schmid95,
    author = {Helmut Schmid},
    title = {Improvements In Part-of-Speech Tagging With an Application To German},
    booktitle = {In Proceedings of the ACL SIGDAT-Workshop},
    year = {1995},
    pages = {47--50}
}

@inproceedings{fersini2018overview,
  title={Overview of the task on automatic misogyny identification at ibereval},
  author={Fersini, Elisabetta and Anzovino, Maria and Rosso, Paolo},
  booktitle={Proceedings of the Third Workshop on Evaluation of Human Language Technologies for Iberian Languages (IberEval 2018), co-located with 34th Conference of the Spanish Society for Natural Language Processing (SEPLN 2018). CEUR Workshop Proceedings. CEUR-WS. org, Seville, Spain},
  year={2018}
}

@article{fersini2018evalitaoverview,
  title={Overview of the evalita 2018 task on automatic misogyny identification (ami)},
  author={Fersini, Elisabetta and Nozza, Debora and Rosso, Paolo},
  journal={Proceedings of the 6th evaluation campaign of Natural Language Processing and Speech tools for Italian (EVALITA’18), Turin, Italy. CEUR. org},
  year={2018}
}

@inproceedings{bosco2018overview,
  title={Overview of the EVALITA 2018 Hate Speech Detection Task},
  author={Bosco, Cristina and Felice, Dell'Orletta and Poletto, Fabio and Sanguinetti, Manuela and Maurizio, Tesconi},
  booktitle={EVALITA 2018-Sixth Evaluation Campaign of Natural Language Processing and Speech Tools for Italian},
  volume={2263},
  pages={1--9},
  year={2018},
  organization={CEUR}
}

@inproceedings{anzovino2018automatic,
  title={Automatic identification and classification of misogynistic language on twitter},
  author={Anzovino, Maria and Fersini, Elisabetta and Rosso, Paolo},
  booktitle={International Conference on Applications of Natural Language to Information Systems},
  pages={57--64},
  year={2018},
  organization={Springer}
}

@article{saleem2017web,
  title={A web of hate: Tackling hateful speech in online social spaces},
  author={Saleem, Haji Mohammad and Dillon, Kelly P and Benesch, Susan and Ruths, Derek},
  journal={arXiv preprint arXiv:1709.10159},
  year={2017}
}

@article{roesslein2020tweepy,
  title={Tweepy: Twitter for Python!},
  author={Roesslein, Joshua},
  journal={URL: https://github.com/tweepy/tweepy},
  year={2020}
}


@book{pustejovsky2012natural,
  title={Natural Language Annotation for Machine Learning: A guide to corpus-building for applications},
  author={Pustejovsky, James and Stubbs, Amber},
  year={2012},
  publisher={" O'Reilly Media, Inc."}
}

@article{blei2003latent,
  title={Latent dirichlet allocation},
  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal={the Journal of machine Learning research},
  volume={3},
  pages={993--1022},
  year={2003},
  publisher={JMLR. org}
}


%%
%%
%% Casos Hate Speech
%%
%%

% Análisis de antisemitismo en gab
@inproceedings{mcilroy2019welcome,
  title={From “welcome new gabbers” to the pittsburgh synagogue shooting: The evolution of gab},
  author={McIlroy-Young, Reid and Anderson, Ashton},
  booktitle={Proceedings of the international aaai conference on web and social media},
  volume={13},
  pages={651--654},
  year={2019}
}


@article{blout2020white,
  title={White Supremacist Terrorism in Charlottesville: Reconstructing ‘Unite the Right’},
  author={Blout, Emily and Burkart, Patrick},
  journal={Studies in Conflict \& Terrorism},
  pages={1--22},
  year={2020},
  publisher={Taylor \& Francis}
}

@article{klein2019twitter,
  title={From Twitter to Charlottesville: Analyzing the fighting words between the Alt-Right and Antifa},
  author={Klein, Adam},
  journal={International Journal of Communication},
  volume={13},
  pages={22},
  year={2019}
}

@article{kennedy2018gab,
  title={The Gab Hate Corpus: A collection of 27k posts annotated for hate speech},
  author={Kennedy, Brendan and Atari, Mohammad and Davani, Aida Mostafazadeh and Yeh, Leigh and Omrani, Ali and Kim, Yehsong and Coombs, Kris and Havaldar, Shreya and Portillo-Wightman, Gwenyth and Gonzalez, Elaine and others},
  year={2018},
  publisher={PsyArXiv}
}

%% Myanmar

@article{irrawaddy2018zuckerberg,
 author  = {Tin Htet Paing},
 date    = {2018-04-12},
 title   = {Zuckerberg Urged to Take Genuine Steps to Stop Use of FB to Spread Hate in Myanmar},
 journal = {The Irrawaddy},
 url     = {https://www.irrawaddy.com/news/burma/zuckerberg-urged-to-take-genuine-steps-to-stop-use-of-fb-to-spread-hate-in-myanmar.html},
}

@article{warofka2018independent,
  title={An independent assessment of the human rights impact of Facebook in Myanmar},
  author={Warofka, Alex},
  journal={Facebook Newsroom, November},
  volume={5},
  year={2018}
}
%%
%%
%% Causalidad
%%
%%
@book{pearl2018book,
  title={The book of why: the new science of cause and effect},
  author={Pearl, Judea and Mackenzie, Dana},
  year={2018},
  publisher={Basic books}
}

%%
%%
%%  BERT y amigos
%%
%%
%%

@inproceedings{howard-ruder-2018-universal,
    title = "Universal Language Model Fine-tuning for Text Classification",
    author = "Howard, Jeremy  and
      Ruder, Sebastian",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1031",
    doi = "10.18653/v1/P18-1031",
    pages = "328--339",
    abstract = "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24{\%} on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{canete2020spanish,
  title={Spanish pre-trained bert model and evaluation data},
  author={Canete, Jos{\'e} and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and P{\'e}rez, Jorge},
  journal={PML4DC at ICLR},
  year={2020}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@article{gonzalez2021twilbert,
  title={TWilBert: Pre-trained deep bidirectional transformers for Spanish Twitter},
  author={Gonzalez, Jose Angel and Hurtado, Llu{\'\i}s-F and Pla, Ferran},
  journal={Neurocomputing},
  volume={426},
  pages={58--69},
  year={2021},
  publisher={Elsevier}
}
@inproceedings{beltagy-etal-2019-scibert,
    title = "{S}ci{BERT}: A Pretrained Language Model for Scientific Text",
    author = "Beltagy, Iz  and
      Lo, Kyle  and
      Cohan, Arman",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1371",
    doi = "10.18653/v1/D19-1371",
    pages = "3615--3620",
    abstract = "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.",
}

@article{huang2019clinicalbert,
  title={Clinicalbert: Modeling clinical notes and predicting hospital readmission},
  author={Huang, Kexin and Altosaar, Jaan and Ranganath, Rajesh},
  journal={arXiv preprint arXiv:1904.05342},
  year={2019}
}

@inproceedings{dat2020bertweet,
title     = {{BERTweet: A pre-trained language model for English Tweets}},
author    = {Dat Quoc Nguyen and Thanh Vu and Anh Tuan Nguyen},
booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
pages     = {9--14},
year      = {2020}
}

@article{faldu2021ki,
  title={KI-BERT: Infusing Knowledge Context for Better Language and Domain Understanding},
  author={Faldu, Keyur and Sheth, Amit and Kikani, Prashant and Akabari, Hemang},
  journal={arXiv preprint arXiv:2104.08145},
  year={2021}
}
%%
%%
%% Domain adaptation
%%
@inproceedings{gururangan-etal-2020-dont,
    title = "Don{'}t Stop Pretraining: Adapt Language Models to Domains and Tasks",
    author = "Gururangan, Suchin  and
      Marasovi{\'c}, Ana  and
      Swayamdipta, Swabha  and
      Lo, Kyle  and
      Beltagy, Iz  and
      Downey, Doug  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.740",
    doi = "10.18653/v1/2020.acl-main.740",
    pages = "8342--8360",
    abstract = "Language models pretrained on text from a wide variety of sources form the foundation of today{'}s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task{'}s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.",
}

%%
%%
%% Datasets de hate speech, offensive language, etc
%%
%%



@inproceedings{hateval2019semeval,
  title={SemEval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter},
  author={Basile, Valerio and Bosco, Cristina and Fersini, Elisabetta and Nozza, Debora and Patti, Viviana and Rangel, Francisco and Rosso, Paolo and Sanguinetti, Manuela},
  booktitle={Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019)},
  year={2019},
  publisher= {Association for Computational Linguistics}
}

@misc{zampieri2019semeval2019,
      title={SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media (OffensEval)},
      author={Marcos Zampieri and Shervin Malmasi and Preslav Nakov and Sara Rosenthal and Noura Farra and Ritesh Kumar},
      year={2019},
      eprint={1903.08983},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{nobata2016abusive,
  title={Abusive language detection in online user content},
  author={Nobata, Chikashi and Tetreault, Joel and Thomas, Achint and Mehdad, Yashar and Chang, Yi},
  booktitle={Proceedings of the 25th international conference on world wide web},
  pages={145--153},
  year={2016}
}

@inproceedings{gertner-etal-2019-mitre,
    title = "{MITRE} at {S}em{E}val-2019 Task 5: Transfer Learning for Multilingual Hate Speech Detection",
    author = "Gertner, Abigail  and
      Henderson, John  and
      Merkhofer, Elizabeth  and
      Marsh, Amy  and
      Wellner, Ben  and
      Zarrella, Guido",
    booktitle = "Proceedings of the 13th International Workshop on Semantic Evaluation",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S19-2080",
    doi = "10.18653/v1/S19-2080",
    pages = "453--459",
    abstract = "This paper describes MITRE{'}s participation in SemEval-2019 Task 5, HatEval: Multilingual detection of hate speech against immigrants and women in Twitter. The techniques explored range from simple bag-of-ngrams classifiers to neural architectures with varied attention mechanisms. We describe several styles of transfer learning from auxiliary tasks, including a novel method for adapting pre-trained BERT models to Twitter data. Logistic regression ties the systems together into an ensemble submitted for evaluation. The resulting system was used to produce predictions for all four HatEval subtasks, achieving the best mean rank of all teams that participated in all four conditions.",
}

@inproceedings{Davidson2017AutomatedHS,
  title={Automated Hate Speech Detection and the Problem of Offensive Language},
  author={Thomas Davidson and Dana Warmsley and Michael W. Macy and Ingmar Weber},
  booktitle={ICWSM},
  year={2017}
}


%
% Definiciones de Hate Speech
%
%


@techreport{cele2019,
title = {Los discursos de odio como amenaza
a los derechos humanos},
author ={Natalia Torres and Víctor Taricco},
  year={2019},
  organization={Centro de Estudios en Libertad de Expresi\'{o}n y Acceso a la Informaci\'{o}n},
  publisher = {Universidad de Palermo, Argentina},
  institution = {CELE}
}

@techreport{article192015,
  title = {Hate Speech Explained: A Toolkit},
  year={2015},
  institution = {Article 19, London, UK},
  author = {{Article 19}},
  publisher = {Article XIX, London, United Kingdom},
  address = "London, UK"
}

@techreport{CIDH2015,
title = {Discurso de Odio y la incitación a la violencia contra las personas lesbianas, gays, bisexuales, trans e intersex en Am\'{e}rica},
  year= {2015},
  author = {CIDH},
  institution = {Comisi\'{o}n Interamericana sobre Derechos Humanos}
}


@misc{humanos2018convencion,
  title={Convenci{\'o}n Americana sobre Derechos Humanos (Pacto de San Jos{\'e})},
  author={Humanos, Convenci{\'o}n Americana Sobre Derechos},
  year={2018},
  publisher={CADH}
}
@book{bishop2006pattern,
author = {Bishop, Christopher M.},
title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
year = {2006},
isbn = {0387310738},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg}
}

@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}


@book{gagliardone2015countering,
  title={Countering online hate speech},
  author={Gagliardone, Iginio and Gal, Danit and Alves, Thiago and Martinez, Gabriela},
  year={2015},
  publisher={Unesco Publishing}
}



%%
%%
%% Bender's Rule
%%
%%
%%

@inproceedings{bender2009linguistically,
  title={Linguistically na{\"\i}ve!= language independent: Why NLP needs linguistic typology},
  author={Bender, Emily M},
  booktitle={Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics: Virtuous, Vicious or Vacuous?},
  pages={26--32},
  year={2009}
}

@article{bender2011achieving,
  title={On achieving and evaluating language-independence in NLP},
  author={Bender, Emily M},
  journal={Linguistic Issues in Language Technology},
  volume={6},
  number={3},
  pages={1--26},
  year={2011}
}

@article{bender2019rule,
author = {Bender, Emily},
title = {The BenderRule: On Naming the Languages We Study and Why It Matters},
journal = {The Gradient},
year = {2019},
howpublished = {\url{https://thegradient.pub/the-benderrule-on-naming-the-languages-we-study-and-why-it-matters/ } },
}

%%
%%
%% Lenguaje Ofensivo
%%
%%

@misc{zampieri2019predicting,
      title={Predicting the Type and Target of Offensive Posts in Social Media},
      author={Marcos Zampieri and Shervin Malmasi and Preslav Nakov and Sara Rosenthal and Noura Farra and Ritesh Kumar},
      year={2019},
      eprint={1902.09666},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{chiril-etal-2020-said,
    title = "He said {``}who{'}s gonna take care of your children when you are at {ACL}?{''}: Reported Sexist Acts are Not Sexist",
    author = "Chiril, Patricia  and
      Moriceau, V{\'e}ronique  and
      Benamara, Farah  and
      Mari, Alda  and
      Origgi, Gloria  and
      Coulomb-Gully, Marl{\`e}ne",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.373",
    doi = "10.18653/v1/2020.acl-main.373",
    pages = "4055--4066",
    abstract = "In a context of offensive content mediation on social media now regulated by European laws, it is important not only to be able to automatically detect sexist content but also to identify if a message with a sexist content is really sexist or is a story of sexism experienced by a woman. We propose: (1) a new characterization of sexist content inspired by speech acts theory and discourse analysis studies, (2) the first French dataset annotated for sexism detection, and (3) a set of deep learning experiments trained on top of a combination of several tweet{'}s vectorial representations (word embeddings, linguistic features, and various generalization strategies). Our results are encouraging and constitute a first step towards offensive content moderation.",
}
%%
%%
%% Contextualizado
%%
%%

@inproceedings{gao-huang-2017-detecting,
    title = "Detecting Online Hate Speech Using Context Aware Models",
    author = "Gao, Lei  and
      Huang, Ruihong",
    booktitle = "Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",
    month = sep,
    year = "2017",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd.",
    url = "https://doi.org/10.26615/978-954-452-049-6_036",
    doi = "10.26615/978-954-452-049-6_036",
    pages = "260--266",
    abstract = "In the wake of a polarizing election, the cyber world is laden with hate speech. Context accompanying a hate speech text is useful for identifying hate speech, which however has been largely overlooked in existing datasets and hate speech detection models. In this paper, we provide an annotated corpus of hate speech with context information well kept. Then we propose two types of hate speech detection models that incorporate context information, a logistic regression model with context features and a neural network model with learning components for context. Our evaluation shows that both models outperform a strong baseline by around 3{\%} to 4{\%} in F1 score and combining these two models further improve the performance by another 7{\%} in F1 score.",
}



%%%
%%% Contextualized detection of hate speech, offensive language and others
%%%

@inproceedings{borkan2019civil,
  author = {Borkan, Daniel and Dixon, Lucas and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
  title = {Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification},
  year = {2019},
  isbn = {9781450366755},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3308560.3317593},
  doi = {10.1145/3308560.3317593},
  abstract = {Unintended bias in Machine Learning can manifest as systemic differences in performance
  for different demographic groups, potentially compounding existing challenges to fairness
  in society at large. In this paper, we introduce a suite of threshold-agnostic metrics
  that provide a nuanced view of this unintended bias, by considering the various ways
  that a classifier’s score distribution can vary across designated groups. We also
  introduce a large new test set of online comments with crowd-sourced annotations for
  identity references. We use this to show how our metrics can be used to find new and
  potentially subtle unintended bias in existing public models.},
  booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
  pages = {491–500},
  numpages = {10},
  location = {San Francisco, USA},
  series = {WWW '19}
}

@article{pavlopoulos2020toxicity,
  title={Toxicity Detection: Does Context Really Matter?},
  author={Pavlopoulos, John and Sorensen, Jeffrey and Dixon, Lucas and Thain, Nithum and Androutsopoulos, Ion},
  journal={arXiv preprint arXiv:2006.00998},
  year={2020}
}

@inproceedings{hua-etal-2018-wikiconv,
    title = "{W}iki{C}onv: A Corpus of the Complete Conversational History of a Large Online Collaborative Community",
    author = "Hua, Yiqing  and
      Danescu-Niculescu-Mizil, Cristian  and
      Taraborelli, Dario  and
      Thain, Nithum  and
      Sorensen, Jeffery  and
      Dixon, Lucas",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1305",
    doi = "10.18653/v1/D18-1305",
    pages = "2818--2823",
    abstract = "We present a corpus that encompasses the complete history of conversations between contributors to Wikipedia, one of the largest online collaborative communities. By recording the intermediate states of conversations - including not only comments and replies, but also their modifications, deletions and restorations - this data offers an unprecedented view of online conversation. Our framework is designed to be language agnostic, and we show that it extracts high quality data in both Chinese and English. This level of detail supports new research questions pertaining to the process (and challenges) of large-scale online collaboration. We illustrate the corpus{'} potential with two case studies on English Wikipedia that highlight new perspectives on earlier work. First, we explore how a person{'}s conversational behavior depends on how they relate to the discussion{'}s venue. Second, we show that community moderation of toxic behavior happens at a higher rate than previously estimated.",
}

@inproceedings{xenos-2021-context,
    title = "Context Sensitivity Estimation in Toxicity Detection",
    author = "Xenos, Alexandros  and
      Pavlopoulos, John  and
      Androutsopoulos, Ion",
    booktitle = "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.woah-1.15",
    doi = "10.18653/v1/2021.woah-1.15",
    pages = "140--145"
}


@inproceedings{mubarak-etal-2017-abusive,
    title = "Abusive Language Detection on {A}rabic Social Media",
    author = "Mubarak, Hamdy  and
      Darwish, Kareem  and
      Magdy, Walid",
    booktitle = "Proceedings of the First Workshop on Abusive Language Online",
    month = aug,
    year = "2017",
    address = "Vancouver, BC, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-3008",
    doi = "10.18653/v1/W17-3008",
    pages = "52--56",
    abstract = "In this paper, we present our work on detecting abusive language on Arabic social media. We extract a list of obscene words and hashtags using common patterns used in offensive and rude communications. We also classify Twitter users according to whether they use any of these words or not in their tweets. We expand the list of obscene words using this classification, and we report results on a newly created dataset of classified Arabic tweets (obscene, offensive, and clean). We make this dataset freely available for research, in addition to the list of obscene words and hashtags. We are also publicly releasing a large corpus of classified user comments that were deleted from a popular Arabic news site due to violations the site{'}s rules and guidelines.",
}

@article{sheth2021defining,
  title={Defining and Detecting Toxicity on Social Media: Context and Knowledge are Key},
  author={Sheth, Amit and Shalin, Valerie L and Kursuncu, Ugur},
  journal={arXiv preprint arXiv:2104.10788},
  year={2021}
}

@inproceedings{gaur2020infusion,
author = {Gaur, Manas and Kursuncu, Ugur and Sheth, Amit and Wickramarachchi, Ruwan and Yadav, Shweta},
title = {Knowledge-Infused Deep Learning},
year = {2020},
isbn = {9781450370981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372923.3404862},
doi = {10.1145/3372923.3404862},
abstract = {Deep Learning has shown remarkable success during the last decade for essential tasks
in computer vision and natural language processing. Yet, challenges remain in the
development and deployment of artificial intelligence (AI) models in real-world cases,
such as dependence on extensive data and trust, explainability, traceability, and
interactivity. These challenges are amplified in high-risk fields, including healthcare,
cyber threats, crisis response, autonomous driving, and future manufacturing. On the
other hand, symbolic computing with knowledge graphs has shown significant growth
in specific tasks with reliable performance. This tutorial (a) discusses the novel
paradigm of knowledge-infused deep learning to synthesize neural computing with symbolic
computing (b) describes different forms of knowledge and infusion methods in deep
learning, and (c) discusses application-specific evaluation methods to assure explainability
and reasoning using benchmark datasets and knowledge-resources. The resulting paradigm
of "knowledge-infused learning'' combines knowledge from both domain expertise and
physical models. A wide variety of techniques involving shallow, semi-deep, and deep
infusion will be discussed along with the corresponding intuitions, limitations, use
cases, and applications. More details can be found urlhttp://kidl2020.aiisc.ai/.},
booktitle = {Proceedings of the 31st ACM Conference on Hypertext and Social Media},
pages = {309–310},
numpages = {2},
keywords = {disaster resilience, public health, cyber-social threats, autonomous driving, neuro-symbolic computing, knowledge-infused learning, knowledge graphs, deep learning},
location = {Virtual Event, USA},
series = {HT '20}
}

@inproceedings{wiegand2021implicitly,
  title={Implicitly Abusive Language--What does it actually look like and why are we not getting there?},
  author={Wiegand, Michael and Ruppenhofer, Josef and Eder, Elisabeth},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={576--587},
  year={2021}
}

% Usado en Gao

@article{pennebaker2001linguistic,
  title={Linguistic inquiry and word count: LIWC 2001},
  author={Pennebaker, James W and Francis, Martha E and Booth, Roger J},
  journal={Mahway: Lawrence Erlbaum Associates},
  volume={71},
  number={2001},
  pages={2001},
  year={2001}
}

@article{mohammad2013nrc,
  title={Nrc emotion lexicon},
  author={Mohammad, Saif M and Turney, Peter D},
  journal={National Research Council, Canada},
  volume={2},
  year={2013}
}

% Listo

%%
%%
%% Random
%%
%%

@article{blondel2008fast,
  title={Fast unfolding of communities in large networks},
  author={Blondel, Vincent D and Guillaume, Jean-Loup and Lambiotte, Renaud and Lefebvre, Etienne},
  journal={Journal of statistical mechanics: theory and experiment},
  volume={2008},
  number={10},
  pages={P10008},
  year={2008},
  publisher={IOP Publishing}
}

@article{zhang-2014-multilabel,
author = {Zhang, Min-Ling and Zhou, Zhi-Hua},
year = {2014},
month = {08},
pages = {1819-1837},
title = {A Review On Multi-Label Learning Algorithms},
volume = {26},
journal = {Knowledge and Data Engineering, IEEE Transactions on},
doi = {10.1109/TKDE.2013.39}
}

@article{izsak2021train,
  title={How to Train BERT with an Academic Budget},
  author={Izsak, Peter and Berchansky, Moshe and Levy, Omer},
  journal={arXiv preprint arXiv:2104.07705},
  year={2021}
}

@article{you2019large,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019}
}
@article{benjamini1995controlling,
  title={Controlling the false discovery rate: a practical and powerful approach to multiple testing},
  author={Benjamini, Yoav and Hochberg, Yosef},
  journal={Journal of the Royal statistical society: series B (Methodological)},
  volume={57},
  number={1},
  pages={289--300},
  year={1995},
  publisher={Wiley Online Library}
}

@article{pang2008opinion,
author = {Pang, Bo and Lee, Lillian},
title = {Opinion Mining and Sentiment Analysis},
year = {2008},
issue_date = {January 2008},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {2},
number = {1–2},
issn = {1554-0669},
url = {https://doi.org/10.1561/1500000011},
doi = {10.1561/1500000011},
abstract = {An important part of our information-gathering behavior has always been to find out what other people think. With the growing availability and popularity of opinion-rich resources such as online review sites and personal blogs, new opportunities and challenges arise as people now can, and do, actively use information technologies to seek out and understand the opinions of others. The sudden eruption of activity in the area of opinion mining and sentiment analysis, which deals with the computational treatment of opinion, sentiment, and subjectivity in text, has thus occurred at least in part as a direct response to the surge of interest in new systems that deal directly with opinions as a first-class object.This survey covers techniques and approaches that promise to directly enable opinion-oriented information-seeking systems. Our focus is on methods that seek to address the new challenges raised by sentiment-aware applications, as compared to those that are already present in more traditional fact-based analysis. We include material on summarization of evaluative text and on broader issues regarding privacy, manipulation, and economic impact that the development of opinion-oriented information-access services gives rise to. To facilitate future work, a discussion of available resources, benchmark datasets, and evaluation campaigns is also provided.},
journal = {Found. Trends Inf. Retr.},
month = {jan},
pages = {1–135},
numpages = {135}
}

@article{horrigan2008online,
  title={Online shopping, pew Internet \& American life project report},
  author={Horrigan, J},
  journal={Washington, DC: Pew Research Center},
  pages={1--42},
  year={2008}
}

@article{pavlopoulos2014aspect,
  title={Aspect based sentiment analysis},
  author={Pavlopoulos, Ioannis},
  journal={Athens University of Economics and Business},
  year={2014}
}

@inproceedings{sharma-etal-2020-semeval,
    title = "{S}em{E}val-2020 Task 8: Memotion Analysis- the Visuo-Lingual Metaphor!",
    author = {Sharma, Chhavi  and
      Bhageria, Deepesh  and
      Scott, William  and
      PYKL, Srinivas  and
      Das, Amitava  and
      Chakraborty, Tanmoy  and
      Pulabaigari, Viswanath  and
      Gamb{\"a}ck, Bj{\"o}rn},
    booktitle = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
    month = dec,
    year = "2020",
    address = "Barcelona (online)",
    publisher = "International Committee for Computational Linguistics",
    url = "https://aclanthology.org/2020.semeval-1.99",
    doi = "10.18653/v1/2020.semeval-1.99",
    pages = "759--773",
    abstract = "Information on social media comprises of various modalities such as textual, visual and audio. NLP and Computer Vision communities often leverage only one prominent modality in isolation to study social media. However, computational processing of Internet memes needs a hybrid approach. The growing ubiquity of Internet memes on social media platforms such as Facebook, Instagram, and Twitter further suggests that we can not ignore such multimodal content anymore. To the best of our knowledge, there is not much attention towards meme emotion analysis. The objective of this proposal is to bring the attention of the research community towards the automatic processing of Internet memes. The task Memotion analysis released approx 10K annotated memes- with human annotated labels namely sentiment(positive, negative, neutral), type of emotion(sarcastic,funny,offensive, motivation) and their corresponding intensity. The challenge consisted of three subtasks: sentiment (positive, negative, and neutral) analysis of memes,overall emotion (humor, sarcasm, offensive, and motivational) classification of memes, and classifying intensity of meme emotion. The best performances achieved were F1 (macro average) scores of 0.35, 0.51 and 0.32, respectively for each of the three subtasks.",
}

@inproceedings{socher-etal-2013-recursive,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D13-1170",
    pages = "1631--1642",
}

@article{likert1932technique,
  title={A technique for the measurement of attitudes.},
  author={Likert, Rensis},
  journal={Archives of psychology},
  year={1932}
}

@inproceedings{bender-koller-2020-climbing,
    title = "Climbing towards {NLU}: {On} Meaning, Form, and Understanding in the Age of Data",
    author = "Bender, Emily M.  and
      Koller, Alexander",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.463",
    doi = "10.18653/v1/2020.acl-main.463",
    pages = "5185--5198",
    abstract = "The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as {``}understanding{''} language or capturing {``}meaning{''}. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of {``}Taking Stock of Where We{'}ve Been and Where We{'}re Going{''}, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.",
}

@article{mitchell2021ai,
  title={Why AI is harder than we think},
  author={Mitchell, Melanie},
  journal={arXiv preprint arXiv:2104.12871},
  year={2021}
}
@inproceedings{li2017data,
  title={Data sets: Word embeddings learned from tweets and general data},
  author={Li, Quanzhi and Shah, Sameena and Liu, Xiaomo and Nourbakhsh, Armineh},
  booktitle={Proceedings of the International AAAI Conference on Web and Social Media},
  volume={11},
  number={1},
  year={2017}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}
@inproceedings{plaza-del-arco-etal-2020-emoevent,
    title = "{E}mo{E}vent: A Multilingual Emotion Corpus based on different Events",
    author = "Plaza del Arco, Flor Miriam  and
      Strapparava, Carlo  and
      Urena Lopez, L. Alfonso  and
      Martin, Maite",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.186",
    pages = "1492--1498",
    abstract = "In recent years emotion detection in text has become more popular due to its potential applications in fields such as psychology, marketing, political science, and artificial intelligence, among others. While opinion mining is a well-established task with many standard data sets and well-defined methodologies, emotion mining has received less attention due to its complexity. In particular, the annotated gold standard resources available are not enough. In order to address this shortage, we present a multilingual emotion data set based on different events that took place in April 2019. We collected tweets from the Twitter platform. Then one of seven emotions, six Ekman{'}s basic emotions plus the {``}neutral or other emotions{''}, was labeled on each tweet by 3 Amazon MTurkers. A total of 8,409 in Spanish and 7,303 in English were labeled. In addition, each tweet was also labeled as offensive or no offensive. We report some linguistic statistics about the data set in order to observe the difference between English and Spanish speakers when they express emotions related to the same events. Moreover, in order to validate the effectiveness of the data set, we also propose a machine learning approach for automatically detecting emotions in tweets for both languages, English and Spanish.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@article{plaza2021pretrained,
title = {Comparing pre-trained language models for Spanish hate speech detection},
journal = {Expert Systems with Applications},
volume = {166},
pages = {114120},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114120},
url = {https://www.sciencedirect.com/science/article/pii/S095741742030868X},
author = {Flor Miriam Plaza-del-Arco and M. Dolores Molina-González and L. Alfonso Ureña-López and M. Teresa Martín-Valdivia},
keywords = {Hate speech, Transfer learning, BERT, BETO, Natural language processing, Text classification},
abstract = {Nowadays, due to the great uncontrolled content posted daily on the Web, there has also been a huge increase in the dissemination of hate speech worldwide. Social media, blogs and community forums are examples where people are freely allowed to communicate. However, freedom of expression is not always respectful since offensive or insulting language is sometimes used. Social media companies often rely on users and content moderators to report on this type of content. Nevertheless, due to the large amount of content generated every day on the Web, automatic systems based on Natural Language Processing techniques are required for identifying abusive language online. To date, most of the systems developed to combat this problem are mainly focused on English content, but this issue is a worldwide concern and therefore other languages such as Spanish are involved. In this paper, we address the task of Spanish hate speech identification on social media and provide a deeper understanding of the capabilities of new techniques based on machine learning. In particular, we compare the performance of Deep Learning methods with recently pre-trained language models based on Transfer Learning as well as with traditional machine learning models. Our main contribution is the achievement of promising results in Spanish by applying multilingual and monolingual pre-trained language models such as BERT, XLM and BETO.}
}

@article{taule2021detoxis,
	author = {Mariona Taulé y Alejandro Ariza y Montserrat Nofre y Enrique Amigó y Paolo Rosso},
	title = {Overview of DETOXIS at IberLEF 2021: DEtection of TOXicity in comments In Spanish},
	journal = {Procesamiento del Lenguaje Natural},
	volume = {67},
	number = {0},
	year = {2021},
	keywords = {},
	abstract = {In this paper we present the DETOXIS task, DEtection of TOxicity in comments In Spanish, which took place as part of the IberLEF 2021 Workshop on Iberian Languages Evaluation Forum at the SEPLN 2021 Conference. We describe the NewsCom-TOX dataset used for training and testing the systems, the metrics applied for their evaluation and the results obtained by the submitted approaches. We also provide an error analysis of the results of these systems.},
	issn = {1989-7553},
	url = {http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6390},
	pages = {209--221}
}

@inproceedings{gupta-yang-2017-crystalnest,
    title = "{C}rystal{N}est at {S}em{E}val-2017 Task 4: Using Sarcasm Detection for Enhancing Sentiment Classification and Quantification",
    author = "Gupta, Raj Kumar  and
      Yang, Yinping",
    booktitle = "Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S17-2103",
    doi = "10.18653/v1/S17-2103",
    pages = "626--633",
    abstract = "This paper describes a system developed for a shared sentiment analysis task and its subtasks organized by SemEval-2017. A key feature of our system is the embedded ability to detect sarcasm in order to enhance the performance of sentiment classification. We first constructed an affect-cognition-sociolinguistics sarcasm features model and trained a SVM-based classifier for detecting sarcastic expressions from general tweets. For sentiment prediction, we developed CrystalNest{--} a two-level cascade classification system using features combining sarcasm score derived from our sarcasm classifier, sentiment scores from Alchemy, NRC lexicon, n-grams, word embedding vectors, and part-of-speech features. We found that the sarcasm detection derived features consistently benefited key sentiment analysis evaluation metrics, in different degrees, across four subtasks A-D.",
}


@inproceedings{ortega2019overview,
  title={Overview of the task on irony detection in Spanish variants},
  author={Ortega-Bueno, Reynier and Rangel, Francisco and Hern{\'a}ndez Far{\i}as, D and Rosso, Paolo and Montes-y-G{\'o}mez, Manuel and Medina Pagola, Jos{\'e} E},
  booktitle={Proceedings of the Iberian languages evaluation forum (IberLEF 2019), co-located with 34th conference of the Spanish Society for natural language processing (SEPLN 2019). CEUR-WS. org},
  volume={2421},
  pages={229--256},
  year={2019}
}

@article{ekman1992argument,
  title={An argument for basic emotions},
  author={Ekman, Paul},
  journal={Cognition \& emotion},
  volume={6},
  number={3-4},
  pages={169--200},
  year={1992},
  publisher={Taylor \& Francis}
}

@inproceedings{han2011lexical,
  title={Lexical normalisation of short text messages: Makn sens a\# twitter},
  author={Han, Bo and Baldwin, Timothy},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies},
  pages={368--378},
  year={2011}
}

@inproceedings{hendrycks-etal-2020-pretrained,
    title = "Pretrained Transformers Improve Out-of-Distribution Robustness",
    author = "Hendrycks, Dan  and
      Liu, Xiaoyuan  and
      Wallace, Eric  and
      Dziedzic, Adam  and
      Krishnan, Rishabh  and
      Song, Dawn",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.244",
    doi = "10.18653/v1/2020.acl-main.244",
    pages = "2744--2751",
    abstract = "Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers{'} performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.",
}


@misc{ruder2021lmfine-tuning,
  author = {Ruder, Sebastian},
  title = {{Recent Advances in Language Model Fine-tuning}},
  year = {2021},
  howpublished = {\url{http://ruder.io/recent-advances-lm-fine-tuning}},
}

@inproceedings{strubell2019energy,
  title={Energy and Policy Considerations for Deep Learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3645--3650},
  year={2019}
}

@inproceedings{kudo-richardson-2018-sentencepiece,
    title = "{S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
    author = "Kudo, Taku  and
      Richardson, John",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-2012",
    doi = "10.18653/v1/D18-2012",
    pages = "66--71",
    abstract = "This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at \url{https://github.com/google/sentencepiece}.",
}

@inproceedings{eisenstein2013bad,
  title={What to do about bad language on the internet},
  author={Eisenstein, Jacob},
  booktitle={Proceedings of the 2013 conference of the North American Chapter of the association for computational linguistics: Human language technologies},
  pages={359--369},
  year={2013}
}
@inproceedings{ritter2011named,
  title={Named entity recognition in tweets: an experimental study},
  author={Ritter, Alan and Clark, Sam and Etzioni, Oren and others},
  booktitle={Proceedings of the 2011 conference on empirical methods in natural language processing},
  pages={1524--1534},
  year={2011}
}

@techreport{gimpel2010part,
  title={Part-of-speech tagging for twitter: Annotation, features, and experiments},
  author={Gimpel, Kevin and Schneider, Nathan and O'Connor, Brendan and Das, Dipanjan and Mills, Daniel and Eisenstein, Jacob and Heilman, Michael and Yogatama, Dani and Flanigan, Jeffrey and Smith, Noah A},
  year={2010},
  institution={Carnegie-Mellon Univ Pittsburgh Pa School of Computer Science}
}

@inproceedings{sennrich2016neural,
  title={Neural Machine Translation of Rare Words with Subword Units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1715--1725},
  year={2016}
}

@INPROCEEDINGS{imagenet2009deng,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},
  title={ImageNet: A large-scale hierarchical image database},
  year={2009},
  volume={},
  number={},
  pages={248-255},
  doi={10.1109/CVPR.2009.5206848}}

@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}

@article{ruder2018nlpimagenet,
author = {Ruder, Sebastian},
title = {NLP's ImageNet moment has arrived},
journal = {The Gradient},
year = {2018},
howpublished = {\url{https://thegradient.pub/nlp-imagenet/ } },
}

@inproceedings{luong2013better,
  title={Better word representations with recursive neural networks for morphology},
  author={Luong, Minh-Thang and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the seventeenth conference on computational natural language learning},
  pages={104--113},
  year={2013}
}

@inproceedings{cho-etal-2014-learning,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}

@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}

@ARTICLE{brufau2020emotion,
  title={Emotion Network Analysis During COVID-19 Quarantine ‐ A Longitudinal Study},
  author={Martín-Brufau, Ramón and Suso-Ribera, Carlos and Corbalán, Javier},
  journal={Frontiers in Psychology},
  volume={11},
  pages={2802},
  year={2020},
  url={https://www.frontiersin.org/article/10.3389/fpsyg.2020.559572},
  doi={10.3389/fpsyg.2020.559572},
  issn={1664-1078},
}
@inbook{graells2019abortion,
author = {Graells-Garrido, Eduardo and Baeza-Yates, Ricardo and Lalmas, Mounia},
title = {How Representative is an Abortion Debate on Twitter?},
year = {2019},
isbn = {9781450362023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292522.3326057},
abstract = {Today, more than ever, social networks and micro-blogging platforms are used as tools for political exchange. However, these platforms are biased in several aspects, from their algorithms to the population participating in them. With respect to the latter, we analyze the discussion on Twitter about an abortion bill in Chile, proposed in January 2015, and approved as law in September 2017. We find that Twitter has strong biases in population representation. Still, when carefully paired with demographic attributes, Twitter-based insights on the characteristics of political discussion match those from national-level surveys.},
booktitle = {Proceedings of the 10th ACM Conference on Web Science},
pages = {133–134},
numpages = {2}
}

@article{zuiderveen2018online,
  title={Online political microtargeting: promises and threats for democracy},
  author={Zuiderveen Borgesius, Frederik and M{\"o}ller, Judith and Kruikemeier, Sanne and {\'O} Fathaigh, Ronan and Irion, Kristina and Dobber, Tom and Bodo, Balazs and de Vreese, Claes H},
  journal={Utrecht Law Review},
  volume={14},
  number={1},
  pages={82--96},
  year={2018}
}

@article{calderon2020topic,
  title={Topic modeling and characterization of hate speech against immigrants on Twitter around the emergence of a far-right party in Spain},
  author={Calder{\'o}n, Carlos Arcila and de la Vega, Gonzalo and Herrero, David Blanco},
  journal={Social Sciences},
  volume={9},
  number={11},
  pages={188},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@InProceedings{maas-EtAl:2011:ACL-HLT2011,
  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
  title     = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {142--150},
  url       = {http://www.aclweb.org/anthology/P11-1015}
}

@inproceedings{pavlopoulos2021semeval,
  title={Semeval-2021 task 5: Toxic spans detection},
  author={Pavlopoulos, John and Sorensen, Jeffrey and Laugier, L{\'e}o and Androutsopoulos, Ion},
  booktitle={Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)},
  pages={59--69},
  year={2021}
}

@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}

@article{yue2019survey,
  title={A survey of sentiment analysis in social media},
  author={Yue, Lin and Chen, Weitong and Li, Xue and Zuo, Wanli and Yin, Minghao},
  journal={Knowledge and Information Systems},
  volume={60},
  number={2},
  pages={617--663},
  year={2019},
  publisher={Springer}
}

@article{plaza2020detecting,
  title={Detecting misogyny and xenophobia in Spanish tweets using language technologies},
  author={Plaza-Del-Arco, Flor-Miriam and Molina-Gonz{\'a}lez, M Dolores and Ure{\~n}a-L{\'o}pez, L Alfonso and Mart{\'\i}n-Valdivia, M Teresa},
  journal={ACM Transactions on Internet Technology (TOIT)},
  volume={20},
  number={2},
  pages={1--19},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{pereira2019detecting,
  title={Detecting and monitoring hate speech in Twitter},
  author={Pereira-Kohatsu, Juan Carlos and Quijano-S{\'a}nchez, Lara and Liberatore, Federico and Camacho-Collados, Miguel},
  journal={Sensors},
  volume={19},
  number={21},
  pages={4654},
  year={2019},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@article{landauer1997solution,
  title={A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.},
  author={Landauer, Thomas K and Dumais, Susan T},
  journal={Psychological review},
  volume={104},
  number={2},
  pages={211},
  year={1997},
  publisher={American Psychological Association}
}


@article{bengio2003neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Janvin, Christian},
  journal={The journal of machine learning research},
  volume={3},
  pages={1137--1155},
  year={2003},
  publisher={JMLR. org}
}

@article{collobert2011natural,
  title={Natural language processing (almost) from scratch},
  author={Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
  journal={Journal of machine learning research},
  volume={12},
  number={ARTICLE},
  pages={2493--2537},
  year={2011}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@inproceedings{peters2017semi,
  title={Semi-supervised sequence tagging with bidirectional language models},
  author={Peters, Matthew and Ammar, Waleed and Bhagavatula, Chandra and Power, Russell},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1756--1765},
  year={2017}
}

@inproceedings{merity2018regularizing,
  title={Regularizing and Optimizing LSTM Language Models},
  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{gumperz1992contextualization,
  title={Contextualization revisited},
  author={Gumperz, John J},
  journal={The contextualization of language},
  volume={22},
  pages={39--53},
  year={1992},
  publisher={John Benjamins Amsterdam}
}

@inproceedings{lai2018stance,
  title={Stance evolution and twitter interactions in an italian political debate},
  author={Lai, Mirko and Patti, Viviana and Ruffo, Giancarlo and Rosso, Paolo},
  booktitle={International Conference on Applications of Natural Language to Information Systems},
  pages={15--27},
  year={2018},
  organization={Springer}
}

@article{furman2021you,
  title={You can simply rely on communities for a robust characterization of stances},
  author={Furman, Dami{\'a}n and Marro, Santiago and Cardellino, Cristian and Popa, Diana and Alemany, Laura Alonso},
  journal={Florida Artificial Intelligence Research Society},
  volume={34},
  number={1},
  year={2021}
}

@inproceedings{zhang2019ernie,
  title={ERNIE: Enhanced Language Representation with Informative Entities},
  author={Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={1441--1451},
  year={2019}
}

@book{krippendorff2018content,
  title={Content analysis: An introduction to its methodology},
  author={Krippendorff, Klaus},
  year={2018},
  publisher={Sage publications}
}
@misc{he2021racism,
      title={Racism is a Virus: Anti-Asian Hate and Counterspeech in Social Media during the COVID-19 Crisis},
      author={Bing He and Caleb Ziems and Sandeep Soni and Naren Ramakrishnan and Diyi Yang and Srijan Kumar},
      year={2021},
      eprint={2005.12423},
      archivePrefix={arXiv},
      primaryClass={cs.SI}
}

@article{hosseini2017deceiving,
  title={Deceiving google's perspective api built for detecting toxic comments},
  author={Hosseini, Hossein and Kannan, Sreeram and Zhang, Baosen and Poovendran, Radha},
  journal={arXiv preprint arXiv:1702.08138},
  year={2017}
}
@INPROCEEDINGS{jain2018adversarial,  author={Jain, Edwin and Brown, Stephan and Chen, Jeffery and Neaton, Erin and Baidas, Mohammad and Dong, Ziqian and Gu, Huanying and Artan, Nabi Sertac},  booktitle={2018 International Conference on Computational Science and Computational Intelligence (CSCI)},   title={Adversarial Text Generation for Google's Perspective API},   year={2018},  volume={},  number={},  pages={1136-1141},  doi={10.1109/CSCI46756.2018.00220}}

@misc{battaglia2018relational,
      title={Relational inductive biases, deep learning, and graph networks},
      author={Peter W. Battaglia and Jessica B. Hamrick and Victor Bapst and Alvaro Sanchez-Gonzalez and Vinicius Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Caglar Gulcehre and Francis Song and Andrew Ballard and Justin Gilmer and George Dahl and Ashish Vaswani and Kelsey Allen and Charles Nash and Victoria Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matt Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
      year={2018},
      eprint={1806.01261},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{luong2015effective,
  title={Effective Approaches to Attention-based Neural Machine Translation},
  author={Luong, Thang and Pham, Hieu and Manning, Christopher D},
  booktitle={EMNLP},
  year={2015}
}

@inproceedings{parikh-etal-2016-decomposable,
    title = "A Decomposable Attention Model for Natural Language Inference",
    author = {Parikh, Ankur  and
      T{\"a}ckstr{\"o}m, Oscar  and
      Das, Dipanjan  and
      Uszkoreit, Jakob},
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1244",
    doi = "10.18653/v1/D16-1244",
    pages = "2249--2255",
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya}
}

@article{taylor1953cloze,
  title={“Cloze procedure”: A new tool for measuring readability},
  author={Taylor, Wilson L},
  journal={Journalism quarterly},
  volume={30},
  number={4},
  pages={415--433},
  year={1953},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{wu2016google,
  title={Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  year={2016}
}
@article{brown1992class,
  title={Class-based n-gram models of natural language},
  author={Brown, Peter F and Della Pietra, Vincent J and Desouza, Peter V and Lai, Jennifer C and Mercer, Robert L},
  journal={Computational linguistics},
  volume={18},
  number={4},
  pages={467--480},
  year={1992}
}

@InProceedings{zhu2015bookscorpus,
    title = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
    author = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
    booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
    month = {December},
    year = {2015}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}



@inproceedings{polignano2019alberto,
  title={Alberto: Italian BERT language understanding model for NLP challenging tasks based on tweets},
  author={Polignano, Marco and Basile, Pierpaolo and De Gemmis, Marco and Semeraro, Giovanni and Basile, Valerio},
  booktitle={6th Italian Conference on Computational Linguistics, CLiC-it 2019},
  volume={2481},
  pages={1--6},
  year={2019},
  organization={CEUR}
}

@article{muller2020covid,
  title={Covid-twitter-bert: A natural language processing model to analyse covid-19 content on twitter},
  author={M{\"u}ller, Martin and Salath{\'e}, Marcel and Kummervold, Per E},
  journal={arXiv preprint arXiv:2005.07503},
  year={2020}
}

@article{perez2021robertuito,
  title={RoBERTuito: a pre-trained language model for social media text in Spanish},
  author={P{\'e}rez, Juan Manuel and Furman, Dami{\'a}n A and Alemany, Laura Alonso and Luque, Franco},
  journal={arXiv preprint arXiv:2111.09453},
  year={2021}
}


@misc{gutierrezfandino2021spanish,
      title={Spanish Language Models},
      author={Asier Gutiérrez-Fandiño and Jordi Armengol-Estapé and Marc Pàmies and Joan Llop-Palao and Joaquín Silveira-Ocampo and Casimiro Pio Carrino and Aitor Gonzalez-Agirre and Carme Armentano-Oller and Carlos Rodriguez-Penagos and Marta Villegas},
      year={2021},
      eprint={2107.07253},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{raffel2020exploringt5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@article{kruskal1952use,
  title={Use of ranks in one-criterion variance analysis},
  author={Kruskal, William H and Wallis, W Allen},
  journal={Journal of the American statistical Association},
  volume={47},
  number={260},
  pages={583--621},
  year={1952},
  publisher={Taylor \& Francis}
}

@article{dunn1961multiple,
  title={Multiple comparisons among means},
  author={Dunn, Olive Jean},
  journal={Journal of the American statistical association},
  volume={56},
  number={293},
  pages={52--64},
  year={1961},
  publisher={Taylor \& Francis}
}

@inproceedings{saha2019prevalence,
  title={Prevalence and psychological effects of hateful speech in online college communities},
  author={Saha, Koustuv and Chandrasekharan, Eshwar and De Choudhury, Munmun},
  booktitle={Proceedings of the 10th ACM conference on web science},
  pages={255--264},
  year={2019}
}

@article{bilewicz2020hate,
  title={Hate speech epidemic. The dynamic effects of derogatory language on intergroup relations and political radicalization},
  author={Bilewicz, Micha{\l} and Soral, Wiktor},
  journal={Political Psychology},
  volume={41},
  pages={3--33},
  year={2020},
  publisher={Wiley Online Library}
}

@inproceedings{lai2017race,
  title={RACE: Large-scale ReAding Comprehension Dataset From Examinations},
  author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={785--794},
  year={2017}
}

@article{mcculloch1943logical,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={The bulletin of mathematical biophysics},
  volume={5},
  number={4},
  pages={115--133},
  year={1943},
  publisher={Springer}
}

@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group}
}