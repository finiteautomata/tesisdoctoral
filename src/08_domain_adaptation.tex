
En este capítulo exploraremos como mejorar la detección de discurso de odio desde una perspectiva más general, analizando en general la clasificación de textos sociales. Hemos visto en capítulos anteriores que las técnicas de representación utilizadas en los últimos años (desde los word-embeddings hasta ) generan representaciones ricas al ser entrenadas en dominios sociales. Así mismo, también observamos que en algunos modelos pre-entrenados (como AWD-LSTM usando la técnica de ULMFit) \todo{meter citas} realizar un ajuste de dominio más extenso sobre

Entrenar modelos de lenguaje basados en Transformers toman una cantidad de recursos importantes, algo que puede imposibilitar que

Abordaremos la pregunta ¿cómo se compara en el dominio social el ajuste de dominio de modelos pre-entrenados sobre textos formales con los modelos que fueron generados desde cero en textos sociales? Para ello, utilizamos como benchmark de este análisis las tareas que hemos tratado en esta tesis. Entrenamos desde cero un modelo de lenguaje basado en transformers (RoBERTa)\cite{liu2019roberta} sobre tweets, y comparamos su performance contra ajustes de dominio hechos sobre otros modelos pre-entrenados.

Comenzamos este capítulo haciendo una pequeña recapitulación de las técnicas de adaptación de dominio.

\section{Adaptación de dominio}

\citet{goodfellow2016deep} definen la adaptación de dominio como una situación similar a la de Transfer Learning: dado un modelo que fue entrenado sobre una distribución de datos o dominio $P_1$, lo utilizamos sobre una distribución $P_2$ relativamente similar.

En el caso de la adaptación de dominio, nos referimos a la aplicación de alguna técnica que ajuste la distribución de la entrada (de $P_1$) a la distribución de nuestro nuevo dominio. \citet{glorot2011domain} es uno de los primeros trabajos que aplica esta técnica en NLP, usando denoising auto-encoders para este fin.

Para lo que nos concierne en NLP solemos querer, dado un modelo de lenguaje (tanto causal como enmascarado) entrenado en un dominio, ajustarlo a otro dominio distinto. Por ejemplo, un modelo BERT pre-entrenado en textos formales (como Wikipedia o noticias) queremos ajustarlo a la distribución de textos sociales, que si bien ambas mantienen el idioma (inglés o español) suelen tener distribuciones notoriamente distintas.

Dentro de la última ola que sacudió NLP de modelos pre-entrenados, ULMFit \citet{howard-ruder-2018-universal} contempla una etapa de adaptación de dominio utilizando de manera no-supervisada el texto del dataset supervisado de la tarea atacada.

Recientemente, \citet{gururangan-etal-2020-dont} analizan el impacto de los ajustes de dominio. Para ello, consideran varios dominios como ser biomédico, reviews de películas, papers de cs. de la computación (CS), y noticias. Plantean dos configuraciones de adaptación de dominio:

\begin{itemize}
    \item Domain Adaptation: ajustar el modelo de lenguaje sobre un extenso conjunto de datos no etiquetado, usualmente el ``sobrante'' del proceso de recolección que no es anotado
    \item Task Adaptation: ajustar el modelo de lenguaje sobre el dataset, de la misma manera que se hace en ULM-Fit
\end{itemize}

En todos los casos, usando modelos del estado del arte como RoBERTa, que aplicar conjuntamente lo que ellos consideran Domain Adaptation y Task Adaptation mejora la performance significativamente


Algo que queda pendiente de este trabajo es analizar el dominio social, y por otro lado, hacer una comparación de los ajustes de dominios contra entrenar el modelo desde cero en dicho dominio
