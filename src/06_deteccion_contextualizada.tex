\label{chap:06_contextualized_hate_speech}

En este capítulo analizamos el impacto de añadir contexto en la tarea de detección de discurso de odio en redes sociales. Como hemos marcado en los capítulos anteriores, la utilización del contexto ha recibido poca atención en la literatura, limitando la tarea a analizar comentarios aislados de cualquier tópico relacionado o hilo conversacional. Para este estudio, utilizamos el conjunto de datos construído en el Capítulo \ref{chap:05_dataset_creation}, cuyos datos constan de comentarios de artículos periodísticos en Twitter.

El formato de los datos empleados nos brinda información adicional a cada comentario tanto por el tweet del medio periodístico al que contestan como así también por el contenido del artículo. Para evaluar si la adición de contexto resulta en una mejora en la detección de discurso de odio, realizamos experimentos de clasificación con modelos que consumen tres tipos de entrada: el comentario sin contexto, el comentario junto al tweet del medio periodístico, y el comentario junto al tweet y el cuerpo del artículo asociado.

\todo{buscar todos los verbos conjugados en futuro}

El conjunto de datos empleado nos permite analizar una posible combinación más en base al detalle de las características ofendidas por cada comentario. Esta información granular permite no sólo analizar la existencia de discurso de odio sino que permite predecir con más detalle la ofensa cometida. Proponemos en base a esto dos tareas de clasificación: una tarea de detección \textbf{binaria}, donde sólo predecimos si hay o no discurso de odio; y una tarea de detección \textbf{granular}, donde además predecimos todas las características ofendidas (potencialmente más de una). Para estas tareas, propusimos algoritmos de clasificación sobre modelos pre-entrenados de lenguaje que tienen como entrada los distintos tipos de contexto posibles. Estos modelos tienen incorporados naturalmente la posibilidad de consumir dos entradas --el contexto y el texto-- con lo cual son ideales para nuestros experimentos.

Evaluamos los resultados de los experimentos de clasificación tanto en términos del rendimiento de las distintas configuraciones de nuestros clasificadores, como así también realizando análisis de error comparativos entre los modelos contextualizados y los no contextualizados. También evaluamos en este capítulo las dificultades más generales que presenta la detección de este fenómeno sobre comentarios de notas periodísticas.

\section{Trabajo previo}
\label{sec:06_classification_previous}

Como mencionamos en la Sección \ref{sec:dataset_previous}, no se ha dado demasiada atención en la literatura a la utilización de información contextual en la detección de discurso de odio y otros fenómenos similares. Pasamos ahora a repasar los algoritmos de detección utilizados sobre los conjuntos de datos descriptos en dicha sección.

\citet{gao-huang-2017-detecting} proponen utilizar dos tipos de modelos sobre el dataset que ellos mismos recolectaron sobre comentarios de Fox News: regresiones logísticas y redes neuronales recurrentes. Para las regresiones logísticas, usaron como entradas bolsas de palabras, bolsas de caracteres, vectores semánticos producidos con Linguistic Inquiry and Word Count (LIWC) \cite{pennebaker2001linguistic}, y otras variables de un lexicón de emociones \cite{mohammad2013nrc}. Por otro lado, los autores también entrenan LSTM bidireccionales con mecanismo de atención de Bahdanau \cite{bahdanau2014neural} que consumen embeddings \emph{word2vec} de dimensión 100.

Un punto criticable de este trabajo es que utiliza el nombre de usuario como entrada, algo que a priori no suele hacerse ya que permitiría prejuzgar a un usuario antes que por el contenido de sus tweets. Si bien es cierto que la información de usuarios y sus conexiones es valiosa, introducir esta información a nuestros modelos puede dar lugar a correlaciones espurias que es preferible evitar. Otras críticas sobre el proceso de anotación de los datos fueron realizadas ya en la Sección \ref{sec:dataset_previous}.


\begin{figure*}[t]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{img/pavlopoulos_rnn_rnn_classifier.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{img/pavlopoulos_rnn_bert_classifier.png}
    \end{minipage}

    \begin{minipage}[b]{0.35\textwidth}
        \includegraphics[width=\textwidth]{img/pavlopoulos_bert_sep_classifier.png}
    \end{minipage}


    \caption{Clasificadores que consumen contexto propuestos por \citet{pavlopoulos2020toxicity}. Los dos primeros clasificadores proponen una arquitectura de dos encoders, uno para el texto y otro para el contexto usando bi-LSTMs y BERT como posibilidades. El tercer clasificador propuesto es un BERT usando su estructura natural para codificar dos oraciones separadas por el token $SEP$ }
    \label{fig:pavlopoulos_classifiers}
\end{figure*}


En la Sección \ref{sec:dataset_previous} hemos descripto el conjunto de datos construído por \citet{pavlopoulos2020toxicity}, dedicado a la detección de toxicidad y que incorpora información conversacional sobre comentarios de Wikipedia Talk Pages. Nos detenemos un momento para analizar sus experimentos de clasificación ya que guardan importantes similaridades con lo hecho en este capítulo. En ese trabajo se obtuvieron dos conjuntos de entrenamiento: uno en el cual los etiquetadores tenían información del contexto, y otro conjunto en el que no. El conjunto de test, por otro lado, fue anotado teniendo en cuenta el contexto bajo la asunción de que el etiquetado es de mejor calidad al tener más información contextual. Sobre la base de estos datos, los autores plantearon dos preguntas:

\begin{itemize}
    \item ¿Mejora el rendimiento de los clasificadores que son entrenados con el conjunto de datos etiquetado con contexto?
    \item ¿Mejora el rendimiento de los clasificadores consumiendo información contextual?
\end{itemize}

Para responder estas preguntas, los autores consideraron las siguientes combinaciones para sus experimentos: utilizar conjunto de entrenamiento etiquetado con o sin contexto, y entrenar el clasificador con o sin contexto. Para aquellos clasificadores que no consumen contexto, los autores consideraron las mismas alternativas que hemos visto en capítulos anteriores: bi-LSTM o \bert{}. Para aquellos que sí consumen contexto, se evaluaron dos estrategias: la primera consistió en usar una única red que codifique la entrada del texto y el contexto concatenada con un token especial; la segunda consistió en usar dos codificadores distintos para el contexto y el texto. Para la segunda alternativa, y dados los recursos computacionales disponibles, no utilizaron dos modelos pre-entrenados, sino un codificador LSTM para el contexto y un \bert{} para el texto. A su vez, también utilizaron la API Perspective de Google con la misma estrategia de concatenación. Para todas las combinaciones posibles, la mejora en el rendimiento resultante de disponer de información contextual no es estadísticamente significativa.

Dos versiones de \bert{} fueron utilizadas como base para entrenar los modelos de Transformers: una, usando los pesos del modelo de \bert{} de \citet{devlin2018bert}; y la segunda, haciendo un ajuste de dominio de \bert{} sobre un dataset grande y no etiquetado relacionado a la tarea en cuestión. Este proceso de \emph{ajuste de dominio} o \emph{fine-tuning} consiste en ajustar el modelo de lenguaje sobre un conjunto de datos no etiquetados y afines a nuestra tarea final. Esta técnica ha demostrado ser efectiva para lograr mejoras sensibles en el desempeño de clasificadores sobre dominios particulares \cite{gururangan-etal-2020-dont}, y será estudiada más detenidamente en el Capítulo \ref{chap:07_domain_adaptation}. Para este trabajo, el ajuste es realizado sobre un subconjunto de comentarios del dataset de \emph{Civil Comments} \cite{borkan2019civil} sin ningún tipo de contexto. A priori, ajustar el modelo de lenguaje sobre comentarios a secas podría inducir a pensar que puede deteriorar el rendimiento al entrenar posteriormente sobre contexto; sin embargo, en la versión no adaptada de \bert{} tampoco se observó una mejora significativa en el rendimiento.

Algunas limitaciones de este trabajo marcadas por los autores son:

\begin{itemize}
    \item Contexto muy pequeño: sólo se consideraron como contexto el título de la discusión de Wikipedia Talk Pages y adicionalmente el comentario previo.
    \item El hilo completo de los comentarios es ignorado: sólo se observa el comentario previo.
    \item Los comentarios fueron muestreados aleatoriamente, sin tener en cuenta algunos ámbitos más propicios para la toxicidad.
\end{itemize}

\citet{xenos-2021-context} continuaron el trabajo de \citet{pavlopoulos2020toxicity} reetiquetando el conjunto de datos de Civil Comments con información contextual y --como mencionamos en la Sección \ref{sec:dataset_previous}-- presentando una nueva tarea de detección de sensibilidad al contexto. Usando la API Perspective (y la estrategia de concatenación básica de texto y contexto) notaron que la performance del clasificador contextualizado mejora sensiblemente si restringimos nuestra atención a comentarios más sensibles a su entorno de acuerdo a la métrica definida. De esto último puede concluirse que, a diferencia del trabajo anterior, si bien  el contexto no es realmente necesario para comprender la toxicidad del grueso de los comentarios, hay cierto subconjunto para los cuales esta información adicional resulta relevante.


\section{Tareas de clasificación propuestas}
\label{sec:tasks}

Para analizar el impacto del contexto en la detección de discurso de odio, y teniendo en cuenta que contamos de un dataset con anotaciones granulares sobre las características ofendidas, propusimos dos tareas de clasificación:

\begin{enumerate}
    \item \textbf{Detección binaria}: Dado un tweet y su contexto, predecir si es discriminatorio.
    \item \textbf{Detección granular}: Dado un tweet y su contexto, predecir las características ofendidas (de haber alguna) y si contiene un llamado a la acción.
\end{enumerate}

%%
%%
%% Link
%% https://docs.google.com/drawings/d/11sAaOuGJlU0P61mkrPxKduFwnNOuPV31tXUFJWEwbVU/edit?usp=drive_web&ouid=117313784631536396179
%%
%%
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{img/06/hate_detection_tasks.pdf}
    \caption{Tareas propuestas de detección de discurso de odio. La tarea de detección binaria consta de predecir si un tweet contiene contenido discriminatorio, discriminando la frontera conjunta de todas las características. En la tarea granular, predecimos por separado cada una de las características ofendidas, pudiendo haber más de una o bien ninguna.}
    \label{fig:hate_detection_tasks}
\end{figure}



Puede pensarse la tarea de detección binaria (la que usualmente se aborda en la literatura sobre el tema) como una relajación de la tarea granular: mientras la primera sólo nos permite detectar si hay o no contenido discriminatorio, la segunda requiere información más precisa acerca de las características ofendidas, permitiendo a su vez tener mayor información sobre la salida de los clasificadores y dando lugar a una mejor interpretación de sus errores. La Figura \ref{fig:hate_detection_tasks} ilustra las dos tareas propuestas en forma de Diagrama de Venn: mientras en la tarea binaria sólo debemos decidir de qué lado de la frontera se encuentra un comentario (si tiene o no discurso de odio) en la tarea granular se debe decidir esto mismo para cada una de las características,


Viendo las tareas propuestas como problemas de clasificación, la detección binaria consta de predecir una sola etiqueta binaria, mientras que la tarea granular consta de predecir $n$ etiquetas binarias. Esto último puede también verse como $n$ problemas distintos de clasificación, o una tarea de \tbf{multiclasificación}. Una observación sobre estos dos enfoques del problema es que podemos construir un clasificador para la tarea binaria a través de un clasificador entrenado para la tarea granular tomando la disyunción lógica de sus salidas: hay discurso de odio sí y sólo sí hay al menos una característica ofendida. Retomamos esta idea más adelante al hablar de cómo evaluamos nuestras técnicas de clasificación para cada tarea.


\begin{table}[h]
    \centering
    \begin{tabular}{l c r}
        \toprule
        Partición     & Artículos       &  Comentarios         \\
        \midrule
        Entrenamiento & \mr{2}{990}     & \num{36420}                    \\
        Desarrollo    &                 & \num{9106}                    \\
        \hline
        Test          & \num{248}       & \num{11343}           \\
        \bottomrule
    \end{tabular}
    \caption{Particiones del conjunto de datos utilizado para las dos tareas}
    \label{tab:data_splits}
\end{table}

Para las dos tareas utilizamos el conjunto de datos construido en el Capítulo \ref{chap:05_dataset_creation} separando las instancias en tres particiones: entrenamiento, desarrollo, y test. Para las dos primeras (entrenamiento y desarrollo) reservamos \num{990} artículos mientras que  para el dataset de test tomamos \num{248} artículos y sus comentarios. Ambos conjuntos de noticias son disjuntos para intentar maximizar las instancias realmente diferentes para las etapas de entrenamiento y evaluación. Sobre los primeros \num{990} artículos, dividimos el conjunto en \num{36420} instancias de entrenamiento y \num{9106} instancias de desarrollo, sin garantizar que provengan de notas periodísticas distintas.


\section{Modelos de clasificación}
\label{sec:contextualized_classifiers}


Entrenamos clasificadores neuronales basados en el modelo pre-entrenado \beto{} \cite{canete2020spanish} tanto para la tarea binaria como para la granular. Incorporamos la información contextual en cada comentario teniendo en cuenta tres tipos de entrada por instancia: el comentario sin ningún tipo de contexto (notaremos \tbf{sin contexto}), el comentario con el tweet al que responde como contexto (\tbf{tweet}), y finalmente el comentario con el tweet al que responde y el texto del artículo periodístico (\tbf{tweet + artículo}). Para las dos versiones que consumen información contextual, separamos el texto y el contexto con el token especial \septok{}.

%%
%%
%% Link a Draw
%% https://docs.google.com/drawings/d/1F8iVSIRqHhGkQ0zglxqXLGD36RHZ9OhHMZYsg_xFOS4/edit
%%
%%

\begin{figure}
    \centering
    \includegraphics[width=1.10\textwidth]{img/06/bert_contextual_classifier.pdf}
    \caption{Modelo de multiclasificación para la tarea granular basado en \beto{}. Los modelos son entrenados de tres maneras distintas: sin contexto (sólo el comentario), con el contexto del tweet, y con el contexto del tweet y el texto del artículo. La salida consta de la probabilidad de que el comentario tenga contenido odioso para alguna de las características en cuestión o bien contenga una llamada a la acción}
    \label{fig:05_multi_bert_classifier}
\end{figure}

Para la tarea binaria, la salida es la estándar para un clasificador binario. En cuanto a la tarea de detección granular, la abordamos de manera similar a la \subtaskb{} del Capítulo \ref{chap:04_hate_speech}, en este caso como la predicción de nueve variables distintas: llamado a la acción (LLAMA) y las ocho características ofendidas (MUJER, RACISMO, CLASE, LGBTI, CRIMINAL, ASPECTO, DISCAPACIDAD, POLITICA). En lugar de entrenar un clasificador diferente por cada característica, entrenamos un modelo de multiclasificación BERT que comparte todos sus pesos salvo nueve capas lineales de salida distintas.

Como función de costo, empleamos la suma de las funciones de costo de cada característica. Concretamente, si $y$ son las etiquetas de un comentario e $\widehat{y}$ las predicciones del modelo:

\begin{equation*}
    L(y, \widehat{y}) = \sum\limits_{\mathclap{c \in CHAR'}} J(y_c, \widehat{y}_c)
\end{equation*}

\noindent donde $CHAR'$ es el conjunto de todas las características protegidas junto a la variable de llamada a la acción (LLAMA), y $ J$ es la función de entropía cruzada. Compartir los pesos entre todas las salidas tiene dos objetivos: primero, poder generar un modelo más compacto (de otra forma serían nueve BERT distintos que suman alrededor de \num{1000}M parámetros) y segundo, compartir información común entre las distintas características atacadas, ya que guardan similaridades y muchas de ellas tienen una importante intersección, como hemos visto en la Sección \ref{sec:analisis_dataset_por_caracteristica}.

Para tener costos computacionales más amigables, limitamos los largos de las secuencias a 128, 256 y 512 tokens para los modelos con entrada sin contexto, tweet, y tweet+cuerpo respectivamente. Preprocesamos ambos tweets --contexto y texto-- utilizando las técnicas descriptas en la Sección \ref{sec:03_preprocessing}: conversión de usernames a un token especial (\verb|usuario|), tratamiento de hashtags (separación e inserción de un hashtag especial), y conversión de emojis a su representación textual. La Figura \ref{fig:05_multi_bert_classifier} ilustra el modelo de clasificación para la tarea granular, junto a los tres tipos de entrada considerados. La configuración de hiperparámetros utilizados en el entrenamiento es la misma que la detallada en la Sección \ref{sec:03_classification}.

\subsection{Adaptación de dominio}
\label{sec:06_domain_adaptation}

Una práctica cada vez más extendida en trabajos del área de clasificación de documentos es realizar una adaptación de dominio para mejorar la performance sobre la tarea final. La técnica de adaptación de dominio para modelos de lenguaje pre-entrenados consiste en continuar el pre-entrenamiento sobre un dataset grande y no supervisado relacionado a nuestro dominio o directamente sobre el dataset de la tarea si no tenemos acceso a otros datos \cite{gururangan-etal-2020-dont}. En la Sección \ref{sec:domain_adaptation_previous_work} del siguiente capítulo haremos una reseña más extensa de esta técnica, pero por lo pronto podemos entender que esta técnica ajusta el modelo de lenguaje a nuestros datos, ya que estos pueden tener diferencias considerables respecto de los empleados en el pre-entrenamiento. En nuestro caso puntual, mientras \beto{} fue entrenado en Wikipedia y textos formales, nuestro dominio consta de comentarios en Twitter a notas periodísticas, con expresiones muy distintas a las encontradas en medios más formales.

\begin{table}[t]
    \centering
    \begin{tabular}{lr}
        \toprule
        Hiperparámetro & Valor         \\
        \midrule
        Pasos               & \num{10000}           \\
        Tamaño de batch     & \num{2048}            \\
        Tamaño de secuencia & 128, 256 y 512  \\
        $\beta_1$           & $0.9$           \\
        $\beta_2$           & $0.98$          \\
        $\epsilon$          & $10^{-6}$       \\
        Decay               & $0.01$          \\
        LR pico             & $0.0004$     \\
        Pasos de warmup     & $0.1$             \\
        \bottomrule
    \end{tabular}
    \caption{Hiperparámetros para la adaptación de dominio de BERT}
    \label{tab:hs_ft_hyperparameter}
\end{table}


\citet{pavlopoulos2020toxicity} realizaron una adaptación de dominio sobre los comentarios del corpus de \emph{Civil Comments} sin utilizar ningún tipo de contexto. Proponemos, a diferencia de este trabajo, tres tipos de adaptaciones de acuerdo al contexto considerado: una adaptación sin contexto, una adaptación con el contexto del tweet, y una adaptación con el contexto del tweet y el cuerpo de la noticia. Los datos usados para este ajuste fueron el sobrante de la recolección del anterior capítulo: alrededor de $288$ mil artículos con $5$ millones de comentarios que no están incluídos en el conjunto de datos etiquetado \footnote{Utilizamos algunos datos extra recolectados a posteriori de lo mencionado en el capítulo anterior}.



\citet{liu2019roberta} recomienda descartar en el pre-entrenamiento de modelos de lenguajes el objetivo NSP, con lo cual realizamos nuestro ajuste de dominio exclusivamente con la tarea de modelado de lenguaje enmascarado (MLM). La Tabla \ref{tab:hs_ft_hyperparameter} contiene los hiperparámetros utilizados al correr la adaptación de dominio correspondientes al optimizador Adam con warmup lineal para el learning rate. Estos ajustes los realizamos sobre una \emph{TPU v2-8} y una máquina de \emph{Google Colab Pro}, tomando alrededor de 10 hs en su largo de cadena máximo.

\subsection{Rendimiento humano en la tarea}


\begin{table}
    \centering
    \begin{tabular}{l cc  cc}
                   & \multicolumn{2}{c}{Entre anotadores} & \multicolumn{2}{c}{Contra etiquetas} \\
        {}         &  F1 media &  F1 mediana  & F1 media  &  F1 mediana \\
        \hline
        ODIO       &  $65.3$ &   $67.5$    & $82.9$   &   $85.1$   \\
        LLAMA      &  $43.4$ &   $49.5$   &  $70.4$   &   $84.2$  \\
        \hline
        MUJER      &  $49.0$ &   $46.8$   &  $74.1$   &   $75.9$  \\
        LGBTI      &  $59.6$ &   $57.7$   &  $84.6$   &   $91.5$  \\
        RACISMO    &  $65.3$ &   $64.4$   &  $87.1$   &   $87.9$  \\
        CLASE      &  $44.3$ &   $44.4$   &  $72.2$   &   $73.2$  \\
        POLITICA   &  $46.1$ &   $43.6$   &  $79.5$   &   $81.5$  \\
        DISCAPACIDAD& $55.0$ &   $60.0$   &  $81.3$   &   $84.2$  \\
        APARIENCIA &  $64.9$ &   $74.3$   &  $83.1$   &   $91.5$  \\
        CRIMINAL   &  $52.7$ &   $58.0$   &  $84.1$   &   $92.9$  \\
        \hline
        Macro F1   &  $53.4$ &   $55.4$   &  $79.6$   &   $84.8$  \\
        \hline
    \end{tabular}

    \caption{Estadísticos del F1 (en porcentaje) entre anotadores. Las dos primeras columnas marcan las métricas medidas entre anotadores, y las dos últimas la de los anotadores contra las etiquetas asignadas en el conjunto de datos. La Macro F1 es el promedio de los F1 todas las características y de la llamada a la acción (LLAMA). }
    \label{tab:ia_f1_scores}
\end{table}


La tarea de detección de lenguaje discriminatorio contiene una alta cantidad de ruido y un bajo acuerdo entre humanos, como se atestigua en recientes revisiones de los conjuntos de datos generados para su clasificación \cite{poletto2021resources}. En este escenario, cabe preguntarse una cota al rendimiento que puede lograr un algoritmo de detección automática para esta tarea, algo que esperamos --por la misma naturaleza del problema-- que diste mucho de la perfección. Si bien en muchos trabajos se ha logrado que algoritmos basados en Transformers superen el rendimiento humano para distintas tareas de NLP \footnote{Algo discutible dado que el rendimiento humano para estos benchmarks --como GLUE-- está medido a través de trabajadores contratados a través de crowdsourcing con pagas por debajo del salario mínimo}, consideramos el desempeño de nuestros anotadores como una cota superior razonable para las técnicas automáticas que hemos propuesto.

Para obtener números indicativos del rendimiento humano en la detección de discurso de odio, utilizamos la información recolectada durante la anotación del conjunto de datos. Consideramos las anotaciones de cada uno de los seis participantes en el proceso como predicciones y las evaluamos de dos formas: la primera tomando como etiquetas doradas las anotaciones de otro anotador; la segunda, tomando las etiquetas generadas en la Sección \ref{sec:asignacion}. En el primer caso tenemos $\binom{6}{2}=15$ combinaciones, mientras que en la segunda tenemos una para cada anotador, 6 combinaciones. Para estas dos formas de calcular rendimientos humanos, calculamos la medida \emph{F1} entre las predicciones de cada anotador y las etiquetas doradas, y calculamos medias y medianas para obtener estimaciones puntuales de los rendimientos.

Algo a tener en cuenta en la comparación contra las etiquetas del conjunto de datos es que este \emph{gold standard} es construido mediante votación mayoritaria de los dos o tres anotadores empleados, por lo cual estamos calculando la métrica entre dos variables correlacionadas. Mientras esta cota por un lado es muy grosera, por el otro, las métricas calculadas entre anotadores pueden ser algo bajas debido a que son predicciones muy ruidosas a diferencia de las etiquetas doradas que son más robustas al estar generadas por varios usuarios. El mejor escenario para estimar el rendimiento hubiera sido contar con una anotación extra para cada instancia y compararla contra la etiqueta dorada, aunque esta metodología es poco eficiente en términos de recursos.

La Tabla \ref{tab:ia_f1_scores} contiene las medias y medianas del puntaje F1 tanto entre anotadores como contra el \emph{gold-standard}. La mediana entre anotadores de la F1 es $67.5$, un puntaje relativamente bajo para la detección de odio, mientras que contra el gold standard es de $85.1$ puntos de F1; podemos suponer que la performance humana para la tarea se encuentra en algún lugar entre esos dos números. Respecto a las características, el rendimiento para su reconocimiento entre humanos en algunas de ellas se mantiene muy bajo, particularmente en MUJER, CLASE y POLITICA. Esto se desprende de las observaciones y dificultades descriptas en el Capítulo \ref{chap:05_dataset_creation} durante el proceso de anotación, como así también del hecho que estas características tienen un solapamiento no menor entre sí.



\section{Resultados}
\label{sec:06_results}
\input{src/06_results.tex}

\section{Comparación de clasificadores y análisis de error}

\input{src/06_error_analysis.tex}



\section{Discusión}

Para analizar el impacto del contexto en la detección de discurso de odio, planteamos dos tareas de clasificación sobre el conjunto de datos construido en el Capítulo \ref{chap:05_dataset_creation}: la tarea de detección binaria, donde predecimos si un comentario contiene discurso de odio; y la tarea de detección granular, donde se deben predecir las características protegidas ofendidas (si agrede a las mujeres, al colectivo LGBTI, si es racista, etc). Propusimos clasificadores basados en \beto{} que consumieron tres tipos de entrada: el comentario sin contexto, el comentario con el contexto del tweet al que responden, y por último el comentario más el tweet al que responde junto al texto del artículo periodístico.

Para ambas tareas, pudimos observar en los experimentos realizados que el contexto en su versión simple (sólo el tweet de la noticia) brinda una mejora moderada pero estadísticamente significativa en la tarea de detección binaria de discurso de odio (alrededor de 3 puntos F1) y una mejora considerable en la tarea granular (alrededor de 6 puntos de Macro F1) contra los clasificadores que no tienen información contextual. Esto indicaría que el contexto puede ser aprovechado para mejorar los algoritmos de detección de discurso de odio. Si bien este resultado podría estar en aparente contradicción con trabajos recientes que no encontraron ninguna mejora en el uso del contexto en la detección de toxicidad \cite{pavlopoulos2020toxicity}, se puede señalar que la detección del discurso de odio es una de las formas más complejas de este comportamiento. Como tal, el contexto podría permitir --para este subconjunto de contenido tóxico-- que los clasificadores tengan más información para predecir si el texto dado es discriminatorio o no. Otra razón detrás de este resultado es el dominio de nuestro conjunto de datos: mientras que \citet{pavlopoulos2020toxicity} usaron el título de un thread de Wikipedia Talk Pages y parcialmente el hilo conversacional, nosotros sólo usamos el título y el cuerpo del artículo como contexto para los comentarios de los usuarios. Más recientemente, \citet{xenos-2021-context} han observado que los algoritmos de detección de toxicidad pueden aprovechar esta información adicional al restringir el análisis a un subconjunto de comentarios sensibles al contexto (ver Secciones \ref{sec:06_classification_previous} y \ref{sec:dataset_previous} para más información).

En nuestros experimentos, la utilización de un contexto más largo (el artículo de la noticia) no reportó mejoras en el rendimiento de los clasificadores. Este resultado sería compatible con el hecho de que los respuestas de los usuarios a artículos periodísticos suelen estar basadas en poquísima información como el titular o en este caso el tweet acerca de la noticia. Sin embargo, los humanos solemos tener acceso a un contexto mucho más rico --muchas veces equivalente a haber leído la nota-- y también a conocimiento adicional del mundo real, algo que se ha mostrado que los modelos pre-entrenados de lenguaje carecen \cite{logan-2019-baracks,bender2021dangers}. Una posible razón adicional a esto puede ser atribuido a que el modelo pre-entrenado que usamos para codificar el texto del artículo (\beto{}) fue pre-entrenado sobre textos más cortos. Teniendo esto en cuenta, realizamos el ajuste de dominio usando los textos de artículos periodísticos, pero aún así el desempeño del clasificador que consume esta entrada se mantuvo por debajo del que consumía sólo el tweet original. Trabajo futuro debería estudiar formas de incorporar esta información de manera que pueda ser utilizada adecuadamente por los modelos.

El análisis del error realizado dio muestras de que la tarea de detección de discurso de odio es difícil para muchas características, aún considerando la mejora que reporta la utilización de contexto. Un caso notable observado en nuestros experimentos es la discriminación contra el colectivo LGBTI. En las instancias del dataset --y en muchos de los comentarios en los que el algoritmo de detección falla--  puede verse que las agresiones contra este colectivo y sus miembros son sumamente sofisticadas, lejos de las agresiones meramente basadas en palabras o expresiones insultantes. Los clasificadores entrenados, aún en sus mejores versiones, obtuvieron una baja performance en la detección de este fenómeno (alrededor de $48$ puntos de F1) comparada con la tasa de detección humana (casi $60$ puntos). Estos resultados marcan la no trivialidad de esta tarea, indicando la necesidad de analizar este punto más detenidamente debido a la complejidad de estos mensajes, que suelen reunir ironía, metáforas, y otros artilugios que hacen difícil su detección.

Una particularidad de un subconjunto considerable de comentarios homofóbicos y transfóbicos en noticias periodísticas es que suelen ser dirigidos contra un individuo integrante de la comunidad LGBTI. Esta información --su pertenencia al colectivo-- no está en todos los casos disponible en el contexto de la noticia o no puede llegar a ser captado por nuestros modelos, hecho que dificulta la detección de este tipo de agresiones. Este problema es generalizable a otras características protegidas además de LGBTI, y puede también marcar una posibilidad de mejora con la incorporación de conocimiento externo a los modelos (como es propuesto por \citet{liu2020kbert}) para ayudar a los clasificadores a mejorar su tasa de detección.

En el caso de la detección de agresiones misóginas (MUJER) obtuvimos también tasas de desempeño muy bajas. Analizando los errores, pudimos observar que tenemos casos complejos de descifrar en los datos como, por ejemplo, ataques velados a mujeres víctimas de violación (llamarlas mentirosas). Estos casos caen en las agresiones individualizadas recién descriptas, las cuales pueden requerir conocimiento adicional del mundo real. De todas maneras, la baja tasa de acuerdo entre humanos pone una cota baja al desempeño máximo de los algoritmos automáticos.

Algo que debe ser tenido en cuenta para matizar estos resultados es que utilizamos un amplio espectro de características protegidas. Incluso, la característica más beneficiada por la adición del contexto es CRIMINAL, una categoría que puede ser considerada ad-hoc para este experimento. En contrapeso, otras características no convencionales son poco beneficiadas por el contexto (como discurso de odio en base a la apariencia, opinión política y discapacidad). Otro subtipo de discurso de odio muy beneficiado por la adición de contexto es el dirigido contra la comunidad LGBTI, a pesar de las dificultades señaladas en su detección.

En los experimentos observamos que los clasificadores mejoran levemente su performance en términos de detección de discurso de odio al ser entrenados para la tarea granular. Si bien la mejora es marginal (cerca de un punto de F1) y no es apreciable de manera subjetiva mediante un análisis de error, una posible razón detrás de esto es que la señal más precisa acerca de la categoría ofendida puede ayudar a distinguir mejor las fronteras de este fenómeno. Este resultado es coherente con lo observado en la Sección \ref{sec:04_results}, donde la adición de otras variables a predecir no empeoraban el desempeño de los modelos, y con \citet{gertner-etal-2019-mitre} donde se reporta una mejora al modelar --de manera no supervisada-- las características ofendidas. De todas formas, aún cuando no existiese mejora en la detección, poder tener una salida más interpretable y granular es mejor que simplemente obtener una predicción binaria.

Una limitación importante de este estudio es que todos los modelos de clasificación fueron entrenados sobre datos etiquetados observando el contexto. Algunos ejemplos de errores que observamos en el clasificador no contextualizado --celebraciones marcadas como discurso de odio-- dan cuenta del problema que esto genera. Un estudio más completo del impacto del contexto debería incluir datos que sean etiquetados sin observar ningún tipo de contexto como es realizado en \citet{pavlopoulos2020toxicity}, algo que por limitaciones de tiempo y recursos no fue posible realizar en esta tesis.

En el terreno de la aplicación, un problema práctico de este resultado es que no siempre tenemos un contexto disponible para un texto dado. Incluso si podemos encontrarlo, muchas veces este contexto puede no ser en forma de artículo de noticias sino como un hilo de conversación o incluso de alguna otra representación. Teniendo en cuenta alguna de las consideraciones hechas en esta discusión, una línea de investigación podría evaluar la incorporación de distintos tipos de contexto, desde más mensajes en el hilo de la conversación, conocimiento estructurado que consuma algún modelo \emph{knowledge-aware} (\emph{ERNIE} \cite{zhang2019ernie} o variantes de BERT que inyectan extractos de grafos de conocimiento \cite{liu2020kbert,faldu2021ki}) o bien una combinación de diversas fuentes de información.

\section{Conclusiones}

Hemos analizado aquí el impacto de la utilización del contexto en la detección automática de discurso de odio, realizando para ello experimentos de clasificación sobre el conjunto de datos construido en el capítulo anterior. Los resultados de estos experimentos dan indicios de que cierta información contextual puede ser de ayuda para mejorar la capacidad de detección de estos algoritmos. Si bien en nuestros experimentos el contexto más pequeño (el tweet del artículo de la noticia) fue el que mejor resultados obtuvo, una línea de trabajo futuro podría explorar formas de incorporar otras fuentes de información.

%Así mismo, observamos una pequeña pero estadísticamente significativa mejora en la detección de discurso de odio al entrenar un clasificador granular al ser evaluado de manera binaria. En este caso, obtenemos una ventaja al obtener una salida más interpretable de las características ofendidas, y que además que no sólo no empeora la performance de nuestro clasificador sino que hasta incluso mejora levemente.

Del análisis de error, se puede apreciar que algunas categorías del discurso de odio se muestran esquivas para los algoritmos de detección del estado del arte. Uno de estos casos son los mensajes abusivos contra la comunidad LGBTI, que contienen mensajes semánticamente complejos, con carga irónica y metáforas que son difícilmente interpretables para los clasificadores basados en modelos de lenguaje del estado del arte. A pesar de estas limitaciones, la detección de discurso de odio contra la comunidad LGBTI fue una de las más beneficiadas por la adición de contexto.

Podemos concluir que los datasets de discurso de odio deberían --en la medida de lo posible-- contener \textbf{información contextual} sobre los comentarios analizados. Esta información puede darse en forma de artículos de noticias, como un hilo de conversación, o incluso como otras formas --por ejemplo, como una base o grafo de conocimiento. Sobre esto, trabajo futuro debería explorar el impacto de utilizar esta información adicional para integrarla en algoritmos de detección de discurso de odio. La evidencia de los experimentos realizados --por ahora preliminares, y con las limitaciones marcadas en la discusión-- indica que los modelos del estado del arte pueden utilizar esta información para mejorar la detección de discurso de odio en redes sociales. En segundo lugar, los datasets de discurso de odio deberían incluir \textbf{información granular} acerca de las características atacadas --y no sólo una etiqueta binaria-- ya que por un lado esto mejora la interpretabilidad de los algoritmos de detección, y, por el otro, resultados preliminares de este estudio indican que utilizar información detallada de las característica ofendidas mejora marginalmente el rendimiento en la detección en general.

Finalmente, un aspecto que introdujimos en este capítulo fue el de adaptar un modelo de lenguaje pre-entrenado a su dominio, siendo en nuestro caso los comentarios sobre notas periodísticas en Twitter. Las mejoras que reportó la utilización de estas técnicas fueron considerables, en consonancia con otros trabajos recientes. Pasaremos a continuación a estudiar estas técnicas en el marco más general de la clasificación de textos sociales.