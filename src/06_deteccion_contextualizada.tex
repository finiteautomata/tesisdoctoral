
En base a las discusiones del capítulo anterior, abordaremos una de las preguntas centrales de la tesis: ¿puede ayudar el contexto a mejorar la performance de métodos automáticos de lenguaje de odio? Para intentar contestar esta pregunta, utilizaremos el dataset que construímos en el capítulo \ref{chap:dataset_creation}, y aplicaremos técnicas del estado del arte basadas en transformers. Exploraremos dos versiones de clasificadores: descontextualizadas, donde sólo observamos el comentario analizado; y contextualizadas, donde podemos consumir el título o el título y el cuerpo del artículo.

Proponemos en este capítulo dos tareas: detección ``binaria'', donde sólo predecimos la característica de si hay o no discurso de odio; y la detección ``granular'', donde además predecimos todas las características ofendidas (potencialmente más de una). Analizaremos el impacto de agregar contexto para cada una de estos problemas de clasificación, y también si tener información granular nos ayuda a poder detectar mejor el discurso de odio, con mejores ``fronteras'' entre cada una de las características ofendidas.


\section{Trabajos previos}
\label{sec:06_classification_previous}

Como contamos en la anterior sección, son pocos los trabajos y datasets que poseen contexto. Ver \ref{sec:dataset_previous} para un repaso de los distintos datasets contextualizados.

\citet{gao-huang-2017-detecting} propone dos tipos de modelos: regresiones logísticas y redes neuronales recurrentes. Para los modelos de regresiones logísticas, usan como inputs bolsas de palabras, bolsas de caracteres, vectores semánticos producidos con Linguistic Inquiry and Word Count (LIWC) \cite{pennebaker2001linguistic} y features de un lexicon de emociones \cite{mohammad2013nrc}. Por otro lado, utiliza LSTM bidireccionales con mecanismo de atención de Bahdanau \cite{bahdanau2014neural} usando embeddings \emph{word2Vec} de dimensión 100.

Un punto criticable de este trabajo es que utiliza el nombre de usuario como feature; algo que a priori no suele hacerse ya que permitiría ``prejuzgar'' a un usuario antes que por el contenido de sus tweets. Si bien es cierto que la información de usuarios y sus conexiones es valiosa, introducir esta información a nuestros modelos da lugar a posibles correlaciones espurias que es preferible evitar.


\begin{figure*}[t]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{img/pavlopoulos_rnn_rnn_classifier.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{img/pavlopoulos_rnn_bert_classifier.png}
    \end{minipage}

    \begin{minipage}[b]{0.35\textwidth}
        \includegraphics[width=\textwidth]{img/pavlopoulos_bert_sep_classifier.png}
    \end{minipage}


    \caption{Clasificadores que consumen contexto propuestos por \citet{pavlopoulos2020toxicity}. Los dos primeros clasificadores proponen una arquitectura de dos encoders, uno para el texto y otro para el contexto usando bi-LSTMs y BERT como posibilidades. El tercer clasificador propuesto es un BERT usando su estructura natural para codificar dos oraciones separadas por el token $SEP$ }
    \label{fig:pavlopoulos_classifiers}
\end{figure*}


En la sección \ref{sec:dataset_previous} hemos descripto el dataset construído por \citet{pavlopoulos2020toxicity}. Nos detendremos un momento para analizar sus experimentos de clasificación  ya que guardan importantes similaridades con lo que haremos en este capítulo. En ese trabajo se obtuvieron dos datasets de entrenamiento, uno etiquetado viendo el contexto y otro sin verlo. El dataset de test fue etiquetado viendo el contexto, considerando que el etiquetado es de mejor calidad usando más información. Tenemos entonces, dos preguntas: ¿mejora la performance de la tarea usando el dataset etiquetado con contexto? ¿mejora la performance del clasificador consumiendo información contextual? Estas dos preguntas nos brinda entonces dos elecciones: dataset de entrenamiento con o sin contexto, y clasificador con o sin contexto. Tenemos 4 posibles combinaciones de experimentos, sin aún considerar posibles técnicas de clasificación.

Para cada una de estas combinaciones, se consideraron técnicas del estado del arte de clasificación. Para aquellos clasificadores que no consumen contexto, las opciones son las mismas que hemos visto en capítulos anteriores: bi-LSTM o BERT. Para aquellos que sí consumen contexto, se evaluan dos estrategias: una, concatenar con algún caracter, y otra usando dos encoders distintos para el contexto y el texto. A su vez también utilizan la API Perspective de Google \todo{Agregar alguna cita de esto, y quizás alguna explicación en capítulo 4}.

Para todas las combinaciones posibles, si bien hay una mejora en la performance medida con ROC-AUC al usar contexto en ambas formas, esta no es significativa. De esto los autores concluyen que no pueden encontrar evidencia suficiente sobre la utilidad del contexto en la detección de toxicidad.

Algo a mencionar (que retomaremos en este y en el siguiente capítulo) es que usan dos versiones de BERT: una usando los pesos del modelo de BERT, y otro haciendo un ajuste de dominio () corriendo la tarea de MLM sobre un dataset grande y no etiquetado. En el caso de el trabajo mencionado, sólo hacen un fine-tuning sobre comentarios sueltos del dataset de Civil Comments. Esto podría tener algún efecto deteriorando la performance al usar contexto; sin embargo, en el BERT a secas (sin hacer ajustes) tampoco se observa mejora significativa en la performance.

Algunas limitaciones marcadas por los autores son:

\begin{itemize}
    \item Contexto muy pequeño: sólo el título más el comentario previo
    \item Se ignora el hilo completo de comentarios
    \item Los datos fueron sampleados aleatoriamente
\end{itemize}

En \citet{xenos-2021-context}, continuación de este trabajo, reetiquetaron el dataset de Civil Comments usando contexto y --como mencionamos en la sección \ref{sec:dataset_previous}-- presentaron una nueva tarea de detección de sensibilidad al contexto. Usando la API Perspective (y la estrategia de concatenación ``básica'' con algún caracter), notaron que la performance del clasificador que consume el contexto mejora con respecto al que no lo hace a medida que restringimos nuestra atención a comentarios más ``sensibles al contexto'' (de acuerdo a la métrica definida por los autores)


\section{Tareas de clasificación propuestas}



Ahora que tenemos este corpus especialmente diseñado que contiene el contexto, ahora dirigimos nuestra atención a responder nuestra pregunta original: ¿pueden los clasificadores aprovechar el contexto para mejorar su desempeño en la tarea de detección del discurso discriminatorio? Para ello, proponemos las siguientes tareas de clasificación:

\begin{enumerate}
    \item \textbf{Detección ``binaria''}: Dado un tweet y su contexto, predecir si contiene contenido discriminatorio
    \item \textbf{Detección ``granular''}\footnote{En inglés usamos la denominación \emph{fine-grained}, aunque no hay una traducción precisa para este término en español}: Dado un tweet y su contexto, predecir las características ofendidas (si hay alguna) y si contiene un llamado a la acción
\end{enumerate}


Puede pensarse la tarea de detección binaria (la que usualmente se aborda en la literatura sobre el tema) como una relajación de la tarea detallada: mientras la primera sólo nos permite detectar si hay o no contenido discriminatorio, la segunda nos requiere información más precisa acerca de las características ofendidas. Estas segunda tarea es posible dado que el dataset que construímos contiene esta información, algo usualmente faltante en otros datasets.


%%
%%
%% Link
%% https://docs.google.com/drawings/d/11sAaOuGJlU0P61mkrPxKduFwnNOuPV31tXUFJWEwbVU/edit?usp=drive_web&ouid=117313784631536396179
%%
%%
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{img/hate_detection_tasks.pdf}
    \caption{Tareas propuestas de detección}
    \label{fig:hate_detection_tasks}
\end{figure}

La tarea en su versión fina nos permite a su vez tener mayor entendimiento de la salida e interpretar mejor sus errores. La figura \ref{fig:hate_detection_tasks} ilustra las dos tareas propuestas. Mientras en la tarea binaria sólo debemos decidir la frontera entre si el contenido es discriminatorio o no, en la tarea granular necesitamos decir en cuál de todas las intersecciones está el comentario y su contexto.

Planteándolos como problemas de clasificación, la detección binaria consta de predecir una sola etiqueta binaria, mientras que la tarea granular consta de $n$ etiquetas binarias; es decir, $n$ problemas distintos de clasificación. Vale mencionar que, entendiendo una tarea como una relajación de la otra, si tenemos un clasificador entrenado para la tarea granular podemos construir un clasificador para la tarea binaria tomando la disyunción lógica de sus salidas; o dicho más coloquialmente, poniendo una compuerta OR sobre la salida del clasificador granular. Retomaremos esta idea más adelante al hablar de cómo evaluamos nuestras técnicas de clasificación para cada tarea.




\subsection{Performance humana de la tarea}


Como observamos en la anterior sección, la tarea de detección de lenguaje discriminatorio contiene una alta cantidad de ruido, y el acuerdo entre humanos es moderado. En este contexto, cabe preguntarse una cota a la performance que puede lograr un clasificador para esta tarea. Por la misma naturaleza del problema, claramente no puede ser perfecta. Para obtener algunas medidas de esto, calculamos en primer lugar las F1 usando todos los posibles pares de anotadores. Como la F1 es simétrica (invirtiendo roles se invierten la precisión y la sensibilidad) no necesitamos hacer ninguna asunción sobre cuál sus roles.

Algo a tener en cuenta es que nuestra métrica final será contra la etiqueta resultante del (nuestro \emph{gold standard}). Una cota que seguro está por arriba de nuestra performance es el acuerdo que haya entre los anotadores y este \emph{gold standard}; hay que también observar que cada etiqueta ``de oro'' codifica información de sus anotaciones, con lo cual éste número es una cota superior sin dudas pero también pueden ser demasiado ``gruesa''.

\begin{table}
    \centering
    \begin{tabular}{lll|ll}
        \hline
                   & \multicolumn{2}{c}{Entre anotadores} & \multicolumn{2}{c}{Gold} \\
        {}         &  F1 mean&  F1 median  & F1 Mean  &  F1 Median \\
        \hline
        HATEFUL    &  0.6525 &   0.6751    & 0.8285   &   0.8515   \\
        \hline
        CALLS      &  0.4341 &   0.4950   &  0.7042   &   0.8424  \\
        WOMEN      &  0.4896 &   0.4676   &  0.7406   &   0.7593  \\
        LGBTI      &  0.5959 &   0.5765   &  0.8462   &   0.9152  \\
        RACISM     &  0.6532 &   0.6444   &  0.8712   &   0.8789  \\
        CLASS      &  0.4431 &   0.4444   &  0.7220   &   0.7317  \\
        POLITICS   &  0.4609 &   0.4360   &  0.7951   &   0.8155  \\
        DISABLED   &  0.5502 &   0.6000   &  0.8127   &   0.8421  \\
        APPEARANCE &  0.6485 &   0.7428   &  0.8314   &   0.9146  \\
        CRIMINAL   &  0.5265 &   0.5801   &  0.8415   &   0.9292  \\
        \hline
        Macro F1   &  0.5336 &   0.5541   &  0.7961   &   0.8477  \\
        \bottomrule
    \end{tabular}

    \caption{Estadísticos de los cálculos de F1 entre anotadores. Cada característica es tomada como una etiqueta binaria independientemente del cálculo de la métrica para discurso de odio. La Macro F1 es el promedio de los F1 todas las características y la F1 de la llamada a la acción (CALLS). Las dos primeras columnas marcan las métricas medidas entre anotadores, y las dos últimas la de los anotadores contra la etiqueta gold}
    \label{tab:ia_f1_scores}
\end{table}


La tabla \ref{tab:ia_f1_scores} contiene estadísticos de estos cálculos, tanto entre anotadores como contra el \emph{gold-standard}. Como podemos observar, la mediana entre anotadores de la F1 (usada para obviar outliers) es relativamente baja para la detección de odio ($\sim 0.67$), mientras que contra el gold standard es de $0.85$. De esto entendemos que la performance máxima en la detección está entre esos dos números.

\subsection{Preprocesamiento}

Para cada tweet, aplicamos el mismo procesamiento descripto en secciones anteriores: primero, cortamos las repeticiones de caracteres hasta tres ocurrencias; normalizamos las risas; convertimos los identificadores de usuario en un token especial \verb|@usuario| ; convertimos emojis en una representación textual usando la biblioteca de python \emph{emoji} \footnote {\url{https://github.com/carpedm20/emoji/}}. Los hashtags se eliminan, están rodeados por una ficha especial \emph{hashtag } y se dividen en palabras si están en mayúsculas.

Aunque no realizamos un análisis de ablación para evaluar el impacto de cada paso del preprocesamiento, el proceso general pareció mejorar el rendimiento de la clasificación en el conjunto de datos de desarrollo.

\subsection{Clasificadores}
\label{sec:contextualized_classifiers}

%%
%%
%% Link a Draw
%% https://docs.google.com/drawings/d/1F8iVSIRqHhGkQ0zglxqXLGD36RHZ9OhHMZYsg_xFOS4/edit
%%
%%



Proponemos varios clasificadores, todos basados en \beto{}\cite{canete2020spanish}, una versión en español de \emph{BERT} \cite{devlin2018bert}. Para cada tarea, proponemos tres tipos de entrada por instancia: el comentario sin ningún tipo de contexto, con contexto simple (el tweet al que responde), y con contexto largo (el tweet al que responde + el texto del artículo periodístico).



\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{img/bert_multioutput.pdf}
    \caption{Muestra de la recolección de datos}
    \label{fig:bert_classifier}
\end{figure}

Para la tarea de detección del discurso de odio, presentamos versiones sin contexto y conscientes del contexto, utilizando el título y el cuerpo completo como posibles contextos. Usamos el token especial BERT \emph {[SEP]} para codificar el contexto y el texto analizado. Recuerde que el token \emph {[SEP]} se usa para la tarea de predicción de la siguiente oración (tarea NSP) en el preentrenamiento al estilo BERT.


En cuanto a la detección de características ofendidas, consideramos este problema como una tarea de clasificación multibinaria; es decir, dado un texto odioso y una característica protegida, lo consideramos como una tarea de clasificación binaria para predecir si el texto ofende la característica respectiva. En lugar de entrenar un clasificador diferente para cada característica, entrenamos un BERT de múltiples salidas, compartiendo todos sus pesos con la excepción de 9 capas lineales diferentes para cada salida. La pérdida utilizada es

\begin{equation*}
    J = \sum\limits_{c \in CHAR \cup \{CALLS\}} J_c
\end{equation*}

donde $CHAR$ es el conjunto de todas las características protegidas (MUJERES, LGBTI, RACISMO, CLASE, etc.) y $CALLS$ es `` llamadas a la acción ''. $ J_c $, ya que cualquier $ c $ es una pérdida de entropía cruzada binaria.

Para tener costos computacionales más amigables, limitamos nuestras secuencias a 128, 256 y 512 tokens para el modelo no contextualizado, el modelo de título y el modelo de título y cuerpo, respectivamente.

Una práctica cada vez más extendida en trabajos del área de clasificación de documentos es realizar una adaptación de dominio sobre textos relacionados a nuestro problema; en nuestro caso, al dominio de redes sociales. Esto se realiza corriendo la tarea de MLM (ver sección YYY) sobre un dataset grande y no supervisado relacionado a nuestro dominio, o directamente sobre el dataset de la tarea si esto no está disponible. En la sección \ref{sec:domain_adaptation_previous_work} del siguiente capítulo haremos una reseña de esta técnica, pero por lo pronto podemos entender que ajusta nuestro ``conocimiento'' a los textos disponibles (en este caso, BETO entrenado en Wikipedia y noticias es ajustado a un dominio de tweets).

Para lo que nos concierne en esta sección, \citet{pavlopoulos2020toxicity} realizó una adaptación de dominio sobre los comentarios del corpus de \emph{Civil Comments} (a lo que denomina BERT-CCTK), sin realizar una adaptación sobre textos con contexto. Proponemos, entonces, y a diferencia de este trabajo que tomamos como referencia, 3 tipos de adaptaciones: una adaptación sin contexto, una adaptación con el contexto del tweet, y una adaptación con el contexto del tweet y el cuerpo de la noticia.
\todo{agregar secciones}

Realizamos el ajuste utilizando el sobrante de la recolección de datos del anterior capítulo: alrededor de 288k artículos y 5.1M comentarios\footnote{Utilizamos algunos datos extra recolectados a posteriori de lo mencionado en el capítulo anterior}. La tabla \ref{tab:hs_ft_hyperparameter} contiene los hiperparámetros utilizados. Para correr estos experimentos, las corrimos sobre una TPU v2-8 y una máquina de Google Colab Pro por alrededor de 10 hs (en su largo de cadena máximo).

\begin{table}[t]
    \centering
    \begin{tabular}{ll}
        \toprule
        Hiperparámetro & Valor         \\
        \midrule
        Steps          & $10K$           \\
        Batch size     & 2048            \\
        max seq length & 128, 256 y 512  \\
        $\beta_1$      & $0.9$           \\
        $\beta_2$      & $0.98$          \\
        $\epsilon$     & $10^{-6}$       \\
        decay          & $0.01$          \\
        Peak LR        & $4*10^{-4}$     \\
        warmup ratio   & 0.1             \\
        \bottomrule
    \end{tabular}
    \caption{Hiperparámetros para la adaptación de dominio de BERT}
    \label{tab:hs_ft_hyperparameter}
\end{table}


\section{Resultados}


\begin{table*}
    \centering
    \begin{tabular}{l |ll  | ll | ll}
        Métrica        &\mc{2}{Sin Contexto}& \mc{2}{Tweet}          &  \mc{2}{Tweet + Cuerpo}    \\
                                 & $\neg$FT&  FT    & $\neg$FT&    FT  & $\neg$ FT&    FT \\
                       accuracy  & 0.889   &  0.899 & 0.902 &\textbf{0.910} & 0.904 &    0.905 \\
                       precision & 0.678   &  0.718 & 0.731 &\textbf{0.748} & 0.739 &    0.728 \\
                       recall    & 0.568   &  0.602 & 0.601 &\textbf{0.653} & 0.611 &    0.641 \\
                       F1        & 0.618   &  0.655 & 0.660 &\textbf{0.697} & 0.669 &    0.681 \\
                       Macro  f1 & 0.776   &  0.798 & 0.801 &\textbf{0.822} & 0.806 &    0.813 \\
        \bottomrule
    \end{tabular}


    \caption{Resultados de los experimentos de clasificación para la tarea \emph{binaria} de detección de discurso de odio. Cada modelo es un BERT con 3 posibles entradas: sólo el comentario (\emph{Sin contexto}), el tweet de la noticia a la cual responde el comentario (\emph{Tweet}), y el tweet más el cuerpo de la noticia (\emph{Tweet + Cuerpo}). Para cada una de estas posibilidades usamos dos versiones: una sobre BETO($\neg$FT) y otra sobre BETO ajustado al dominio (FT) de acuerdo a lo descripto en la sección \ref{sec:contextualized_classifiers}}
    \label{tab:task_a_results}
\end{table*}


La tabla \ref{tab:task_a_results} contiene los resultados de la tarea de clasificación binaria, medidos por accuracy, precisión, recall, F1 de la clase positiva y Macro F1 entre las dos clases. Tenemos 3 posibles modelos dependiendo del contexto utilizado, y para cada uno dos posibilidades de acuerdo si ajustamos al dominio o no. Podemos observar que, en todos los casos, la adaptación de dominio (las columnas marcadas con FT) obtienen mejor performance que los modelos que no están adaptados ($\neg$FT), resultando en los casos sin contexto y con contexto de tweet en una mejora de alrededor de 4 puntos de F1. Entre los modelos sin ajustar a dominio, el modelo que consume el contexto completo (tweet + cuerpo de la noticia) obtiene el mejor desempeño; sin embargo, el contexto simple mejora esta performance cuando es adaptado.

Con respecto al contexto, el modelo que consume el contexto simple y está ajustado a dominio (columna Tweet y FT) obtiene la mejor performance. La mejora contra el modelo que no consume contexto es de 4.2 puntos de F1. El modelo con el contexto completo, si bien mejora la performance general contra no tener contexto, pierde precisión al ser adaptado al dominio.

que agregar contexto parece mejorar el desempeño; en particular, agregar el texto parece proporcionar suficiente contexto para la tarea de detección del discurso de odio. Agregar el cuerpo mejora marginalmente el rendimiento, pero a un costo computacional más alto (recuerde que la longitud máxima con título se establece en 256, y en el otro caso es 512). La mejora en la puntuación de F1 con solo agregar el título es de aproximadamente 3 puntos; título y cuerpo suma alrededor de 3,5 puntos F1 sobre la clasificación no contextualizada. Al analizar los modelos contextuales, el cuerpo completo parece mejorar el recuerdo al tiempo que disminuye ligeramente la precisión, con una puntuación general igual de F1 que el modelo de solo título.

\begin{table*}
    \centering
    \begin{tabular}{l |ll  | ll | ll}
        Métrica        &\mc{2}{Sin Contexto}& \mc{2}{Tweet}          &  \mc{2}{Tweet + Cuerpo}    \\
                       & $\neg$FT&    FT    & $\neg$FT   &    FT     & $\neg$ FT&    FT     \\
        \hline
        Calls F1       & 0.646 &    0.651   & 0.638 &\textbf{0.685}  & 0.653 &    0.680    \\
        Women F1       & 0.373 &    0.389   & 0.411 &\textbf{0.421}  & 0.381 &\textbf{0.421} \\
        Lgbti F1       & 0.351 &    0.366   & 0.451 &\textbf{0.482}  & 0.427 &    0.445    \\
        Racism F1      & 0.635 &    0.653   & 0.688 &\textbf{0.720}  & 0.691 &    0.711    \\
        Class F1       & 0.401 &    0.433   & 0.491 &\textbf{0.511}  & 0.451 &    0.476    \\
        Politics F1    & 0.555 &    0.611   & 0.579 &0.625           & 0.591 &\textbf{0.648} \\
        Disabled F1    & 0.551 &    0.582   & 0.585 &\textbf{0.609}  & 0.557 &    0.578    \\
        Appearance F1  & 0.726 &    0.742   & 0.741 &\textbf{0.766}  & 0.755 &    0.758    \\
        Criminal F1    & 0.513 &    0.529   & 0.650 &\textbf{0.699}  & 0.654 &    0.668    \\
        \hline
        Macro F1       & 0.528 &    0.551   & 0.582 &\textbf{0.613}  & 0.573 &    0.598    \\
        Macro Precision& 0.558 &    0.630   & 0.642 &\textbf{0.702}  & 0.677 &    0.678    \\
        Macro Recall   & 0.506 &    0.499   & 0.540 &\textbf{0.551}  & 0.504 &    0.541    \\
        \hline
        Hate Precision & 0.679 &    0.712   & 0.722 &\textbf{0.760}  & 0.748 &    0.741    \\
        Hate Recall    & 0.621 &    0.631   & 0.642 &\textbf{0.666}  & 0.621 &    0.658    \\
        Hate F1        & 0.648 &    0.668   & 0.679 &\textbf{0.710}  & 0.679 &    0.697    \\
        \bottomrule
        \end{tabular}
    \caption{Performance de los modelos para la tarea de detección fina de discurso de odio. Cada modelo es un BERT con 3 posibles entradas: sólo el comentario (\emph{Sin contexto}), el tweet de la noticia a la cual responde el comentario (\emph{Tweet}), y el tweet más el cuerpo de la noticia (\emph{Tweet + Cuerpo}). Para cada una de estas posibilidades usamos dos versiones: una sobre BETO($\neg$FT) y otra sobre BETO ajustado al dominio (FT) de acuerdo a lo descripto en la sección \ref{sec:contextualized_classifiers}. Reportamos también la F1 sobre la clase de discurso de odio, ignorando las características de la misma manera que hacemos en la detección binaria}
    \label{tab:task_b_results}
\end{table*}



La tabla \ref{tab:task_b_results} muestra los resultados de los experimentos de clasificación para la tarea de detección fina, medida en F1 para cada una de las características, y las medidas agrupadas de forma macro precision, recall, y F1. Se muestra en cada caso el resultado de la media de 10 corridas del experimento. Como era esperable, la ganancia de tener contexto disponible es más evidente en este punto, con una diferencia de Macro F1 aproximadamente 6 puntos de F1 entre la mejor versión sin contexto y la mejor versión con contexto (0.55 Macro F1 de la versión FT sin contexto vs 0.61 F1 de la versión FT con el contexto del tweet).


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{img/task_b_scores.pdf}
    \caption{Métrica F1 para cada característica de la tarea Task B. Las características están ordenadas de mayor a menor de acuerdo a la diferencia de performance entre el modelo sin contexto y el modelo contextualizado. }
    \label{fig:barplot_task_b_results}
\end{figure*}

La figura \ref{fig:barplot_task_b_results} muestra los resultados de las F1 por característica, esta vez ordenados de mayor a menor según el gap entre la performance contextualizada vs la no contextualizada, junto a sus intervalos de confianza 95\%, usando como modelos las versiones ajustadas al dominio. Todas las características obtienen una mejora estadísticamente significativa al correr un test Mann-Whitney U ($p \leq 0.005$, p valores ajustados por Benjamini-Hochberg\cite{benjamini1995controlling}). \todo{Mandar esto a un apéndice} Las diferencias más sustanciales se dan en el caso de CRIMINAL ($+0.17$ F1 de diferencia), LGBTI ($+0.12$), CLASE ($+0.08$), y RACISMO (casi $+0.07$). Del otro lado, las características con menos mejora son APARIENCIA y POLITICA, algo esperable dado que el fenómeno tiene características poco dependientes del contexto (ver Apéndice \ref{app:manual_criterios_anotacion} para ejemplos).

Los modelos con contexto simple (que consumen sólo el tweet) son los que mejor performance tienen, en general y para cada característica, con la excepción de POLITICA.





\begin{figure}[t]
    \centering
    \small
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=0.85\textwidth]{img/context_classification/precision_barplot.pdf}
        \caption{Precisión}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=0.85\textwidth]{img/context_classification/exact_recall_barplot.pdf}
        \caption{Recall exacto}
        \label{subfig:exact_recall}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=0.85\textwidth]{img/context_classification/hate_recall_barplot.pdf}
        \caption{Recall por categoría}
        \label{subfig:total_recall}
    \end{subfigure}
    \caption{Precision y Recall para cada característica de la tarea Task B. Las diferentes barras marcan el tipo de entrada que recibe el clasificador basado en \beto{}. Recall exacto (Figura \ref{subfig:exact_recall}) cuenta el recall sobre la salida de cada categoría (es decir, sólo cuenta como recuperado un tweet marcado como MUJER si la salida del clasificador es MUJER). Recall total cuenta como recuperado un tweet si al menos alguna característica del clasificador lo marca como discurso de odio.}
    \label{fig:precision_recall}
\end{figure}

La tabla \ref{fig:precision_recall} muestra la precisión y sensibilidad de los clasificadores de la tarea granular. La sensibilidad es analizada de dos maneras: exacta, donde consideramos recuperado un tweet sólo si el clasificador acierta a la característica analizada (es decir, si la característica es MUJER, tiene que decir MUJER); y total, donde consideramos un tweet recuperado si alguna característica se enciende, independientemente si es la adecuada. Podemos ver que la categoría MUJER pasa de ser la de menor sensibilidad a dejar a la categoría LGBTI como la que tiene menor cantidad de recuperados. Análogamente la categoría CLASE obtiene una mejora sustancial en su sensibilidad, otra que es esperable que tenga cierta mezcla con otras características como RACISMO y posiblemente CRIMINAL.


\todo{Actualizar esto con última corrida}

\begin{table*}
    \centering
    \begin{tabular}{l |ll  | ll | ll}
        Métrica        &\mc{2}{Sin Contexto}& \mc{2}{Tweet}          &  \mc{2}{Tweet + Cuerpo}    \\
                       & Plano &    Granular    & Plano   &    Granular     & Plano &   Granular     \\
        \hline
        Precision & 0.718 & 0.711 & 0.748 & 0.759 &      0.727 & 0.740 \\
        Recall    & 0.602 & 0.636 & 0.653 & 0.667 &      0.641 & 0.660 \\
        F1        & 0.655 & 0.671 & 0.697 & 0.710 &      0.681 & 0.697 \\
        Macro F1  & 0.798 & 0.806 & 0.822 & 0.830 &      0.812 & 0.822 \\
        \bottomrule
        \end{tabular}
    \caption{Performance de los modelos para la tarea de detección de discurso de odio. Los modelos son BERT ajustados a dominio, consumiendo los 3 tipos de contexto: sin contexto, con contexto de tweet, y con contexto completo (tweet+cuerpo). A su vez, comparamos la performance entre los clasificadores que fueron entrenados sobre la tarea \textbf{binaria} y la tarea \textbf{granular}, esta última luego relajándose para obtener un clasificador binario.}
    \label{tab:plain_vs_granular_hate_detection}
\end{table*}

La tabla \ref{tab:plain_vs_granular_hate_detection} compara la tarea de detección de discurso de odio binaria para dos tipos de clasificadores: aquellos que fueron entrenados sobre la detección binaria




\section{Análisis de error}

\input{src/06_error_analysis.tex}



\section{Discusión}

Para analizar el impacto del contexto en la detección de discurso de odio, planteamos dos tareas de clasificación sobre el dataset construído en el capítulo \ref{chap:dataset_creation}: la tarea de detección binaria, donde respondemos si un comentario contiene discurso de odio; y una tarea de detección ``fina'', donde se debe mencionar la o las características protegidas ofendidas (si agrede a las mujeres, al colectivo LGBTI, si es racista, etc). En ambas tareas planteadas, propusimos clasificadores que consuman 3 tipos de entrada: el comentario sin contexto, el comentario con el contexto del tweet al que responden, y el comentario más el tweet al que responde y sumado el texto del artículo. Pudimos observar que el contexto parece dar una mejora moderada pero significativamente estadística en la tarea de detección del discurso de odio (alrededor de 3 puntos F1), y una mejora considerable en la tarea característica ofendida (alrededor de 6 puntos F1 medios).

Esto, en algún punto, indicaría que el contexto puede ser aprovechado para mejorar los algoritmos de detección de discurso de odio. Si bien este resultado podría estar en aparente contradicción con un trabajo reciente que no encontró ninguna mejora en el uso del contexto en la detección de toxicidad \cite{pavlopoulos2020toxicity}, se puede señalar que la detección del discurso de odio es una de las formas más complejas de comportamiento ``tóxico'' y, como tal, podría permitir que los clasificadores tengan más información para predecir si el texto dado es odioso o no. Otra razón detrás de este resultado es el dominio de nuestro conjunto de datos: mientras que \citet{pavlopoulos2020toxicity} usa el contexto conversacional, nosotros usamos el título y el cuerpo del artículo como contexto para los comentarios de los usuarios. Más recientemente, y como marcamos en la sección de trabajo previo, \citet{xenos-2021-context} ha observado que, restringiendo el análisis a un subconjunto de comentarios sensibles al contexto (ver \ref{sec:06_classification_previous} y \ref{sec:dataset_previous} para más información), la performance de los algoritmos de detección de toxicidad mejoran de manera significativa.

Algo que observamos es que la utilización de un contexto más largo como el artículo de la noticia no mejora la performance de nuestros clasificadores. Hay varias interpretaciones posibles de esto: en primer lugar, los humanos suelen contestar sin leer el artículo, con lo cual este resultado pareciera ser coherente con esta observación. Por otro lado, los humanos solemos tener acceso a un contexto mucho más rico, muchas veces equivalente a haber leído la nota, algo que nuestros clasificadores carecerían. Podría también esto ser atribuido al modelo pre-entrenado que usamos para codificar esto (BETO, la variante en español de BERT) que suele estar entrenada para textos más cortos (ver sección ZZZ para más información \todo{Escribir esto}). Teniendo esto en cuenta, realizamos el ajuste de dominio usando el contexto largo, pero aún así la performance del clasificador que consume este contexto ``largo'' se mantuvo por debajo del que usaba el contexto ``simple'' (sólo el tweet del artículo).

El análisis del error realizado demostró que, si bien el contexto pareciera mejorar la detección de discurso de odio, para muchas características protegidas se mantiene aún como una tarea difícil. Un caso ejemplificador de esto es la discriminación contra el colectivo LGBTI. Si bien el dataset recolectado no contiene demasiadas instancias de este fenómeno, en él puede verse que las agresiones contra este colectivo y sus miembros son sumamente sofisticadas, lejos de las agresiones meramente basadas en insultos u otras palabras ofensivas. Nuestros clasificadores, aún en sus mejores versiones (usando ajuste de dominio y contexto) obtuvieron una baja performance en la detección de este fenómeno (alrededor de $0.5$ F1 score) dando cuenta que es un problema no trivial y merece ser analizado más detenidamente debido a la complejidad de estos mensajes, que suelen reunir ironía, metáforas, y otros artilugios que hacen difícil su detección.

En el caso de la categoría mujer, inesperadamente, también obtuvimos una performance muy baja en la detección de agresiones misóginas. En el análisis de error, podemos observar que tenemos casos complejos de descifrar que fueron marcados por nuestros anotadores: por ejemplo, ataques ``velados'' a mujeres víctimas de violación (llamarlas mentirosas). \todo{Escribir un poco más sobre esto}

Algo que debe ser tenido en cuenta para matizar estos resultados es que utilizamos un amplio espectro de características protegidas. Incluso, la que más se beneficia del contexto es una que introdujimos ad-hoc para este experimento (discurso de odio contra criminales). En contrapeso, otras características ``no convencionales'' son poco beneficiadas por el contexto (como discurso de odio en base a la apariencia, opinión política y discapacidad).

Un resultado que también observamos es que pareciera ser que nuestros clasificadores mejoran su performance en un contexto de aprender cada característica por separado en vez de sólo aprender a distinguir la etiqueta binaria de discurso de odio.

Una limitación de aplicación de este resultado es que no siempre tenemos un contexto disponible para un texto dado. Incluso si podemos encontrarlo, a veces este contexto puede no ser en forma de artículo de noticias, sino como un hilo de conversación o incluso de alguna otra forma (¿una base de conocimiento?). Teniendo en cuenta alguna de las consideraciones hechas en esta discusión, una posible línea de investigación seria la de incorporar distintos tipos de contexto, desde más mensajes en el hilo de la conversación como así también conocimiento estructurado \todo{Agregar alguna cita, quizás de knowledge infusion}

\section{Conclusiones}

En este capítulo hemos realizado experimentos de clasificación sobre el dataset construído en el capítulo anterior, focalizándonos en analizar el impacto de la utilización del contexto en la performance de los modelos. Planteamos dos tareas: una tarea binaria --detectar si existe discurso de odio no-- y una tarea granular --definir qué categorías son atacadas en un tweet, si es que las hay--. Para ambas tareas, los modelos contextualizados obtienen mejoras significativas en la performance, dando algún indicio de que información adicional al comentario analizado puede ayudar a detectar discurso de odio. Si bien en nuestros experimentos el contexto más pequeño (el tweet del artículo de la noticia) fue el que mejor resultados obtuvo, una línea de trabajo futuro podría explorar otras formas de integrar el contexto más largo -- en este caso, el artículo de la noticia.

Así mismo, observamos una pequeña mejora --aunque estadísticamente significativa-- en la detección de discurso de odio al entrenar un clasificador granular pero evaluándolo de manera binaria. En este caso, obtenemos una ventaja al obtener una salida más interpretable de las características ofendidas, y que además que no sólo no empeora la performance de nuestro clasificador sino que hasta incluso mejora levemente.

Del análisis de error, observamos que algunas categorías del discurso de odio se muestran elusivas para nuestros clasificadores. Uno de estos casos son los mensajes abusivos contra la comunidad LGBTI+. Algunos mensajes son realmente muy complejos, velados, y conteniendo metáforas que son esquivas para los modelos actuales. A pesar de esto, esta característica fue una de las más beneficiadas por la adición de contexto, aunque su desempeño sigue siendo bajo, teniendo una sensibilidad de alrededor de 0.5. \todo{Fijarse si no vale chamuyar un poco con el estado actual de AI}

Podemos concluir que los datasets de discurso de odio deberían --en la medida de lo posible-- contener \textbf{información contextual} sobre los comentarios analizados. Esta información puede darse en forma de artículos de noticias, como un hilo de conversación, o incluso como otras formas (como una base de conocimiento, por ejemplo). Sobre esto, trabajo futuro debería explorar el impacto de utilizar esta información adicional para detectar discurso de odio. La evidencia de estos experimentos indica que los modelos del estado del arte pueden aprovecharlo para mejorar su performance. En segundo lugar, los datasets de discurso de odio deberían incluir \textbf{información granular} acerca de las características atacadas --y no sólo una etiqueta binaria-- mejoran la interpretabilidad de los modelos, y hasta indicarían que mejoran marginalmente el desempeño de estos.

Finalmente, un aspecto que introdujimos en este capítulo fue el de adaptar un modelo de lenguaje pre-entrenado a su dominio, siendo en nuestro caso los comentarios sobre notas periodísticas en Twitter. Las mejoras que reportó la utilización de estas técnicas fue significativa, en consonancia con otros trabajos recientes. Pasaremos a continuación a estudiar estas técnicas en el marco más general de la clasificación de textos sociales.