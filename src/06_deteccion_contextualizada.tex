\label{chap:06_contextualized_hate_speech}
En base a las discusiones previas, en este capítulo analizaremos el impacto del contexto en la tarea de detección de discurso de odio en redes sociales, algo que ha recibido poca atención en trabajos previos. Utilizaremos el dataset construído en el capítulo \ref{chap:05_dataset_creation} que está basado en comentarios de artículos periodísticos en Twitter, y que nos brinda información adicional a cada comentario tanto por el tweet del medio periodístico como el contenido del artículo. Para evaluar si la adición de contexto resulta en una mejora en la detección de discurso de odio realizaremos experimentos de clasificación con modelos que sólo consuman el comentario en cuestión, y otros que a su vez consuman algún tipo de contexto asociado. Por la naturaleza de nuestro dataset, tenemos dos tipos de contexto naturales: uno ``corto'', que contiene el tweet del medio periodístico, y otro ``largo'' que incorpora además el cuerpo del artículo asociado.

La anotación del dataset, con información detallada de las características ofendidas, nos permite salir de analizar exclusivamente la existencia de discurso de odio sino que podemos pedir más detalle sobre la ofensa cometida. Proponemos entonces dos tareas: una tarea de detección \textbf{binaria}, donde sólo predecimos si hay o no discurso de odio; y una tarea de detección \textbf{granular}, donde además predecimos todas las características ofendidas (potencialmente más de una). Para estas tareas, propondremos algoritmos de clasificación sobre modelos pre-entrenados de lenguaje, concretamente sobre \beto{} (la versión en español de \bert{}). Estos modelos tienen incorporados naturalmente la posibilidad de consumir dos entradas, con lo cual son ideales para nuestros experimentos.

Evaluaremos los resultados tanto en términos de la performance de las distintas configuraciones de nuestros clasificadores, como así también realizando análisis de error comparativos entre los modelos contextualizados y los no contextualizados. También analizaremos las dificultades en general que presenta la detección de este fenómeno sobre comentarios de notas periodísticas.

\section{Trabajos previos}
\label{sec:06_classification_previous}

Como mencionamos en la sección \ref{sec:dataset_previous}, no se ha dado demasiada atención en la literatura a la utilización de contexto en la detección de discurso de odio y otros fenómenos similares (como la detección de toxicidad, por ejemplo). En dicho capítulo se puede encontrar una descripción de algunos de los datasets que sí contienen algún tipo de contexto. Pasamos ahora a describir los algoritmos de detección que utilizaron en los trabajos correspondientes.

\citet{gao-huang-2017-detecting} propone dos tipos de modelos: regresiones logísticas y redes neuronales recurrentes. Para los modelos de regresiones logísticas, usan como inputs bolsas de palabras, bolsas de caracteres, vectores semánticos producidos con Linguistic Inquiry and Word Count (LIWC) \cite{pennebaker2001linguistic} y features de un lexicon de emociones \cite{mohammad2013nrc}. Por otro lado, utiliza LSTM bidireccionales con mecanismo de atención de Bahdanau \cite{bahdanau2014neural} usando embeddings \emph{word2Vec} de dimensión 100.

Un punto criticable de este trabajo es que utiliza el nombre de usuario como feature; algo que a priori no suele hacerse ya que permitiría ``prejuzgar'' a un usuario antes que por el contenido de sus tweets. Si bien es cierto que la información de usuarios y sus conexiones es valiosa, introducir esta información a nuestros modelos da lugar a posibles correlaciones espurias que es preferible evitar.


\begin{figure*}[t]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{img/pavlopoulos_rnn_rnn_classifier.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{img/pavlopoulos_rnn_bert_classifier.png}
    \end{minipage}

    \begin{minipage}[b]{0.35\textwidth}
        \includegraphics[width=\textwidth]{img/pavlopoulos_bert_sep_classifier.png}
    \end{minipage}


    \caption{Clasificadores que consumen contexto propuestos por \citet{pavlopoulos2020toxicity}. Los dos primeros clasificadores proponen una arquitectura de dos encoders, uno para el texto y otro para el contexto usando bi-LSTMs y BERT como posibilidades. El tercer clasificador propuesto es un BERT usando su estructura natural para codificar dos oraciones separadas por el token $SEP$ }
    \label{fig:pavlopoulos_classifiers}
\end{figure*}


En la sección \ref{sec:dataset_previous} hemos descripto el dataset construído por \citet{pavlopoulos2020toxicity}. Nos detendremos un momento para analizar sus experimentos de clasificación ya que guardan importantes similaridades con lo que haremos en este capítulo. En ese trabajo se obtuvieron dos datasets de entrenamiento: uno en el cual los etiquetadores tenían información del contexto y otro en el que no, mientras que el dataset de test fue etiquetado viendo el contexto, considerando que el etiquetado es de mejor calidad usando más información. Los autores plantearon dos preguntas sobre esta base: ¿mejora la performance de los clasificadores que son entrenados con el dataset etiquetado con contexto? ¿mejora la performance de los clasificadores consumiendo información contextual? Con estas preguntas, tenemos que tomar dos decisiones: dataset de entrenamiento con o sin contexto, y clasificador con o sin contexto. Tenemos 4 posibles combinaciones de experimentos, sin aún considerar posibles técnicas de clasificación.

Para cada una de estas combinaciones, se consideraron técnicas del estado del arte de clasificación. Para aquellos clasificadores que no consumen contexto, las opciones son las mismas que hemos visto en capítulos anteriores: bi-LSTM o BERT. Para aquellos que sí consumen contexto, se evaluan dos estrategias: una, concatenar con algún caracter, y otra usando dos encoders distintos para el contexto y el texto. A su vez también utilizan la API Perspective de Google, que finalmente obtiene los mejores resultados en términos de performance. En todas las combinaciones posibles, si bien hay una mejora en la performance medida con ROC-AUC al usar contexto en ambas formas, esta no es estadísticamente significativa.

Algo a mencionar (que retomaremos en este y en el siguiente capítulo) es que usan dos versiones de \bert{}: una usando los pesos del modelo de \bert{}, y otro haciendo un ajuste de dominio () corriendo la tarea de MLM sobre un dataset grande y no etiquetado. En el caso de el trabajo mencionado, sólo hacen un fine-tuning sobre comentarios sueltos del dataset de Civil Comments. Esto podría tener algún efecto deteriorando la performance al usar contexto; sin embargo, en el \bert{} a secas (sin hacer ajustes) tampoco se observa mejora significativa en la performance.

Algunas limitaciones marcadas por los autores son:

\begin{itemize}
    \item Contexto muy pequeño: sólo el título más el comentario previo
    \item Se ignora el hilo completo de comentarios
    \item Los datos fueron sampleados aleatoriamente
\end{itemize}

En \citet{xenos-2021-context}, continuación de este trabajo, reetiquetaron el dataset de Civil Comments usando contexto y --como mencionamos en la sección \ref{sec:dataset_previous}-- presentaron una nueva tarea de detección de sensibilidad al contexto. Usando la API Perspective (y la estrategia de concatenación ``básica'' con algún caracter), notaron que la performance del clasificador que consume el contexto mejora con respecto al que no lo hace a medida que restringimos nuestra atención a comentarios más ``sensibles al contexto'' (de acuerdo a la métrica definida por los autores)


\section{Tareas de clasificación propuestas}
\label{sec:tasks}

Para analizar el impacto del contexto en la detección de discurso de odio, y teniendo en cuenta que contamos de un dataset con anotaciones granulares sobre las características ofendidas, proponemos dos tareas de clasificación:

\begin{enumerate}
    \item \textbf{Detección binaria}: Dado un tweet y su contexto, predecir si contiene contenido discriminatorio.
    \item \textbf{Detección granular}\footnote{En inglés usamos la denominación \emph{fine-grained}, aunque no hay una traducción precisa para este término en español}: Dado un tweet y su contexto, predecir las características ofendidas (si hay alguna) y si contiene un llamado a la acción.
\end{enumerate}

%%
%%
%% Link
%% https://docs.google.com/drawings/d/11sAaOuGJlU0P61mkrPxKduFwnNOuPV31tXUFJWEwbVU/edit?usp=drive_web&ouid=117313784631536396179
%%
%%
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{img/06/hate_detection_tasks.pdf}
    \caption{Tareas propuestas de detección de discurso de odio. La tarea de detección binaria consta de predecir si un tweet contiene contenido discriminatorio, discriminando la frontera conjunta. En la tarea granular, predecimos por separado cada una de las características ofendidas, pudiendo haber más de una o ninguna.}
    \label{fig:hate_detection_tasks}
\end{figure}



Puede pensarse la tarea de detección binaria (la que usualmente se aborda en la literatura sobre el tema) como una relajación de la tarea detallada: mientras la primera sólo nos permite detectar si hay o no contenido discriminatorio, la segunda nos requiere información más precisa acerca de las características ofendidas. Esta segunda tarea es posible dado que el dataset que construímos contiene esta información, algo usualmente faltante en otros trabajos. La tarea granular nos permite a su vez tener mayor entendimiento de la salida e interpretar mejor sus errores. La figura \ref{fig:hate_detection_tasks} ilustra las dos tareas propuestas. Mientras en la tarea binaria sólo debemos decidir la frontera sobre si el contenido es discriminatorio o no, en la tarea granular necesitamos decir en cuál de todas las intersecciones está el comentario y su contexto.


Planteándolos como problemas de clasificación, la detección binaria consta de predecir una sola etiqueta binaria, mientras que la tarea granular consta de $n$ etiquetas binarias; es decir, $n$ problemas distintos de clasificación. Vale mencionar que, entendiendo una tarea como una relajación de la otra, si tenemos un clasificador entrenado para la tarea granular podemos construir un clasificador para la tarea binaria tomando la disyunción lógica de sus salidas; o dicho más coloquialmente, poniendo una compuerta OR sobre la salida del clasificador granular. Retomaremos esta idea más adelante al hablar de cómo evaluamos nuestras técnicas de clasificación para cada tarea.


\section{Modelos de clasificación}
\label{sec:contextualized_classifiers}





Para las dos tareas, entrenamos varios clasificadores basados en \beto{} \cite{canete2020spanish}. Respecto a la información contextual, tenemos tres tipos de entrada por instancia: el comentario sin ningún tipo de contexto, con contexto simple (el tweet al que responde), y con contexto largo (el tweet al que responde + el texto del artículo periodístico). Usamos el token especial BERT \emph {[SEP]} para separar el contexto y el texto analizado. Este token es el que \emph {[SEP]} se usa para la tarea de predicción de la siguiente oración (tarea NSP) en el preentrenamiento al estilo BERT (ver la sección \ref{sec:02_transformers} para más información).

%%
%%
%% Link a Draw
%% https://docs.google.com/drawings/d/1F8iVSIRqHhGkQ0zglxqXLGD36RHZ9OhHMZYsg_xFOS4/edit
%%
%%

\begin{figure}
    \centering
    \includegraphics[width=1.10\textwidth]{img/06/bert_contextual_classifier.pdf}
    \caption{Modelo de multiclasificación para la tarea granular. Los modelos son entrenados de 3 maneras distintas: sin contexto (sólo el comentario), con el contexto del tweet, y con el contexto del tweet y el texto del artículo.}
    \label{fig:05_multi_bert_classifier}
\end{figure}

Para la tarea de clasificación binaria, la salida de los clasificadores es la salida estándar de clasificadores del tipo \bert{}: una capa softmax para las dos posibles clases (ver la sección \ref{sec:03_classification} o \ref{sec:04_classifiers} para más información). En cuanto a la tarea de detección granular, lo planteamos como la predicción de 9 variables distintas: llamado a la acción (CALLS) y las 8 características ofendidas: MUJER, RACISMO, CLASE, LGBTI, CRIMINAL, ASPECTO, DISCAPACIDAD, POLITICA. En lugar de entrenar un clasificador diferente para cada característica, entrenamos un BERT de múltiples salidas, compartiendo todos sus pesos con la excepción de 9 capas lineales diferentes para cada salida. La función de costo utilizada es:

\begin{equation*}
    J = \sum\limits_{c \in CHAR'} J_c
\end{equation*}

donde $CHAR'$ es el conjunto de todas las características protegidas y además la variable de llamada a la acción, y cada $ J_c $ es la función de entropía cruzada. Compartir los pesos entre todas las salidas tiene dos objetivos: primero, poder generar un modelo más compacto (de otra forma serían 9 BERT distintos) y segundo, para compartir información común entre las distintas características atacadas, ya que consideramos que guardan similaridades -- y muchas de ellas tienen cierta intersección, como hemos visto en el capítulo anterior. Para tener costos computacionales más amigables, limitamos nuestras secuencias a 128, 256 y 512 tokens para el modelo sin contexto, el modelo que consume el tweet, y el modelo de tweet + cuerpo respectivamente. La figura \ref{fig:05_multi_bert_classifier} muestra el modelo de clasificación para la tarea granular, junto a los 3 tipos de entrada descriptos.

Una práctica cada vez más extendida en trabajos del área de clasificación de documentos es realizar una adaptación de dominio sobre textos relacionados a nuestro. Esto se realiza corriendo la tarea de masked language model (ver sección \ref{sec:02_transformers}) sobre un dataset grande y no supervisado relacionado a nuestro dominio, o directamente sobre el dataset de la tarea si esto no está disponible \cite{gururangan-etal-2020-dont}. En la sección \ref{sec:domain_adaptation_previous_work} del siguiente capítulo haremos una reseña más extensa de esta técnica, pero por lo pronto podemos entender que ajusta el modelo de lenguaje a los textos disponibles; en este caso, \beto{} fue entrenado en Wikipedia y textos formales, con esta técnica lo ajustaremos a nuestro dominio particular de comentarios en Twitter a notas periodísticas.

\begin{table}[t]
    \centering
    \begin{tabular}{ll}
        \toprule
        Hiperparámetro & Valor         \\
        \midrule
        Steps          & $10K$           \\
        Batch size     & 2048            \\
        max seq length & 128, 256 y 512  \\
        $\beta_1$      & $0.9$           \\
        $\beta_2$      & $0.98$          \\
        $\epsilon$     & $10^{-6}$       \\
        decay          & $0.01$          \\
        Peak LR        & $4*10^{-4}$     \\
        warmup ratio   & 0.1             \\
        \bottomrule
    \end{tabular}
    \caption{Hiperparámetros para la adaptación de dominio de BERT}
    \label{tab:hs_ft_hyperparameter}
\end{table}

Para lo que nos concierne en esta sección, \citet{pavlopoulos2020toxicity} realizó una adaptación de dominio sobre los comentarios del corpus de \emph{Civil Comments} (a lo que denomina BERT-CCTK). Esta adaptación la realizó únicamente con los comentarios, sin utilizar ningún tipo de contexto. Proponemos a diferencia de este trabajo, 3 tipos de adaptaciones: una adaptación sin contexto, una adaptación con el contexto del tweet, y una adaptación con el contexto del tweet y el cuerpo de la noticia.

Realizamos el ajuste utilizando el sobrante de la recolección de datos del anterior capítulo: alrededor de 288k artículos y 5.1M comentarios\footnote{Utilizamos algunos datos extra recolectados a posteriori de lo mencionado en el capítulo anterior}. La tabla \ref{tab:hs_ft_hyperparameter} contiene los hiperparámetros utilizados. Estos ajustes los realizamos sobre una TPU v2-8 y una máquina de Google Colab Pro por alrededor de 10 hs (en su largo de cadena máximo).

Preprocesamos ambos tweets --contexto y texto-- utilizando las técnicas descriptas en la sección \ref{sec:03_preprocessing}: conversión de usernames a un token especial (\verb|usuario|), tratamiento de hashtags (separación e inserción de un hashtag especial), y conversión de emojis a su representación textual.


\subsection{Performance humana de la tarea}


\begin{table}
    \centering
    \begin{tabular}{l cc  cc}
                   & \multicolumn{2}{c}{Entre anotadores} & \multicolumn{2}{c}{Gold} \\
        {}         &  F1 mean&  F1 median  & F1 Mean  &  F1 Median \\
        \hline
        ODIO       &  0.653 &   0.675    & 0.829   &   0.851   \\
        CALLS      &  0.434 &   0.495   &  0.704   &   0.842  \\
        \hline
        MUJER      &  0.490 &   0.468   &  0.741   &   0.759  \\
        LGBTI      &  0.596 &   0.577   &  0.846   &   0.915  \\
        RACISMO    &  0.653 &   0.644   &  0.871   &   0.879  \\
        CLASE      &  0.443 &   0.444   &  0.722   &   0.732  \\
        POLITICA   &  0.461 &   0.436   &  0.795   &   0.815  \\
        DISCAPACIDAD& 0.550 &   0.600   &  0.813   &   0.842  \\
        APARIENCIA &  0.649 &   0.743   &  0.831   &   0.915  \\
        CRIMINAL   &  0.527 &   0.580   &  0.841   &   0.929  \\
        \hline
        Macro F1   &  0.534 &   0.554   &  0.796   &   0.848  \\
        \hline
    \end{tabular}

    \caption{Estadísticos de las cotas de performance entre anotadores. Cada característica es tomada como una etiqueta binaria independientemente del cálculo de la métrica para discurso de odio. La Macro F1 es el promedio de los F1 todas las características y la F1 de la llamada a la acción (CALLS). Las dos primeras columnas marcan las métricas medidas entre anotadores, y las dos últimas la de los anotadores contra la etiqueta gold}
    \label{tab:ia_f1_scores}
\end{table}


Como observamos en la anterior sección, la tarea de detección de lenguaje discriminatorio contiene una alta cantidad de ruido, y el acuerdo entre humanos es moderado. En este contexto, cabe preguntarse una cota a la performance que puede lograr un clasificador para esta tarea, que claramente por la misma naturaleza del problema, va a distar mucho de la perfección. Para obtener algunas medidas de esto, calculamos en primer lugar las F1 usando todos los posibles pares de anotadores. Como la F1 es simétrica no necesitamos hacer ninguna asunción sobre sus roles.

Algo a tener en cuenta es que la métrica final será contra la etiqueta resultante de la votación mayoritaria (nuestro \emph{gold standard}). Una cota que seguro está por arriba de nuestra performance es el acuerdo que haya entre los anotadores y este \emph{gold standard}; hay que también observar que cada etiqueta asignada (ver sección \ref{sec:assign}) codifica información de cada anotador, con lo cual éste número es una cota superior pero puede que sobreestimada.


La tabla \ref{tab:ia_f1_scores} contiene estadísticos para estas cotas, tanto entre anotadores como contra el \emph{gold-standard}. Como podemos observar, la mediana entre anotadores de la F1 (usada para obviar outliers) es relativamente baja para la detección de odio ($\sim 0.67$), mientras que contra el gold standard es de $0.85$. De esto entendemos que la performance una cota superior a la performance está entre esos dos números.



\section{Resultados}

\input{src/06_results.tex}

\section{Análisis de error}

\input{src/06_error_analysis.tex}



\section{Discusión}

Para analizar el impacto del contexto en la detección de discurso de odio, planteamos dos tareas de clasificación sobre el dataset construído en el capítulo \ref{chap:05_dataset_creation}: la tarea de detección binaria, donde respondemos si un comentario contiene discurso de odio; y una tarea de detección granular, donde se debe mencionar la o las características protegidas ofendidas (si agrede a las mujeres, al colectivo LGBTI, si es racista, etc). En ambas tareas planteadas propusimos clasificadores que consumen 3 tipos de entrada: el comentario sin contexto, el comentario con el contexto del tweet al que responden, y el comentario más el tweet al que responde y también el texto del artículo. Pudimos observar que el contexto parece dar una mejora moderada pero significativamente estadística en la tarea de detección del discurso de odio (alrededor de 3 puntos F1), y una mejora considerable en la tarea característica ofendida (alrededor de 6 puntos F1 medios).

Esto, en algún punto, indicaría que el contexto puede ser aprovechado para mejorar los algoritmos de detección de discurso de odio. Si bien este resultado podría estar en aparente contradicción con trabajos recientes que no encontraron ninguna mejora en el uso del contexto en la detección de toxicidad \cite{pavlopoulos2020toxicity}, se puede señalar que la detección del discurso de odio es una de las formas más complejas de comportamiento ``tóxico'' y, como tal, podría permitir que los clasificadores tengan más información para predecir si el texto dado es odioso o no. Otra razón detrás de este resultado es el dominio de nuestro conjunto de datos: mientras que \citet{pavlopoulos2020toxicity} usa el contexto conversacional, nosotros usamos el título y el cuerpo del artículo como contexto para los comentarios de los usuarios. Más recientemente, y como marcamos en la sección de trabajo previo, en \citet{xenos-2021-context} han observado que, restringiendo el análisis a un subconjunto de comentarios sensibles al contexto (ver \ref{sec:06_classification_previous} y \ref{sec:dataset_previous} para más información), la performance de los algoritmos de detección de toxicidad mejoran de manera significativa.

La utilización de un contexto más largo como el artículo de la noticia no mejora la performance de los clasificadores en nuestros experimentos. Hay varias interpretaciones posibles de esto: en primer lugar, los humanos suelen contestar sin leer el artículo, con lo cual este resultado pareciera ser coherente con esta observación. Por otro lado, los humanos solemos tener acceso a un contexto mucho más rico, muchas veces equivalente a haber leído la nota, algo que nuestros clasificadores carecerían. Podría también esto ser atribuido al modelo pre-entrenado que usamos para codificar esto (BETO, la variante en español de BERT) suele estar pre-entrenada para textos más cortos. Teniendo esto en cuenta, realizamos el ajuste de dominio usando el contexto largo, pero aún así la performance del clasificador que consume este contexto largo se mantuvo por debajo del que usaba el contexto simple.

El análisis del error realizado demostró que, si bien el contexto pareciera mejorar la detección de discurso de odio, para muchas características protegidas se mantiene aún como una tarea difícil. Un caso ejemplificador de esto es la discriminación contra el colectivo LGBTI. En las instancias del dataset --y en muchos de los ejemplos en los que el algoritmo de detección falla--  puede verse que las agresiones contra este colectivo y sus miembros son sumamente sofisticadas, lejos de las agresiones meramente basadas en insultos u otras palabras ofensivas. Nuestros clasificadores, aún en sus mejores versiones (usando ajuste de dominio y contexto) obtuvieron una baja performance en la detección de este fenómeno (alrededor de $0.5$ F1 score) dando cuenta que es un problema no trivial y merece ser analizado más detenidamente debido a la complejidad de estos mensajes, que suelen reunir ironía, metáforas, y otros artilugios que hacen difícil su detección.

En el caso de la categoría mujer, inesperadamente, también obtuvimos una performance muy baja en la detección de agresiones misóginas. En el análisis de error, podemos observar que tenemos casos complejos de descifrar que fueron marcados por nuestros anotadores: por ejemplo, ataques velados a mujeres víctimas de violación (llamarlas mentirosas). \todo{Escribir un poco más sobre esto}

Algo que debe ser tenido en cuenta para matizar estos resultados es que utilizamos un amplio espectro de características protegidas. Incluso, la que más se beneficia del contexto es una que introdujimos ad-hoc para este experimento (discurso de odio contra criminales). En contrapeso, otras características ``no convencionales'' son poco beneficiadas por el contexto (como discurso de odio en base a la apariencia, opinión política y discapacidad).

Un resultado que también observamos es que pareciera ser que nuestros clasificadores mejoran leve pero significativamente su performance en un contexto de aprender cada característica por separado en vez de sólo aprender a distinguir la etiqueta binaria de discurso de odio. Si bien la mejora es marginal (cerca de un punto de F1) y no es observable de manera subjetiva mediante un análisis de error, una posible razón detrás de esto es que la señal más precisa acerca de la categoría ofendida puede ayudar a distinguir mejor las fronteras de este fenómeno. Aún cuando esta mejora no fuese tal, poder tener una salida más interpretable y granular es mejor que simplemente obtener una predicción binaria.

Una limitación importante de este estudio es que el entrenamiento lo realizamos sobre datos entrenados únicamente observando el contexto. Entrenar a los clasificadores no contextualizados sobre estas etiquetas induce a los clasificadores a tomar decisiones erróneas, como asumir que celebraciones son discurso de odio debido a instancias que --con contexto-- tienen esa naturaleza. Un estudio más completo del impacto del contexto en esta tarea debería incluir los datos entrenados sin contexto.

En el terreno de la aplicación, un problema práctico de este resultado es que no siempre tenemos un contexto disponible para un texto dado. Incluso si podemos encontrarlo, a veces este contexto puede no ser en forma de artículo de noticias, sino como un hilo de conversación o incluso de alguna otra representación. Teniendo en cuenta alguna de las consideraciones hechas en esta discusión, una posible línea de investigación seria la de incorporar distintos tipos de contexto, desde más mensajes en el hilo de la conversación, conocimiento estructurado (por ejemplo, la propuesta en \emph{ERNIE} \cite{zhang2019ernie} o \emph{KI-BERT} \cite{faldu2021ki}) o bien una combinación de diversas fuentes, incluso multimodales.

\section{Conclusiones}

En este capítulo hemos realizado experimentos de clasificación sobre el dataset construído en el capítulo anterior, focalizándonos en analizar el impacto de la utilización del contexto en la performance de los modelos. Planteamos dos tareas: una tarea binaria --detectar si existe discurso de odio o no-- y una tarea granular --definir qué categorías son atacadas en un tweet, si es que las hay--. Para ambas tareas, los modelos contextualizados obtienen mejoras significativas en la performance, dando indicios de que información adicional al comentario analizado puede ayudar a detectar el discurso de odio. Si bien en nuestros experimentos el contexto más pequeño (el tweet del artículo de la noticia) fue el que mejor resultados obtuvo, una línea de trabajo futuro podría explorar otras formas de incorporar el contexto más largo -- en este caso, el artículo de la noticia.

Así mismo, observamos una pequeña pero estadísticamente significativa mejora en la detección de discurso de odio al entrenar un clasificador granular al ser evaluado de manera binaria. En este caso, obtenemos una ventaja al obtener una salida más interpretable de las características ofendidas, y que además que no sólo no empeora la performance de nuestro clasificador sino que hasta incluso mejora levemente.

Del análisis de error, se observa que algunas categorías del discurso de odio se muestran elusivas para los algoritmos de detección. Uno de estos casos son los mensajes abusivos contra la comunidad LGBTI+, conteniendo mensajes semánticamente complejos, implícitos y con metáforas que son esquivas para los modelos propuestos. A pesar de estas limitaciones, esta característica fue una de las más beneficiadas por la adición de contexto, aunque su desempeño sigue siendo bajo, teniendo una puntuación de F1 de alrededor de 0.5.

Podemos concluir que los datasets de discurso de odio deberían --en la medida de lo posible-- contener \textbf{información contextual} sobre los comentarios analizados. Esta información puede darse en forma de artículos de noticias, como un hilo de conversación, o incluso como otras formas --por ejemplo, como una base de conocimiento. Sobre esto, trabajo futuro debería explorar el impacto de utilizar esta información adicional para integrarla en algoritmos de detección de discurso de odio. La evidencia de estos experimentos --por ahora preliminares, y con las limitaciones marcadas en la discusión-- indica que los modelos del estado del arte pueden utilizar esta información para mejorar la detección de discurso de odio en redes sociales. En segundo lugar, los datasets de discurso de odio deberían incluir \textbf{información granular} acerca de las características atacadas --y no sólo una etiqueta binaria-- ya que por un lado esto mejora la interpretabilidad de los algoritmos de detección, y resultados preliminares de este estudio indicarían que mejoran marginalmente la performance en la detección en general.

Finalmente, un aspecto que introdujimos en este capítulo fue el de adaptar un modelo de lenguaje pre-entrenado a su dominio, siendo en nuestro caso los comentarios sobre notas periodísticas en Twitter. Las mejoras que reportó la utilización de estas técnicas fue significativa, en consonancia con otros trabajos recientes. Pasaremos a continuación a estudiar estas técnicas en el marco más general de la clasificación de textos sociales.