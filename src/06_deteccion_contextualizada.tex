\label{chap:06_contextualized_hate_speech}

En este capítulo analizamos el impacto de añadir contexto en la tarea de detección de discurso de odio en redes sociales. Como hemos marcado en los capítulos anteriores, la utilización del contexto ha recibido poca atención en la literatura, limitando la tarea a analizar comentarios aislados de cualquier tópico relacionado o hilo conversacional. Para este estudio, utilizamos el conjunto de datos construído en el Capítulo \ref{chap:05_dataset_creation}, cuyos datos constan de comentarios de artículos periodísticos en Twitter.

El formato de los datos empleados nos brinda información adicional a cada comentario tanto por el tweet del medio periodístico al que contestan como así también por el contenido del artículo. Para evaluar si la adición de contexto resulta en una mejora en la detección de discurso de odio, realizamos experimentos de clasificación con modelos que consumen tres tipos de entrada: el comentario sin contexto, el comentario junto al tweet del medio periodístico, y el comentario junto al tweet y el cuerpo del artículo asociado.

\todo{buscar todos los verbos conjugados en futuro}

El conjunto de datos empleado nos permite analizar una posible combinación más en base al detalle de las características ofendidas por cada comentario. Esta información granular permite no sólo analizar la existencia de discurso de odio sino que permite predecir con más detalle la ofensa cometida. Proponemos en base a esto dos tareas de clasificación: una tarea de detección \textbf{binaria}, donde sólo predecimos si hay o no discurso de odio; y una tarea de detección \textbf{granular}, donde además predecimos todas las características ofendidas (potencialmente más de una). Para estas tareas, propusimos algoritmos de clasificación sobre modelos pre-entrenados de lenguaje que tienen como entrada los distintos tipos de contexto posibles. Estos modelos tienen incorporados naturalmente la posibilidad de consumir dos entradas --el contexto y el texto-- con lo cual son ideales para nuestros experimentos.

Evaluamos los resultados de los experimentos de clasificación tanto en términos del rendimiento de las distintas configuraciones de nuestros clasificadores, como así también realizando análisis de error comparativos entre los modelos contextualizados y los no contextualizados. También evaluamos en este capítulo las dificultades más generales que presenta la detección de este fenómeno sobre comentarios de notas periodísticas.

\section{Trabajo previo}
\label{sec:06_classification_previous}

Como mencionamos en la Sección \ref{sec:dataset_previous}, no se ha dado demasiada atención en la literatura a la utilización de información contextual en la detección de discurso de odio y otros fenómenos similares. Pasamos ahora a repasar los algoritmos de detección utilizados sobre los conjuntos de datos descriptos en dicha sección.

\citet{gao-huang-2017-detecting} proponen utilizar dos tipos de modelos sobre el dataset que ellos mismos recolectaron sobre comentarios de Fox News: regresiones logísticas y redes neuronales recurrentes. Para las regresiones logísticas, usaron como entradas bolsas de palabras, bolsas de caracteres, vectores semánticos producidos con Linguistic Inquiry and Word Count (LIWC) \cite{pennebaker2001linguistic}, y otras variables de un lexicón de emociones \cite{mohammad2013nrc}. Por otro lado, los autores también entrenan LSTM bidireccionales con mecanismo de atención de Bahdanau \cite{bahdanau2014neural} que consumen embeddings \emph{word2vec} de dimensión 100.

Un punto criticable de este trabajo es que utiliza el nombre de usuario como entrada, algo que a priori no suele hacerse ya que permitiría prejuzgar a un usuario antes que por el contenido de sus tweets. Si bien es cierto que la información de usuarios y sus conexiones es valiosa, introducir esta información a nuestros modelos puede dar lugar a correlaciones espurias que es preferible evitar. Otras críticas sobre el proceso de anotación de los datos fueron realizadas ya en la Sección \ref{sec:dataset_previous}.


\begin{figure*}[t]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{img/pavlopoulos_rnn_rnn_classifier.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{img/pavlopoulos_rnn_bert_classifier.png}
    \end{minipage}

    \begin{minipage}[b]{0.35\textwidth}
        \includegraphics[width=\textwidth]{img/pavlopoulos_bert_sep_classifier.png}
    \end{minipage}


    \caption{Clasificadores que consumen contexto propuestos por \citet{pavlopoulos2020toxicity}. Los dos primeros clasificadores proponen una arquitectura de dos encoders, uno para el texto y otro para el contexto usando bi-LSTMs y BERT como posibilidades. El tercer clasificador propuesto es un BERT usando su estructura natural para codificar dos oraciones separadas por el token $SEP$ }
    \label{fig:pavlopoulos_classifiers}
\end{figure*}


En la Sección \ref{sec:dataset_previous} hemos descripto el conjunto de datos construído por \citet{pavlopoulos2020toxicity}, dedicado a la detección de toxicidad y que incorpora información conversacional sobre comentarios de Wikipedia Talk Pages. Nos detenemos un momento para analizar sus experimentos de clasificación ya que guardan importantes similaridades con lo hecho en este capítulo. En ese trabajo se obtuvieron dos conjuntos de entrenamiento: uno en el cual los etiquetadores tenían información del contexto, y otro conjunto en el que no. El conjunto de test, por otro lado, fue anotado teniendo en cuenta el contexto bajo la asunción de que el etiquetado es de mejor calidad al tener más información contextual. Sobre la base de estos datos, los autores plantearon dos preguntas:

\begin{itemize}
    \item ¿Mejora el rendimiento de los clasificadores que son entrenados con el conjunto de datos etiquetado con contexto?
    \item ¿Mejora el rendimiento de los clasificadores consumiendo información contextual?
\end{itemize}

Para responder estas preguntas, los autores consideraron las siguientes combinaciones para sus experimentos: utilizar conjunto de entrenamiento etiquetado con o sin contexto, y entrenar el clasificador con o sin contexto. Para aquellos clasificadores que no consumen contexto, los autores consideraron las mismas alternativas que hemos visto en capítulos anteriores: bi-LSTM o \bert{}. Para aquellos que sí consumen contexto, se evaluaron dos estrategias: la primera consistió en usar una única red que codifique la entrada del texto y el contexto concatenada con un token especial; la segunda consistió en usar dos codificadores distintos para el contexto y el texto. Para la segunda alternativa, y dados los recursos computacionales disponibles, no utilizaron dos modelos pre-entrenados, sino un codificador LSTM para el contexto y un \bert{} para el texto. A su vez, también utilizaron la API Perspective de Google con la misma estrategia de concatenación. Para todas las combinaciones posibles, la mejora en el rendimiento resultante de disponer de información contextual no es estadísticamente significativa.

Dos versiones de \bert{} fueron utilizadas como base para entrenar los modelos de Transformers: una, usando los pesos del modelo de \bert{} de \citet{devlin2018bert}; y la segunda, haciendo un ajuste de dominio de \bert{} sobre un dataset grande y no etiquetado relacionado a la tarea en cuestión. Este proceso de \emph{ajuste de dominio} o \emph{fine-tuning} consiste en ajustar el modelo de lenguaje sobre un conjunto de datos no etiquetados y afines a nuestra tarea final. Esta técnica ha demostrado ser efectiva para lograr mejoras sensibles en el desempeño de clasificadores sobre dominios particulares \cite{gururangan-etal-2020-dont}, y será estudiada más detenidamente en el Capítulo \ref{chap:07_domain_adaptation}. Para este trabajo, el ajuste es realizado sobre un subconjunto de comentarios del dataset de \emph{Civil Comments} \cite{borkan2019civil} sin ningún tipo de contexto. A priori, ajustar el modelo de lenguaje sobre comentarios a secas podría inducir a pensar que puede deteriorar el rendimiento al entrenar posteriormente sobre contexto; sin embargo, en la versión no adaptada de \bert{} tampoco se observó una mejora significativa en el rendimiento.

Algunas limitaciones de este trabajo marcadas por los autores son:

\begin{itemize}
    \item Contexto muy pequeño: sólo se consideraron como contexto el título de la discusión de Wikipedia Talk Pages y adicionalmente el comentario previo.
    \item El hilo completo de los comentarios es ignorado: sólo se observa el comentario previo.
    \item Los comentarios fueron muestreados aleatoriamente, sin tener en cuenta algunos ámbitos más propicios para la toxicidad.
\end{itemize}

\citet{xenos-2021-context} continuaron el trabajo de \citet{pavlopoulos2020toxicity} reetiquetando el conjunto de datos de Civil Comments con información contextual y --como mencionamos en la Sección \ref{sec:dataset_previous}-- presentando una nueva tarea de detección de sensibilidad al contexto. Usando la API Perspective (y la estrategia de concatenación básica de texto y contexto) notaron que la performance del clasificador contextualizado mejora sensiblemente si restringimos nuestra atención a comentarios más sensibles a su entorno de acuerdo a la métrica definida. De esto último puede concluirse que, a diferencia del trabajo anterior, si bien  el contexto no es realmente necesario para comprender la toxicidad del grueso de los comentarios, hay cierto subconjunto para los cuales esta información adicional resulta relevante.


\section{Tareas de clasificación propuestas}
\label{sec:tasks}

Para analizar el impacto del contexto en la detección de discurso de odio, y teniendo en cuenta que contamos de un dataset con anotaciones granulares sobre las características ofendidas, propusimos dos tareas de clasificación:

\begin{enumerate}
    \item \textbf{Detección binaria}: Dado un tweet y su contexto, predecir si contiene contenido discriminatorio.
    \item \textbf{Detección granular}: Dado un tweet y su contexto, predecir las características ofendidas (si hay alguna) y si contiene un llamado a la acción.
\end{enumerate}

%%
%%
%% Link
%% https://docs.google.com/drawings/d/11sAaOuGJlU0P61mkrPxKduFwnNOuPV31tXUFJWEwbVU/edit?usp=drive_web&ouid=117313784631536396179
%%
%%
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{img/06/hate_detection_tasks.pdf}
    \caption{Tareas propuestas de detección de discurso de odio. La tarea de detección binaria consta de predecir si un tweet contiene contenido discriminatorio, discriminando la frontera conjunta. En la tarea granular, predecimos por separado cada una de las características ofendidas, pudiendo haber más de una o ninguna.}
    \label{fig:hate_detection_tasks}
\end{figure}



Puede pensarse la tarea de detección binaria (la que usualmente se aborda en la literatura sobre el tema) como una relajación de la tarea granular: mientras la primera sólo nos permite detectar si hay o no contenido discriminatorio, la segunda requiere información más precisa acerca de las características ofendidas. Esta segunda tarea es posible dado que el conjunto de datos empleado contiene esta información, algo que usualmente se omite en otros trabajos. La tarea granular nos permite a su vez tener mayor información sobre la salida de los clasificadores y la posibilidad de interpretar mejor sus errores. La Figura \ref{fig:hate_detection_tasks} ilustra las dos tareas propuestas. Mientras en la tarea binaria sólo debemos decidir de qué lado de la frontera se encuentra un comentario (si tiene o no discurso de odio) en la tarea granular se debe decidir esto mismo para cada una de las características,


Viendo las tareas propuestas como problemas de clasificación, la detección binaria consta de predecir una sola etiqueta binaria, mientras que la tarea granular consta de predecir $n$ etiquetas binarias. Esto último puede también verse como $n$ problemas distintos de clasificación, o una tarea de \tbf{multiclasificación}. Algo a remarcar es que podemos construir un clasificador para la tarea binaria a través de un clasificador entrenado para la tarea granular tomando la disyunción lógica de sus salidas: hay discurso de odio sí y sólo sí hay al menos una característica ofendida. Retomamos esta idea más adelante al hablar de cómo evaluamos nuestras técnicas de clasificación para cada tarea.


\section{Modelos de clasificación}
\label{sec:contextualized_classifiers}


Para las dos tareas mencionadas, entrenamos clasificadores neuronales basados en el modelo pre-entrenado \beto{} \cite{canete2020spanish}. Incorporamos la información contextual en cada comentario teniendo enn cuenta tres tipos de entrada por instancia: el comentario sin ningún tipo de contexto (notaremos \tbf{sin contexto}), el comentario con el tweet al que responde como contexto (\tbf{tweet}), y finalmente el comentario con el tweet al que responde y el texto del artículo periodístico (\tbf{tweet + artículo}). Para las dos versiones que consumen información contextual, separamos el texto y el contexto con el token especial \septok{}.

%%
%%
%% Link a Draw
%% https://docs.google.com/drawings/d/1F8iVSIRqHhGkQ0zglxqXLGD36RHZ9OhHMZYsg_xFOS4/edit
%%
%%

\begin{figure}
    \centering
    \includegraphics[width=1.10\textwidth]{img/06/bert_contextual_classifier.pdf}
    \caption{Modelo de multiclasificación para la tarea granular basado en \beto{}. Los modelos son entrenados de tres maneras distintas: sin contexto (sólo el comentario), con el contexto del tweet, y con el contexto del tweet y el texto del artículo. La salida consta de la probabilidad de que el comentario tenga contenido odioso para alguna de las características en cuestión o bien contenga una llamada a la acción}
    \label{fig:05_multi_bert_classifier}
\end{figure}

Para la tarea binaria, la salida de los clasificadores es un puntaje entre 0 y 1 representando la probabilidad de que el comentario tenga contenido odioso. En cuanto a la tarea de detección granular, abordamos esta tarea de manera similar a la \subtaskb{} del Capítulo \ref{chap:04_hate_speech}. Planteamos la detección granular como la predicción de nueve variables distintas: llamado a la acción (CALLS) y las ocho características ofendidas (MUJER, RACISMO, CLASE, LGBTI, CRIMINAL, ASPECTO, DISCAPACIDAD, POLITICA). En lugar de entrenar un clasificador diferente para cada característica, entrenamos un modelo de multiclasificación BERT que comparte todos sus pesos salvo las nueve capas lineales de salida. Si $y$ son las etiquetas e $\widehat{y}$ las predicciones del modelo, la función de costo empleada es:

\begin{equation*}
    L(y, \widehat{y}) = \sum\limits_{\mathclap{c \in CHAR'}} J(y_c, \widehat{y}_c)
\end{equation*}

\noindent donde $CHAR'$ es el conjunto de todas las características protegidas junto a la variable de llamada a la acción (CALLS), y $ J$ es la función de entropía cruzada. Compartir los pesos entre todas las salidas tiene dos objetivos: primero, poder generar un modelo más compacto (de otra forma serían nueve BERT distintos que suman alrededor de \num{1000}M parámetros) y segundo, compartir información común entre las distintas características atacadas, ya que guardan similaridades y muchas de ellas tienen una importante intersección, como hemos visto en la Sección \ref{sec:analisis_dataset_por_caracteristica}.

Para tener costos computacionales más amigables, limitamos los largos de las secuencias a 128, 256 y 512 tokens para los modelos con entrada sin contexto, tweet, y tweet+cuerpo respectivamente. Preprocesamos ambos tweets --contexto y texto-- utilizando las técnicas descriptas en la Sección \ref{sec:03_preprocessing}: conversión de usernames a un token especial (\verb|usuario|), tratamiento de hashtags (separación e inserción de un hashtag especial), y conversión de emojis a su representación textual. La Figura \ref{fig:05_multi_bert_classifier} ilustra el modelo de clasificación para la tarea granular, junto a los tres tipos de entrada considerados.

\subsection{Adaptación de dominio}

Una práctica cada vez más extendida en trabajos del área de clasificación de documentos es realizar una adaptación de dominio para mejorar la performance sobre la tarea final. La técnica de adaptación de dominio para modelos de lenguaje pre-entrenados consiste en continuar el pre-entrenamiento sobre un dataset grande y no supervisado relacionado a nuestro dominio o directamente sobre el dataset de la tarea si no tenemos acceso a otros datos \cite{gururangan-etal-2020-dont}. En la Sección \ref{sec:domain_adaptation_previous_work} del siguiente capítulo haremos una reseña más extensa de esta técnica, pero por lo pronto podemos entender que esta técnica ajusta el modelo de lenguaje a nuestros datos, que pueden tener diferencias considerables respecto de los empleados en el pre-entrenamiento. En nuestro caso puntual, mientras \beto{} fue entrenado en Wikipedia y textos formales, nuestro dominio consta de comentarios en Twitter a notas periodísticas, con expresiones muy distintas a las encontradas en medios más formales.

\begin{table}[t]
    \centering
    \begin{tabular}{lr}
        \toprule
        Hiperparámetro & Valor         \\
        \midrule
        Pasos               & \num{10000}           \\
        Tamaño de batch     & \num{2048}            \\
        Tamaño de secuencia & 128, 256 y 512  \\
        $\beta_1$           & $0.9$           \\
        $\beta_2$           & $0.98$          \\
        $\epsilon$          & $10^{-6}$       \\
        decay               & $0.01$          \\
        LR pico             & $0.0004$     \\
        Pasos de warmup     & $0.1$             \\
        \bottomrule
    \end{tabular}
    \caption{Hiperparámetros para la adaptación de dominio de BERT}
    \label{tab:hs_ft_hyperparameter}
\end{table}


\citet{pavlopoulos2020toxicity} realizaron una adaptación de dominio sobre los comentarios del corpus de \emph{Civil Comments} teniendo únicamente en cuenta los comentarios, sin utilizar ningún tipo de contexto. Proponemos a diferencia de este trabajo, tres tipos de adaptaciones de acuerdo al tipo de contexto considerado: una adaptación sin contexto, una adaptación con el contexto del tweet, y una adaptación con el contexto del tweet y el cuerpo de la noticia. Los datos usados para este ajuste fueron el sobrante de la recolección del anterior capítulo: alrededor de $288$ mil artículos con $5$ millones de comentarios que no están incluídos en el conjunto de datos etiquetado \footnote{Utilizamos algunos datos extra recolectados a posteriori de lo mencionado en el capítulo anterior}.



\cite{liu2019roberta} recomienda descartar en el pre-entrenamiento el objetivo NSP, con lo cual realizamos nuestro ajuste de dominio exclusivamente con la tarea de modelado de lenguaje enmascarado (MLM). La Tabla \ref{tab:hs_ft_hyperparameter} contiene los hiperparámetros utilizados al correr la adaptación de dominio correspondientes al optimizador Adam con warmup lineal para el learning rate. Estos ajustes los realizamos sobre una \emph{TPU v2-8} y una máquina de \emph{Google Colab Pro}, tomando alrededor de 10 hs en su largo de cadena máximo.


\subsection{Performance humana de la tarea}


\begin{table}
    \centering
    \begin{tabular}{l cc  cc}
                   & \multicolumn{2}{c}{Entre anotadores} & \multicolumn{2}{c}{Gold} \\
        {}         &  F1 mean&  F1 median  & F1 Mean  &  F1 Median \\
        \hline
        ODIO       &  $65.3$ &   $67.5$    & $82.9$   &   $85.1$   \\
        CALLS      &  $43.4$ &   $49.5$   &  $70.4$   &   $84.2$  \\
        MUJER      &  $49.0$ &   $46.8$   &  $74.1$   &   $75.9$  \\
        LGBTI      &  $59.6$ &   $57.7$   &  $84.6$   &   $91.5$  \\
        RACISMO    &  $65.3$ &   $64.4$   &  $87.1$   &   $87.9$  \\
        CLASE      &  $44.3$ &   $44.4$   &  $72.2$   &   $73.2$  \\
        POLITICA   &  $46.1$ &   $43.6$   &  $79.5$   &   $81.5$  \\
        DISCAPACIDAD& $55.0$ &   $60.0$   &  $81.3$   &   $84.2$  \\
        APARIENCIA &  $64.9$ &   $74.3$   &  $83.1$   &   $91.5$  \\
        CRIMINAL   &  $52.7$ &   $58.0$   &  $84.1$   &   $92.9$  \\
        Macro F1   &  $53.4$ &   $55.4$   &  $79.6$   &   $84.8$  \\
        \hline
    \end{tabular}

    \caption{Estadísticos de las cotas de performance entre anotadores. Cada característica es tomada como una etiqueta binaria independientemente del cálculo de la métrica para discurso de odio. La Macro F1 es el promedio de los F1 todas las características y la F1 de la llamada a la acción (CALLS). Las dos primeras columnas marcan las métricas medidas entre anotadores, y las dos últimas la de los anotadores contra la etiqueta gold}
    \label{tab:ia_f1_scores}
\end{table}


Como observamos en la anterior sección, la tarea de detección de lenguaje discriminatorio contiene una alta cantidad de ruido, y el acuerdo entre humanos es moderado. En este contexto, cabe preguntarse una cota a la performance que puede lograr un clasificador para esta tarea, que claramente por la misma naturaleza del problema, va a distar mucho de la perfección. Para obtener algunas medidas de esto, calculamos en primer lugar las F1 usando todos los posibles pares de anotadores. Como la F1 es simétrica no necesitamos hacer ninguna asunción sobre sus roles.

Algo a tener en cuenta es que la métrica final será contra la etiqueta resultante de la votación mayoritaria (nuestro \emph{gold standard}). Una cota que seguro está por arriba de nuestra performance es el acuerdo que haya entre los anotadores y este \emph{gold standard}; hay que también observar que cada etiqueta asignada (ver sección \ref{sec:assign}) codifica información de cada anotador, con lo cual éste número es una cota superior pero puede que sobreestimada.


La tabla \ref{tab:ia_f1_scores} contiene estadísticos para estas cotas, tanto entre anotadores como contra el \emph{gold-standard}. Como podemos observar, la mediana entre anotadores de la F1 (usada para obviar outliers) es relativamente baja para la detección de odio ($\sim 0.67$), mientras que contra el gold standard es de $0.85$. De esto entendemos que la performance una cota superior a la performance está entre esos dos números.



\section{Resultados}

\input{src/06_results.tex}

\section{Análisis de error}

\input{src/06_error_analysis.tex}



\section{Discusión}

Para analizar el impacto del contexto en la detección de discurso de odio, planteamos dos tareas de clasificación sobre el dataset construído en el capítulo \ref{chap:05_dataset_creation}: la tarea de detección binaria, donde respondemos si un comentario contiene discurso de odio; y una tarea de detección granular, donde se debe mencionar la o las características protegidas ofendidas (si agrede a las mujeres, al colectivo LGBTI, si es racista, etc). En ambas tareas planteadas propusimos clasificadores que consumen 3 tipos de entrada: el comentario sin contexto, el comentario con el contexto del tweet al que responden, y el comentario más el tweet al que responde y también el texto del artículo. Pudimos observar que el contexto parece dar una mejora moderada pero significativamente estadística en la tarea de detección del discurso de odio (alrededor de 3 puntos F1), y una mejora considerable en la tarea característica ofendida (alrededor de 6 puntos F1 medios).

Esto, en algún punto, indicaría que el contexto puede ser aprovechado para mejorar los algoritmos de detección de discurso de odio. Si bien este resultado podría estar en aparente contradicción con trabajos recientes que no encontraron ninguna mejora en el uso del contexto en la detección de toxicidad \cite{pavlopoulos2020toxicity}, se puede señalar que la detección del discurso de odio es una de las formas más complejas de comportamiento ``tóxico'' y, como tal, podría permitir que los clasificadores tengan más información para predecir si el texto dado es odioso o no. Otra razón detrás de este resultado es el dominio de nuestro conjunto de datos: mientras que \citet{pavlopoulos2020toxicity} usa el contexto conversacional, nosotros usamos el título y el cuerpo del artículo como contexto para los comentarios de los usuarios. Más recientemente, y como marcamos en la sección de trabajo previo, en \citet{xenos-2021-context} han observado que, restringiendo el análisis a un subconjunto de comentarios sensibles al contexto (ver \ref{sec:06_classification_previous} y \ref{sec:dataset_previous} para más información), la performance de los algoritmos de detección de toxicidad mejoran de manera significativa.

La utilización de un contexto más largo como el artículo de la noticia no mejora la performance de los clasificadores en nuestros experimentos. Hay varias interpretaciones posibles de esto: en primer lugar, los humanos suelen contestar sin leer el artículo, con lo cual este resultado pareciera ser coherente con esta observación. Por otro lado, los humanos solemos tener acceso a un contexto mucho más rico, muchas veces equivalente a haber leído la nota, algo que nuestros clasificadores carecerían. Podría también esto ser atribuido al modelo pre-entrenado que usamos para codificar esto (BETO, la variante en español de BERT) suele estar pre-entrenada para textos más cortos. Teniendo esto en cuenta, realizamos el ajuste de dominio usando el contexto largo, pero aún así la performance del clasificador que consume este contexto largo se mantuvo por debajo del que usaba el contexto simple.

El análisis del error realizado demostró que, si bien el contexto pareciera mejorar la detección de discurso de odio, para muchas características protegidas se mantiene aún como una tarea difícil. Un caso ejemplificador de esto es la discriminación contra el colectivo LGBTI. En las instancias del dataset --y en muchos de los ejemplos en los que el algoritmo de detección falla--  puede verse que las agresiones contra este colectivo y sus miembros son sumamente sofisticadas, lejos de las agresiones meramente basadas en insultos u otras palabras ofensivas. Nuestros clasificadores, aún en sus mejores versiones (usando ajuste de dominio y contexto) obtuvieron una baja performance en la detección de este fenómeno (alrededor de $0.5$ F1 score) dando cuenta que es un problema no trivial y merece ser analizado más detenidamente debido a la complejidad de estos mensajes, que suelen reunir ironía, metáforas, y otros artilugios que hacen difícil su detección.

En el caso de la categoría mujer, inesperadamente, también obtuvimos una performance muy baja en la detección de agresiones misóginas. En el análisis de error, podemos observar que tenemos casos complejos de descifrar que fueron marcados por nuestros anotadores: por ejemplo, ataques velados a mujeres víctimas de violación (llamarlas mentirosas). \todo{Escribir un poco más sobre esto}

Algo que debe ser tenido en cuenta para matizar estos resultados es que utilizamos un amplio espectro de características protegidas. Incluso, la que más se beneficia del contexto es una que introdujimos ad-hoc para este experimento (discurso de odio contra criminales). En contrapeso, otras características ``no convencionales'' son poco beneficiadas por el contexto (como discurso de odio en base a la apariencia, opinión política y discapacidad).

Un resultado que también observamos es que pareciera ser que nuestros clasificadores mejoran leve pero significativamente su performance en un contexto de aprender cada característica por separado en vez de sólo aprender a distinguir la etiqueta binaria de discurso de odio. Si bien la mejora es marginal (cerca de un punto de F1) y no es observable de manera subjetiva mediante un análisis de error, una posible razón detrás de esto es que la señal más precisa acerca de la categoría ofendida puede ayudar a distinguir mejor las fronteras de este fenómeno. Aún cuando esta mejora no fuese tal, poder tener una salida más interpretable y granular es mejor que simplemente obtener una predicción binaria.

Una limitación importante de este estudio es que el entrenamiento lo realizamos sobre datos entrenados únicamente observando el contexto. Entrenar a los clasificadores no contextualizados sobre estas etiquetas induce a los clasificadores a tomar decisiones erróneas, como asumir que celebraciones son discurso de odio debido a instancias que --con contexto-- tienen esa naturaleza. Un estudio más completo del impacto del contexto en esta tarea debería incluir los datos entrenados sin contexto.

En el terreno de la aplicación, un problema práctico de este resultado es que no siempre tenemos un contexto disponible para un texto dado. Incluso si podemos encontrarlo, a veces este contexto puede no ser en forma de artículo de noticias, sino como un hilo de conversación o incluso de alguna otra representación. Teniendo en cuenta alguna de las consideraciones hechas en esta discusión, una posible línea de investigación seria la de incorporar distintos tipos de contexto, desde más mensajes en el hilo de la conversación, conocimiento estructurado (por ejemplo, la propuesta en \emph{ERNIE} \cite{zhang2019ernie} o \emph{KI-BERT} \cite{faldu2021ki}) o bien una combinación de diversas fuentes, incluso multimodales.

\section{Conclusiones}

En este capítulo hemos realizado experimentos de clasificación sobre el dataset construído en el capítulo anterior, focalizándonos en analizar el impacto de la utilización del contexto en la performance de los modelos. Planteamos dos tareas: una tarea binaria --detectar si existe discurso de odio o no-- y una tarea granular --definir qué categorías son atacadas en un tweet, si es que las hay--. Para ambas tareas, los modelos contextualizados obtienen mejoras significativas en la performance, dando indicios de que información adicional al comentario analizado puede ayudar a detectar el discurso de odio. Si bien en nuestros experimentos el contexto más pequeño (el tweet del artículo de la noticia) fue el que mejor resultados obtuvo, una línea de trabajo futuro podría explorar otras formas de incorporar el contexto más largo -- en este caso, el artículo de la noticia.

Así mismo, observamos una pequeña pero estadísticamente significativa mejora en la detección de discurso de odio al entrenar un clasificador granular al ser evaluado de manera binaria. En este caso, obtenemos una ventaja al obtener una salida más interpretable de las características ofendidas, y que además que no sólo no empeora la performance de nuestro clasificador sino que hasta incluso mejora levemente.

Del análisis de error, se observa que algunas categorías del discurso de odio se muestran elusivas para los algoritmos de detección. Uno de estos casos son los mensajes abusivos contra la comunidad LGBTI+, conteniendo mensajes semánticamente complejos, implícitos y con metáforas que son esquivas para los modelos propuestos. A pesar de estas limitaciones, esta característica fue una de las más beneficiadas por la adición de contexto, aunque su desempeño sigue siendo bajo, teniendo una puntuación de F1 de alrededor de 0.5.

Podemos concluir que los datasets de discurso de odio deberían --en la medida de lo posible-- contener \textbf{información contextual} sobre los comentarios analizados. Esta información puede darse en forma de artículos de noticias, como un hilo de conversación, o incluso como otras formas --por ejemplo, como una base de conocimiento. Sobre esto, trabajo futuro debería explorar el impacto de utilizar esta información adicional para integrarla en algoritmos de detección de discurso de odio. La evidencia de estos experimentos --por ahora preliminares, y con las limitaciones marcadas en la discusión-- indica que los modelos del estado del arte pueden utilizar esta información para mejorar la detección de discurso de odio en redes sociales. En segundo lugar, los datasets de discurso de odio deberían incluir \textbf{información granular} acerca de las características atacadas --y no sólo una etiqueta binaria-- ya que por un lado esto mejora la interpretabilidad de los algoritmos de detección, y resultados preliminares de este estudio indicarían que mejoran marginalmente la performance en la detección en general.

Finalmente, un aspecto que introdujimos en este capítulo fue el de adaptar un modelo de lenguaje pre-entrenado a su dominio, siendo en nuestro caso los comentarios sobre notas periodísticas en Twitter. Las mejoras que reportó la utilización de estas técnicas fue significativa, en consonancia con otros trabajos recientes. Pasaremos a continuación a estudiar estas técnicas en el marco más general de la clasificación de textos sociales.