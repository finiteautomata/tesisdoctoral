\section{Redes Neuronales}

\subsection{Multi-layer perceptron}

Una red neuronal puede pensarse simplemente como una función $f: \mathbb{R}^m \rightarrow O$ que intenta aproximar una función $f^*$ con misma aridad. Podremos notar $ y = f(x; \Theta)$, siendo $\Theta$ los parámetros de dicha función

El perceptrón, desarrollado en 1958 por Frank Rosenblatt \cite{rosenblatt1958perceptron}, intenta ajustar

\begin{equation*}
    y = H(\Theta_1 x + \Theta_0)
\end{equation*}

donde $H$ es la función de activación, y $\Theta = (\Theta_0, \Theta_1)$ son los parámetros de la función. Este modelo es el primero cuyos pesos se encontraban mediante un algoritmo, considerando que $H$ es una función derivable. Este puede considerarse como el primer modelo de una neurona, junto al modelo de McCulloch-Pitts.

\citet{minsky1969perceptrons} demostraron que este tipo de modelos sólo pueden ajustarse a datos linealmente separables, provocando el primer ``invierno'' de las redes neuronales.

Una forma de sortear estas dificultades planteadas es ``apilar'' (stack en inglés) varias de estas funciones para poder ajustar a más tipos de funciones. En términos matemáticos, esto es tan sólo una composición de funciones, tomando ahora $f = f_3 \circ f_2 \circ f_1$, donde $f_1$ es la primer ``capa'' de nuestra función correspondiente a la entrada, $f_2$ es la capa intermedia u oculta, y $f_3$ es la capa de salida. Si bien este ejemplo consta de 3 capas, se puede generalizar a arbitrarias capas ocultas. Este modelo es el que conocemos como \textbf{Perceptrón Multicapa} o \textbf{Multi-Layer Perceptron} (MLP por sus siglas en inglés)

\subsection{Redes neuronales recurrentes}

Los problemas descriptos de NLP suelen constar de procesar una secuencia de palabras o tokens $x_1, x_2, \ldots, x_k$ de longitud variable, de manera de ajustar a una función

\begin{equation*}
    y = f([x_1, \ldots, x_k])
\end{equation*}

Una manera de ajustar una función de este tipo (usando una entrada de largo fijo) es convertir este problema a ajustar una función autorregresiva

\begin{equation*}
    y_k = f(x_k, y_{k-1})
\end{equation*}

donde tenemos una salida para cada paso $k$ de tiempo. Si $f$ es una red neuronal, llamamos a este tipo de redes neuronales \textbf{recurrentes}, ya que la salida a cada paso ($y_k$) depende de la salida del paso anterior, $y_{k-1}$\footnote{No confundir con las redes neuronales recursivas}.

Una primer aproximación a este problema es la red recurrente de Elman \cite{elman1990finding} definida por las siguientes ecuaciones

\begin{align}
h_t &= \sigma(W_h x_t + U_h h_{t-1} + b_h) \\
y_t &= \sigma(W_y h_t + b_y)
\label{eq:elman}
\end{align}

$h_t$ es normalmente llamado el \textbf{estado oculto} en las redes neuronales recurrentes. Los parámetros a ajustar son $W_h, U_h$ (matrices) y $b_h, b_y$ (escalares). Podemos ver que, a grandes rasgos, este tipo de red recurrente no es nada más que un perceptrón multicapa cuya entrada consta de $x_t$, la entrada original en el tiempo actual $t$, y el estado oculto anterior, $h_{t-1}$.

Para entrenar este tipo de redes recurrentes utilizamos back-propagation through time (BPTT), que consta en desplegar la relación recurrente y aplicar back-propagation de manera normal. \todo{explicar un poco mejor esto}

Este tipo de redes recurrentes sufren de varios problemas: entre ellos, \textbf{vanishing gradient} y \textbf{exploding gradient}. Estos problemas pueden observarse ya que el cálculo del gradiente de las ecuaciones \ref{eq:elman} usando BPTT induce la potencia a la $n$ (donde $n$ es el largo de la secuencia) de las matrices $W_h$ y $U_h$. Usando la descomposición de Jordan de estas matrices, podemos ver que sus elementos en la diagonal que sean distintos de 1, o bien tienden a infinito o a cero.

El problema de \textbf{exploding gradient} puede solucionarse mediante la técnica de \textbf{gradient clipping}\todo{citation needed}, que consta de reajustar la norma del gradiente. Sin embargo, nos queda aún el problema de \textbf{vanishing gradient}. Para ello, se han propuesto otras arquitecturas recurrentes.

\citet{hochreiter1997long} propusieron las \textbf{Long Short-Term Memory} (LSTM) como solución a estos problemas. Para solucionar los problemas mencionados, proponen una arquitectura basadas en compuertas (\textbf{gates}) que regulan los cambios en el estado oculto y en la salida. Concretamente, la arquitectura de las LSTMs está regida por las siguientes ecuaciones\footnote{Para una muy buena explicación de las redes recurrentes, sugerimos este artículo: \url{https://colah.github.io/posts/2015-08-Understanding-LSTMs/}}:


\begin{align}
    h_t &= \sigma(W_h x_t + U_h h_{t-1} + b_h) \\
    y_t &= \sigma(W_y h_t + b_y)
    \label{eq:lstm}
\end{align}



\subsection{Optimización}


\section{Técnicas de representación}

Una de las necesidades que tienen las redes neuronales para poder trabajar con textos es el de tener representaciones continuas de cada token o palabra. Las representaciones utilizadas en la época previa de los modelos lineales --bolsas de palabras/caracteres ponderadas con algún esquema similar a TF/IDF-- adolecen de varios problemas: tienen una altísima dimensionalidad; no tienen representación semántica de la similaridad de las palabras; están concentradas en una o pocas dimensiones y suelen ser discretas.

Trabajo previo ha demostrado que obtener representaciones continuas y distribuidas (de manera opuesta a discretas y concentradas)

Latent Semantic Analysis (LSA) \cite{landauer1997solution} es una de las primeras técnicas de representación continua utilizadas para esta tarea. Plantean el problema de obtener representaciones continuas como el de factorizar una matriz de co-ocurrencia entre tokens y documentos (o contextos). LDA (Latent Dirichlet Allocation) \cite{blei2003latent} es otra técnica basada en modelos gráficos entrenados mediante métodos variacionales, muy utilizada aún en la actualidad ya que genera representaciones latentes de los tópicos de los textos.

\begin{figure}
    \centering
    \includegraphics[width=0.65\textwidth]{img/02/bengio_neural_language_model.pdf}
    \caption{Ilustración del modelo de lenguaje neuronal de \citet{bengio2003neural}. La entrada consta de $n-1$ palabras, que primero pasan por una lookup table (o capa de embeddings), una función de activación y son colapsadas para luego ser utilizadas como entrada de una función softmax.}
    \label{fig:bengio_neural_language_model}

\end{figure}

Dentro de los métodos neuronales, uno de los más populares ha sido el de \citet{bengio2003neural} que propone una arquitectura neuronal para un modelo de lenguaje markoviano. La arquitectura de esta red está ilustrada en la figura \ref{fig:bengio_neural_language_model}. En la capa intermedia contiene una tabla de lookup de vectores de las diferentes palabras (también conocido como capa de embeddings) donde se generan las representaciones de las palabras. Trabajo posterior (con diferentes variaciones de esta misma idea) como el de \citet{collobert2011natural} ha demostrado que la utilización de este tipo de representaciones es útil para diversas tareas de NLP como POS Tagging, NER, y otras.

Uno de los problemas de los métodos vistos hasta el momento es que sufrían problemas de eficiencia, sólo pudiéndose entrenar con pocos millones de palabras y con dimensiones reducidas. La técnica \emph{word2vec} \cite{mikolov2013efficient} permite entrenar representaciones de palabras de mayor dimensión y sobre grandes cantidades de textos de manera eficiente. Los vectores de palabras guardan cierta estructura lineal y semántica, como ilustran los autores con algunos ejemplos, como el ya clásico $v(\text{rey}) - v(\text{hombre}) + v(\text{mujer}) \approx v(\text{reina})$.

Para generar los vectores de \emph{word2vec}, los autores plantean una relajación del problema de modelado de lenguaje mediante dos alternativas: Continuous Bag of Words (CBOW) y Skip-Gram. En CBOW intentamos predecir la palabra faltante dada una bolsa de palabras del contexto, y en skip-gram intentamos predecir las palabras del contexto dada la palabra central. Para ambos problemas, se generan representaciones intermedias ricas para las distintas palabras. \citet{mikolov2013efficient} extiende la idea del anterior trabajo proponiendo plantear el problema de skip-gram como uno de distinguir ruido de palabras efectivamente del contexto, haciendo mucho más eficiente el cálculo de estas representaciones. GloVe \cite{pennington2014glove} es otra técnica de representación de palabras que combina las ideas de factorización de matrices de LSA  mediante un problema de optimización distinto y generando representaciones que superan ligeramente en algunos benchmarks de tareas a los de \emph{word2vec}.

Uno de los problemas que tienen estos métodos es que cada representación se calcula sobre las distintas palabras. En español, por ejemplo, las palabras gato, gata, gatito, gatuno, todas tienen representaciones independientes en \emph{word2vec}, a pesar de tener información morfológica en común. Esto es un problema en varios escenarios: idiomas con muchas inflexiones o aglutinantes (como el turco, alemán o finés) o --lo que es de nuestro interés-- texto altamente desnormalizado como el de redes sociales. La técnica \fasttext{} \cite{bojanowski16} extiende la idea de \emph{word2vec} mediante la asignación de vectores a secuencias de 3 caracteres (subpalabras), capturando así cierta información morfológica. La representación de una palabra se obtiene mediante una combinación lineal de los vectores de las subpalabras que la componen.

\subsection{Tweet Embeddings}
\label{sec:02_tweet_embeddings}

%%
%%
%%
%%  https://docs.google.com/drawings/d/1BU3ulBiqU0NojpW6Fkb4xFlMCDigSWwfjN7z9smO6nY/edit
%%
%%

\begin{figure}[t]
    \centering
    \includegraphics[width=0.80\textwidth]{img/tweet_embeddings.pdf}
    \caption{Representación contínua de un tweet mediante combinación lineal de las representaciones de cada palabra.}
    \label{fig:tweet_embeddings}
\end{figure}

Una forma relativamente simple de obtener una representación de un documento u oración (en nuestra tesis, esto será casi siempre un tweet) es realizar una combinación lineal de las representaciones obtenidas para cada palabra. Es decir, dada una oración $s = w_1 w_2 \ldots w_n$, y representaciones $\overline{w_1}, \overline{w_2}, \ldots, \overline{w_n} \in \mathbb{R}^m$, podemos obtener una representación

\begin{equation}
    \overline{s} = \sum\limits_{i=1}^{n} \alpha_i \overline{w_i}
\end{equation}

con $\alpha_1, \ldots, \alpha_n \in \mathbb{R}$ escalares (dependientes de la oración). De esta manera, obtenemos de $n$ representaciones independientes del contexto una representación para el tweet, sin tener en cuenta posibles interacciones entre los distintos componentes. La figura \ref{fig:tweet_embeddings} ilustra esta metodología simple para obtener representaciones de oraciones.

Tenemos entonces dos posibilidades para determinar la combinación lineal: la forma de obtener las representaciones, y la forma de calcular los coeficientes. Para las representaciones, podemos usar varias de las técnicas que ya vimos como word2vec, GloVe, o \fasttext{}. Para calcular los coeficientes, consideramos en nuestro trabajo dos formas. La primera, la forma canónica, calculando un promedio de las representaciones, es decir, tomando $\alpha_i = \frac{1}{n}$

Se utilizaron combinaciones lineales para calcular una representación de un solo tweet.
Seguimos dos enfoques simples: promedio simple y promedio ponderado. En el segundo caso, utilizamos un esquema que se asemeja a la frecuencia inversa suave (SIF) \cite{arora17}, inspirado TF-IDF.
Cada palabra $ w $ se pondera con $ \frac {a} {a + p (w)} $, donde $ p (w) $ es la palabra probabilidad unigrama y $ a $ es un hiperparámetro de suavizado.
Los valores altos de $ a $ significan más suavizado hacia el promedio simple.



\section{Transfer Learning y modelos pre-entrenados}

\subsection{ELMo y ULMFiT}
\label{subsec:elmo}

Hasta cerca de 2018, la forma canónica de abordar un problema de NLP era entrenar una red neuronal recurrente que consumiera embeddings no contextualizados de los tokens de entrada. Esta arquitectura tiene algunas limitaciones; una de ellas es que, dados dos o más problemas distintos (por ejemplo, análisis de sentimientos e inferencia de lenguaje natural --NLI--) lo único compartido por ambas arquitecturas es la capa más baja de la red -- la capa de embeddings -- teniendo que entrenar desde cero todo el resto de los parámetros del modelo. En términos coloquiales, cada red debe ``aprender a leer'' sobre cada tarea, ignorando muchas construcciones sintácticas y semánticas comunes del lenguaje.

Uno de los primeros esfuerzos exitosos en sobrepasar los embeddings no contextualizados es \elmo{} \cite{peters2018}. Este modelo aprende embeddings ya no sobre una única palabra como \emph{word2vec} y sus variantes, sino sobre todo una oración, generando representaciones contextualizadas de cada palabra.  Para aprenderlas, \elmo{} se entrena sobre modelo de lenguaje bidireccional \footnote{En realidad no es estrictamente bidireccional, sino dos modelos de lenguaje concatenados} recurrente de varias capas sobre grandes cantidades de texto. En dicho trabajo, utilizan luego una combinación lineal de la salida de cada capa para obtener representaciones contextualizadas de cada token. Esta misma idea es una continuación \citet{peters2017semi}, y también parcialmente de \citet{mccann2017learned}; en este último trabajo abordan la construcción de representaciones contextualizadas mediante la tarea de traducción automática.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{img/02/ulmfit.pdf}
    \caption{Universal Language Modeling for Text Classification (ULMFiT). Esquema del método planteado:}
    \label{fig:ulmfit}
\end{figure}

Alrededor de 2018, este paradigma comenzó a cambiar hacia un esquema donde se entrena una red neuronal sobre una tarea genérica para luego ajustarla a la tarea específica, algo muy común en el área de Computer Vision. \citet{howard-ruder-2018-universal} introdujeron la técnica de ULMFiT(Universal Language Modeling for Fine-tuning for text classification), uno de los trabajos fundamentales de este nuevo paradigma. La idea propuesta se puede resumir en entrenar un modelo de lenguaje sobre un gran dataset no etiquetado, y luego utilizar esa misma red (cambiándole la última capa) ajustándola a una tarea específica.

Los autores proponen 3 etapas: primero, el pre-entrenamiento sobre la tarea de modelado de lenguaje en un gran dataset de texto (e.g. Wikipedia o Common Crawl); segundo, un ajuste de la tarea de modelado de lenguaje sobre el texto de la tarea en cuestión (LM fine-tuning); y finalmente, el entrenamiento sobre las etiquetas de la tarea (Classifier fine-tuning). La figura \ref{fig:ulmfit} ilustra las 3 etapas para el problema de clasificación de sentimientos. Entre varias técnicas que utilizan para entrenar estos modelos y evitar el olvido catastrófico, vale destacar el uso de \emph{slanted triangular learning rates}, donde el learning rate tiene una etapa de \emph{warmup} donde sube hasta el pico y luego una etapa de \emph{annealing} donde se reduce linealmente hasta 0 por el resto del entrenamiento. Esta técnica será utilizada por \bert{} y otros modelos de lenguaje basados en transformers.

El modelo de lenguaje utilizado por los autores de ULMFiT utiliza una arquitectura AWD-LSTM \cite{merity2018regularizing}. Estas arquitecturas recurrentes fueron el estado del arte hasta el momento, pero fueron sobrepasados por las basadas en \emph{transformers}, que pasaremos a detallar.


\subsection{Modelos basados en atención}
\label{sec:02_transformers}

Una de las limitaciones de los modelos basados en redes recurrentes es que sufren un \tbf{sesgo de localidad o secuencialidad}(locality bias) \cite{battaglia2018relational}. En palabras coloquiales, las redes recurrentes tienen problemas para aprender dependencias de largo rango en las oraciones, siendo esto producto de su arquitectura autorregresiva donde se construye la salida $y_t$ en base a $y_{t-1}$. Este sesgo es particularmente dañino en tareas de transducción de sequencias con la arquitectura encoder-decoder básica ya que a esto se le suma un cuello de botella forzoso por la compresión de toda la secuencia de entrada en un vector de longitud fija. \footnote{\citet{sutskever2014sequence} de hecho en su trabajo observa que invertir la oración de entrada obtiene mejores resultados para la tarea de traducción automática}.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{img/02/attention_model.pdf}
    \caption{Mecanismo general de atención. En azul, la salida del encoder recurrente de la entrada. En rojo, la salida del decoder recurrente. Fuente: \citet{luong2015effective}}
    \label{fig:attention_mechanism}
\end{figure}


Una de las formas de mitigar este sesgo es la utilización de mecanismos de \emph{atención} \cite{bahdanau2014neural}. Suponiendo una arquitectura recurrente de encoder y decoder, y siguiendo la notación de \citet{luong2015effective}, para la tarea de traducción automática de una secuencia $(x_1, \ldots , x_n)$ a $(y_1, \ldots , y_m)$, con estados ocultos $(\overline{h_1}, \ldots , \overline{h_n})$ para la entrada y $(h_1, \ldots , h_m)$ y para la salida, el mecanismo de atención \footnote{global, en \citet{luong2015effective} se menciona el mecanismo local que no consideramos} consta de calcular para cada paso $t$ un vector de contexto

\begin{equation*}
    c_t = \sum_{i=1}^n \alpha_i^{(t)} \overline{h_i}
\end{equation*}

donde $\alpha^{(t)}$ es el vector de alineamiento, calculado como

\newcommand{\score}[0]{\text{score}}

\begin{equation*}
    \alpha^{(t)} = \softmax(\score(\overline{h_1}, h_t), \score(\overline{h_2}, h_t) \ldots , \score(\overline{h_n}, h_t))
\end{equation*}

Cada $\score(\overline{h_i}, h_t)$ marca una similaridad no normalizada entre sus argumentos. Siguiendo las alternativas planteadas en \citet{luong2015effective}, tenemos algunas posibles opciones para esto:

\begin{equation}
    score(\overline{h_i}, h_t) =  \begin{cases}
        \overline{h_i}^T h_t   & \text{dot} \\
        \overline{h_i}^T W h_t & \text{general} \\
        v^T\text{tanh}(W [\overline{h_i}^T; h_t]) & concat
     \end{cases}
\end{equation}

% copypasting random stuff from the Internetz
% https://tex.stackexchange.com/questions/66537/making-hats-and-other-accents-bold
%
\newcommand{\thicktilde}[1]{\mathbf{\tilde{\text{$#1$}}}}

con $W, v$ parámetros adicionales. En el caso de la atención producto interno (dot) podemos reescribir todas las ecuaciones como:

\begin{equation}
    C = \softmax(H \widehat{H}^T ) \widehat{H}
    \label{eq:attention_product}
\end{equation}

donde $\widehat{H}, H$ son los vectores que tienen $(\overline{h_1}, \ldots , \overline{h_n})$  y $(h_1, \ldots , h_m)$ como filas respectivamente, y $\softmax$ se calcula fila a fila.

Finalmente, el vector $\widetilde{h_t}$ es calculado como una transformación del estado oculto del decoder $h_t$ y el vector contextual $c_t$:


\begin{equation*}
    \widetilde{h_t} = \tanh(W_h [h_t; c_t])
\end{equation*}


El vector $\widetilde{h_t}$ codifica información de manera global de todos los estados ocultos del codificador, mitigando este problema del sesgo de localidad. Esta técnica se convirtió en parte integral de los modelos seq2seq como ser traducción automática, sumarización, entre otras. La figura \ref{fig:attention_mechanism} ilustra esta arquitectura.

La técnica de auto-atención o intra-atención \cite{parikh-etal-2016-decomposable} consiste en aproximadamente la misma idea que la atención sólo que teniendo una única secuencia; podemos asumir ecuaciones similares con $\overline{h_i} = h_i$. La auto-atención genera representaciones de los distintos vectores de entrada observando la totalidad de la secuencia, a diferencia de las redes recurrentes que sólo construyen una representación en base al paso anterior. Esta capa se utiliza en arquitecturas para clasificación de texto encima de una capa recurrente para generar representaciones con dependencias sin distinción de la distancia entre los distintos tokens.

\section{Transformers}

\begin{figure}[t]
    \centering
    \begin{subfigure}[]{0.55\textwidth}
        \centering
        \includegraphics[width=0.55\textwidth]{img/02/transformer_architecture.png}
        \caption{Arquitectura de modelos Transformer}
        \label{fig:transformer_architecture}
    \end{subfigure}
    \begin{subfigure}[]{0.40\textwidth}
        \centering
        \includegraphics[width=0.40\textwidth]{img/02/scaled_self_attention.png}
        \caption{Arquitectura de modelos Transformer}
        \label{fig:scaled_self_attention}
    \end{subfigure}
    \caption{Modelo de transformador y su versión de auto-atención. La subfigura \ref{fig:transformer_architecture} muestra la arquitectura de los codificadores y decodificadores. Fuente: \citet{vaswani2017attention}}
    \label{fig:transformer_mechanism}
\end{figure}

Mencionamos el sesgo de la secuencialidad como uno de los problemas de los modelos recurrentes. Otro de los grandes obstáculos para las arquitecturas autorregresivas es la paralelización. El modelo de cómputo secuencial donde $h_t$ se calcula en base a $h_{t-1}$ inhibe un cálculo paralelo, donde las diferentes representaciones puedan ser generadas simultáneamente. \citet{parikh-etal-2016-decomposable} es uno de los primeros trabajos que proponen una arquitectura para el problema de inferencia (NLI) enteramente basada en modelos de atención, sin modelos recurrentes.

\citet{vaswani2017attention} introdujeron la arquitectura \emph{Transformer} para la tarea de traducción automática. Esta arquitectura no utiliza capas recurrentes ni convolucionales, basándose enteramente en el mecanismo de auto-atención. La figura \ref{fig:transformer_architecture} muestra la arquitectura de los modelos basados en Transformer, organizado en forma de encoder-decoder, con 6 capas de cada uno.

Cada capa del encoder utiliza un mecanismo de auto-atención múltiple seguido de una capa feed-forward punto a punto. Las dos capas de auto-atención o feed-forward están sucedidas por una conexión residual \cite{he2016deep} \footnote{El fin de estas conexiones residuales es facilitar el flujo del gradiente en arquitecturas profundas} y una normalización, de manera que la salida se expresa como:

\begin{equation*}
    \text{Layer}(x) = \text{Norm}(x + \text{subLayer}(x))
\end{equation*}

Las capas decodificadoras son similares, salvo que se les agrega una capa extra de auto-atención donde se combinan las salidas del encoder con las representaciones que genera el decoder. A su vez, las capas de multi-atención están enmascaradas para no poder ``ver'' las representaciones que se generan en pasos posteriores para guardar su naturaleza secuencial en la tarea.

El cálculo de atención utilizado en este trabajo es similar al visto en la ecuación \ref{eq:attention_product}, aunque normalizado por $\sqrt{d_k}$, donde $d_k$ es la dimensión de los vectores de entrada:

\begin{equation*}
    Attention(Q, K, V) = \text{softmax}(\frac{Q^T K}{\sqrt{d_k}}) V
\end{equation*}

Cada capa utiliza varias cabezas de auto-atención, las cuales concatena y proyecta en su salida en su salida. La salida de cada una de capa pasa por una regularización de tipo dropout \cite{srivastava2014dropout}.

Un punto no menor es que el modelo Transformer, siendo que no tiene ningún tipo de recurrencia y convolución, carece de cualquier ordenamiento de la secuencia de tokens. Para inyectar ese conocimiento en la red, utilizan \emph{vectores de posicionamiento} (positional embeddings) que se suman a los vectores de entrada de la capa de embeddings, como se ilustra en la figura \ref{fig:transformer_architecture}. Estos vectores no son parámetros entrenados (como sí lo son en \bert{}) sino que se calculan mediante funciones sinusoidales.


No nos extenderemos más en la explicación de esta arquitectura, y referimos para más información a los excelentes artículos \emph{Transformers from Scratch} \footnote{\url{http://peterbloem.nl/blog/transformers}}, \emph{Annotated Transformer} \footnote{\url{https://nlp.seas.harvard.edu/2018/04/03/attention.html}} y \emph{The Illustrated Transformer} \footnote{\url{https://jalammar.github.io/illustrated-transformer/}}.


\section{GPT, BERT, y modelos pre-entrenados basados en Transformers}

Combinando las ideas de ULMFit --entrenaje semi-supervisado sobre la tarea de modelado de lenguaje-- y la arquitectura Transformer --removiendo las redes recurrentes y facilitando la paralelización del cálculo-- en \citet{radford2018improving} se introduce GPT (\emph{generative pre-training}). Esta técnica consiste de un pre-entrenamiento sobre un gran corpus no etiquetado y luego un fine-tuning discriminativo para cada tarea, muy en la línea de \citet{howard-ruder-2018-universal}, introduciendo parámetros específicos únicamente para cada una de estas. El modelo que usa esta tarea es el de \tbf{modelado de lenguaje causal} -- es decir, de izquierda a derecha. Este modelo obtuvo para muchas tareas como el benchmark GLUE \cite{wang-etal-2018-glue} el estado del arte.


\bert{} \cite{devlin2018bert} (Bidirectional Encoder Representations from Transformers) plantea una modificación sobre GPT: en lugar de pre-entrenar el modelo sobre la tarea de modelado de lenguaje \tbf{causal} --de izquierda a derecha-- entrenar sobre la tarea de modelado de lenguaje \tbf{enmascarado}. Esta tarea (usualmente llamada \emph{Cloze task} \cite{taylor1953cloze}) consta de enmascarar una cierta cantidad de palabras de una frase, y luego intentar predecir las palabras faltantes. Por ejemplo, en la siguiente frase, consta de reemplazar los dos tokens \verb|[MASK]|:

\begin{center}
    El \verb|[MASK]| es celeste y el pasto \verb|[MASK]|
\end{center}


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{img/02/gpt_vs_bert.pdf}
    \caption{Comparación entre ELMo, GPT y BERT. }
\end{figure}

A diferencia de la tarea de modelado de lenguaje causal, los autores argumentan que esta tarea permite generar representaciones bidireccionales ricas. La figura


A modo de comparación entre las distintas estrategias de pre-entrenamiento, la figura



