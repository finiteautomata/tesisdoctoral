
\newcommand{\robertuito}[0]{\emph{RoBERTuito}}

En este capítulo exploraremos como mejorar la detección de discurso de odio desde una perspectiva más general, analizando en general la clasificación de textos sociales. Hemos visto en capítulos anteriores que las técnicas de representación utilizadas en los últimos años (desde los word-embeddings hasta ) generan representaciones ricas al ser entrenadas en dominios sociales. Así mismo, también observamos que en algunos modelos pre-entrenados (como AWD-LSTM usando la técnica de ULMFit) \todo{meter citas} realizar un ajuste de dominio más extenso sobre

Entrenar modelos de lenguaje basados en Transformers toman una cantidad de recursos importantes, algo que puede imposibilitar que

Abordaremos la pregunta ¿cómo se compara en el dominio social el ajuste de dominio de modelos pre-entrenados sobre textos formales con los modelos que fueron generados desde cero en textos sociales? Para ello, utilizamos como benchmark de este análisis las tareas que hemos tratado en esta tesis. Entrenamos desde cero un modelo de lenguaje basado en transformers (RoBERTa)\cite{liu2019roberta} sobre tweets, y comparamos su performance contra ajustes de dominio hechos sobre otros modelos pre-entrenados.

Comenzamos este capítulo haciendo una pequeña recapitulación de las técnicas de adaptación de dominio.

\section{Trabajo previo}
\label{sec:domain_adaptation_previous_work}

\citet{goodfellow2016deep} define la adaptación de dominio como una situación similar a la de Transfer Learning: dado un modelo que fue entrenado sobre una distribución de datos o dominio $P_1$, lo utilizamos sobre una distribución $P_2$ relativamente similar.

En el caso de la adaptación de dominio, nos referimos a la aplicación de alguna técnica que ajuste la distribución de la entrada (de $P_1$) a la distribución de nuestro nuevo dominio. \citet{glorot2011domain} es uno de los primeros trabajos que aplica esta técnica en NLP, usando denoising auto-encoders para este fin. \todo{Escribir un poco más de esto}

Para lo que nos concierne en NLP solemos querer, dado un modelo de lenguaje (tanto causal como enmascarado) entrenado en un dominio, ajustarlo a otro dominio distinto. Por ejemplo, un modelo BERT pre-entrenado en textos formales (como Wikipedia o noticias) queremos ajustarlo a la distribución de textos sociales, que si bien ambas mantienen el idioma (inglés o español) suelen tener distribuciones notoriamente distintas.

Dentro de la última ola que sacudió NLP de modelos pre-entrenados, ULMFit \citet{howard-ruder-2018-universal} contempla una etapa de adaptación de dominio utilizando de manera no-supervisada el texto del dataset supervisado de la tarea atacada.

Recientemente, \citet{gururangan-etal-2020-dont} analizan el impacto de los ajustes de dominio. Para ello, consideran varios dominios como ser biomédico, reviews de películas, papers de cs. de la computación (CS), y noticias. Plantean dos configuraciones de adaptación de dominio:

\begin{itemize}
    \item Domain Adaptation: ajustar el modelo de lenguaje sobre un extenso conjunto de datos no etiquetado, usualmente el ``sobrante'' del proceso de recolección que no es anotado
    \item Task Adaptation: ajustar el modelo de lenguaje sobre el dataset, de la misma manera que se hace en ULM-Fit
\end{itemize}

En todos los casos, usando modelos del estado del arte como RoBERTa muestran que aplicar conjuntamente lo que ellos consideran Domain Adaptation y Task Adaptation mejora la performance significativamente. La adaptación, dado que usan modelos como RoBERTa, consiste tan sólo en correr la tarea de MLM sobre los textos del dominio a adaptar.

\begin{table}
    \centering
    \begin{tabular}{llll}
        Nombre                                 & Idioma            & Dominio                          & Familia     \\
        \hline
        SciBERT\cite{beltagy-etal-2019-scibert}& Inglés            & Papers científicos               & BERT        \\
        BERT-CT                                &                   &                                  &             \\
        BERTweet\cite{bertweet}                & Inglés            & Tweets, algunos COVID-related    & RoBERTa     \\
        TwilBERT                               &                   &                                  &             \\
        AlBERTo                                &                   &                                  &             \\
    \end{tabular}

    \caption{Modelos pre-entrenados sobre distintos dominios. En familia nos referimos a qué tipo de modelo de lenguaje es usado (BERT, RoBERTa, etc)}
    \label{tab:bert_pretrained_models}
\end{table}


Luego del estallido de los modelos de lenguaje basados en transformers, algunos trabajos se han basado en directamente entrenar estos modelos ya no en textos formales como Wikipedia o noticias sino directamente en el dominio en cuestión. Por ejemplo, SciBERT \cite{beltagy-etal-2019-scibert} es un modelo BERT entrenado directamente en textos científicos, BERTweet \cite{bertweet} entrena un modelo RoBERTa\cite{liu2019roberta} sobre cerca de 850M tweets en inglés, una parte de ellos relacionados a la pandemia del COVID-19. La tabla \ref{tab:bert_pretrained_models} lista algunos de estos modelos.

En español tenemos el modelo TwilBERT\cite{gonzalez2021twilbert}; sin embargo, tiene algunas limitaciones: en primer lugar, no queda claro cuánto tiempo de entrenamiento recibió ni si los datos fueron suficientes; en segundo, usaron un modo de entrenamiento basado en una variante de la tarea NSP (ver subsección XXX) cuando numerosos trabajos muestran que el tipo de entrenamiento basado en RoBERTa (sólo tarea MLM) mejora el desempeño. Finalmente, su modelo no es accesible mediante el hub de huggingface\todo{agregar URL o cita}, que usamos para este trabajo.


Algunas oportunidades de mejora de lo estudiado en \citet{gururangan-etal-2020-dont} son, en primer lugar y siguiendo la regla de Bender\cite{bender2011achieving}, realizar el estudio en un idioma distinto del inglés. Por otro lado, un dominio que no está estudiado en dicho trabajo es el dominio de tareas en textos sociales. Finalmente, y dado el estallido dees de interés realizar una comparación de la performance de modelos adaptados al domini

\section{Tareas utilizadas en el benchmark}

Para analizar el impacto de la adaptación de dominio y realizar una comparación contra un modelo entrenado desde cero en dicho dominio usamos un conjunto de tareas como benchmark. Las tareas elegidas son todas las que analizamos en este trabajo hasta el momento:

\begin{enumerate}
    \item Análisis de sentimientos
    \item Análisis de emociones
    \item Detección de discurso de odio
    \item Detección contextualizada de discurso de odio
    \item Detección de Ironía
\end{enumerate}

La única tarea que no estudiamos hasta el momento es la de detección de ironía, que es básicamente una tarea de detección binaria sobre textos sociales para distintos. Todas las tareas (con la excepción de la detección contextualizada explicada en los capítulos anteriores) forman parte de IberLEF.

\section{Recolección de tweets}

A continuación describimos el proceso de recolección de tweets que utilizamos para entrenar \robertuito{}.

El stream de API de acceso gratuito (también conocida como \emph{Spritzer}) es una muestra de alrededor del 1\% de los tweets, supuestamente aleatoria, aunque algunos estudios han mostrado algunas preocupaciones acerca de la posible manipulación de esta muestra \cite{pfeffer2018tampering}. Si bien esto puede ser un problema para estudios de ciencias sociales computacionales, para nuestros fines descartamos estas preocupaciones.

En primer lugar, descargamos una colección de Spritzer subida a Archive.org que data de Mayo de 2019\footnote{\url{https://archive.org/details/archiveteam-twitter-stream-2019-05}}. Filtramos aquellos tweets cuya metadata indique que su idioma no sea español. Sobre los tweets en español, usamos la API de Twitter para descargar los tweets de los usuarios en cuestión. De este proceso recolectamos alrededor de 622 millones de tweets de cerca 432 mil usuarios.

Finalmente, considerando que no queremos entrenar , nos quedamos sólo con aquellos tweets que tengan 6 o más tokens, usando para esto el tokenizador entrenado en BETO \cite{canete2020spanish}, sin contar repeticiones de emojis y haciendo el preprocesado descripto en capítulos anteriores.

De este proceso quedan 500M tweets, los cuales ordenamos en 1000 archivos para facilitar la lectura en procesos posteriores. El repositorio de la recolección de tweets puede encontrarse en \url{https://github.com/finiteautomata/spritzer-tweets}.

\section{Modelo pre-entrenado sobre tweets}


Proponemos tres versiones de \robertuito{}: una versión \emph{cased} que conserva las mayúsculas, una versión \emph{uncased} que convierte todo a minúsculas y una versión \emph{deacc}, que usa minúsculas y elimina los acentos en los tweets. El español usa tildes para enfatizar los acentos en las palabras; por lo general, se pasan por alto en los textos sociales, por lo que queremos analizar si eliminarlos mejora el rendimiento de los modelos.

Entrenamos a los tokenizadores usando el algoritmo \emph{SentencePiece} en los tweets recopilados para cada una de las tres configuraciones. Guardamos 30K tokens para cada uno. Usamos \emph{tokenizers} library \footnote{\url{https://github.com/huggingface/tokenizers}} que proporciona implementaciones rápidas en Rust para muchos algoritmos de tokenización.

\subsection{Arquitectura y entrenamiento}

\begin{table}[t]
    \centering
    \begin{tabular}{l|l}
        \hline
        Hyperparameter  & Value \\
        \hline
        \#Heads           & 12             \\
        \#Layers          & 12             \\
        Hidden Size       & 768            \\
        Intermediate Size & 3072           \\
        Hidden activation & GeLU           \\
        Vocab. size       & 30,000         \\
        \hline
        MLM probability   & 0.15           \\
        Max Seq length    & 128            \\
        Batch Size        & 4,096          \\
        Learning Rate     & $3.5 * 10^{-4}$\\
        Decay             & $0.1$          \\
        $\beta_1$         & 0.9            \\
        $\beta_2$         & 0.98           \\
        $\epsilon$        & $10^{-6}$      \\
        Warmup steps      & 36,000 (6\%)   \\
        \hline
    \end{tabular}
    \caption{Model hyperparameters of \robertuito{} architecture}
    \label{tab:robertuito_architecture}
\end{table}

Se utilizó una arquitectura base RoBERTa en RoBERTuito, con 12 capas de auto atención, 12 cabezas de atención y tamaño oculto 768, de la misma manera que BERTweet \cite{bertweet}. Usamos un objetivo de lenguaje enmascarado, en la misma línea de RoBERTa y BERTweet, sin tener en cuenta la tarea de predicción de la siguiente oración usada en BERT u otras tareas de orden de tweets como la usada en \citet{gonzalez2021twilbert}.

Teniendo en cuenta los hiperparámetros de RoBERTa y BERTweet, decidimos utilizar un tamaño de lote grande para nuestro entrenamiento. Si bien se recomienda un tamaño de lote de 8k en RoBERTa, debido a las limitaciones de recursos, decidimos equilibrar el número de actualizaciones utilizando un tamaño de lote de 4k. Para comprobar la convergencia, primero entrenamos un modelo sin carcasa para 200.000 pasos. y luego procedió a ejecutarlo durante 600K pasos para los tres modelos.

Entrenamos nuestros modelos durante aproximadamente tres semanas en una TPU v3-8 y una máquina interrumpible \emph{e2-standard-16} en GCP. Estas nos fueron provistas por  Nuestro código base usa la biblioteca \emph{huggingface's transformers} y su implementación \emph{RoBERTa} \citet{wolf-etal-2020-transformers}. Cada tweet está tokenizado y enmascarado dinámicamente con una probabilidad igual a $ 0.15 $. La tabla \ref{tab:training_results} muestra los resultados del entrenamiento en términos de pérdida de entropía cruzada y perplejidad.

\begin{table}[t]
    \centering
    \begin{tabular}{l|l|l|l}
        Model   & Train loss & Eval loss   & Eval ppl \\
        \hline
        Cased   & 1.864      & 1.753       & 5.772    \\
        Uncased & 1.940      & 1.834       & 6.259    \\
        Deacc   & 1.951      & 1.826       & 6.209
    \end{tabular}
    \caption{Training results of \robertuito{} for each of its three configurations, expressed in cross-entropy loss for the Masked-Language-Modeling task and perplexity (ppl)}
    \label{tab:training_results}
\end{table}



\section{Adaptación de modelos pre-entrenados}
