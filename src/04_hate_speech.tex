
\label{chap:04_hate_speech}

El discurso de odio contra mujeres, inmigrantes y otros grupos protegidos es un fenómeno generalizado en la Internet y que resulta importante monitorear dada su potencial relación con actos violentos, como hemos comentado en la introducción de esta tesis. En los primeros días de la World Wide Web, algunos académicos se aventuraron a decir a que los prejuicios y el odio serían removidos en este espacio mediante la disolución de identidades en el ámbito virtual \cite{levy2001cyberculture, rheingold1993virtual}. Veinte años después de esta hipótesis, podemos decir que no ha sido el caso. La prevalencia del racismo en la ``World White Web''  y  en las redes sociales ha sido estudiada en numerosos trabajos \cite{adams2005white, kettrey2014staking}, como así también la misoginia en el mundo virtual \cite{filipovic2007blogging, mantilla2013gendertrolling}, entre otros ataques discriminatorios.

Si bien el discurso racista y sexista es una constante en las redes sociales, muchos picos se documentan luego de eventos detonantes, como pueden ser asesinatos con motivos religiosos o políticos \cite{burnap2015cyber}. Debido a esto, algunos estados y organizaciones supranacionales han tomado cartas en el asunto instando a las empresas de redes sociales a que tomen medidas para bajar la incidencia del discurso de odio. Debido a la enorme cantidad de contenido generado por usuarios en estos medios, es necesario desarrollar herramientas que faciliten la labor humana en la detección y prevención de este fenómeno, con particular foco de aquel que incita a la violencia física.


En este capítulo haremos una introducción a este problema desde varias ópticas. Analizaremos las diversas definiciones de discurso de odio y haremos una breve reseña desde un marco legal y de tratados internacionales para luego centrarnos en este problema desde una perspectiva del procesamiento de lenguaje natural. En base al dataset de la competencia \hateval{} \cite{hateval2019semeval}, analizaremos de técnicas de detección de discurso de odio, algunas de ellas presentadas en \citet{perez-2019-atalaya}. Finalmente, marcaremos algunos problemas en los enfoques actuales de la detección de discurso discriminatorio y algunas oportunidades de mejora que abordaremos en capítulos subsiguientes.


\section{¿Qué es el discurso de odio?}
\label{sec:hate_speech_definitions}

\input{src/04_hate_speech_definitions.tex}

\section{Trabajo previo}

Haremos una reseña de la literatura de la detección de discurso de odio y otros fenómenos similares. Un análisis exhaustivo de esta subdisciplina sería inviable debido a la enorme cantidad de trabajo del área, con un ritmo meteórico en los últimos años. Referimos para repasos más extensivos a \citet{schmidt2017survey} y \citet{fortuna2018survey}. Más recientemente, \citet{poletto2021resources} hacen un análisis pormenorizado y actualizado de los recursos existentes para esta tarea.

La detección del discurso del odio es una tarea de clasificación de textos relacionada con el análisis de sentimientos y ha sido estudiada para varias redes sociales \cite{thelwall2008social, pak2010twitter, saleem2017web}. Uno de los primeros trabajos al respecto es el de \citet{greevy2004classifying}, quienes utilizan bolsas de palabras y Support Vector Machines para detectar contenido racista en páginas web, utilizando un dataset construido de manera semi-supervisada buscando sitios mediante keywords y sus links en motores de búsqueda. Siguiendo un enfoque similar, \citet{warner2012detecting} usaron unigramas y Brown clusters \cite{brown1992class} con SVMs para detectar mensajes antisemitas en Twitter.

\citet{waseem2016hateful} anotaron un corpus y usaron técnicas basadas en n-gramas de caracteres para detectar discurso de odio en comentarios de Twitter. \citet{badjatiya2017deep} usaron el mismo conjunto de datos para entrenar modelos de aprendizaje profundo con embeddings ajustados a los datos, obteniendo mejoras sustanciales en el rendimiento para la tarea en cuestión aunque sujeto a algunos problemas de entrenamiento observado por otros trabajos \cite{arango2019hate}. \citet{zhang2018detecting} entrenaron una red neuronal profunda que combina CNNs con Gated Recurrent Units \cite{cho2014learning}, superando a los sistemas anteriores en varios conjuntos de datos de detección de discurso de odio. \citet{anzovino2018automatic} recopilaron un corpus de tweets misóginos y propusieron una taxonomía para distinguirlos en diferentes categorías. A su vez, los autores mostraron que enfoques simples (como el uso de modelos lineales junto con n-gramas) logran un rendimiento competitivo en conjuntos de datos de pequeño tamaño.

En cuanto a las tareas compartidas, \citet{fersini2018overview} presentaron un dataset para la detección de misoginia en Twitter, tanto en español como en inglés, mientras que \citet{fersini2018evalitaoverview} planteó un desafío similar pero en italiano e inglés. \citet{bosco2018overview} propuso un concurso de detección automática sobre publicaciones de Twitter y comentarios de Facebook, que incluía discursos de odio en general.

Una de las herramientas más utilizadas, no sólo para la detección de discurso de odio sino para la detección de contenido tóxico en general es Perspective API de Google, desarrollada originalmente por Jigsaw \footnote{\url{https://developers.perspectiveapi.com/s/}}. Esta API de acceso libre brinda un analizador muy potente para la detección de lenguaje tóxico, con información granular sobre los tipos de ataques. Algunos trabajos lo utilizan como algoritmo de detección en modalidad zero-shot, obteniendo mejores resultados que modelos entrenados sobre los propios datos \cite{pavlopoulos2020toxicity}. Sin embargo, algunas de sus debilidades han sido marcadas mediante ejemplos adversariales, algo que obviamente es propio de las actuales limitaciones de las técnicas de NLP \cite{hosseini2017deceiving,jain2018adversarial}. Más aún, la información de grano fino --e.g. si es un ataque a un grupo protegido y a cuál se ataca-- sólo está disponible para el inglés.

Dentro de los trabajos en español, \citet{plaza2021pretrained} evalúan distintos modelos pre-entrenados de lenguaje sobre la tarea de detección de discriminación usando dos datasets: el primero, \citet{pereira2019detecting} que consta de 6000 tweets, recolectado por el Estado Español para monitorear el discurso de odio en redes sociales; y el segundo, el dataset de SemEval 2019 Task 5 (\hateval{}) \cite{hateval2019semeval}, presentado en contexto de una shared-task y que comprende ataques contra inmigrantes y mujeres.


\section{Descripción del dataset utilizado}
\label{sec:hateval_dataset}

\begin{table}[t]
    \centering
    \begin{tabular}{l c c c  c c c}
        Categoría  &    \mc{3}{Español}                          & \mc{3}{Inglés}                                \\
                   &Train   & Dev    & Test   &Train   & Dev   & Test           \\
        \thline{2.5}
        No HS      &$2643$  & $278$  & $940$  &$5217$  & $573$ & $1740$  \\
        HS         &$1857$  & $222$  & $660$  &$3783$  & $427$ & $1260$  \\
        TR         &$1129$  & $137$  & $423$  &$1341$  & $219$ & $529$    \\
        AG         &$1502$  & $176$  & $474$  &$1559$  & $204$ & $594$    \\
        \hline
        Total      &$4500$  & $500$  & $1600$ &$9000$  & $1000$& $3000$  \\
        \thline{2.5}
    \end{tabular}
    \caption{Números del dataset de \citet{hateval2019semeval}, por idioma y por partición. No HS representa los tweets que no tienen contenido odioso, HS aquellos que sí, TR aquellos que son individualizados, y AG aquellos que son agresivos. Entre paréntesis encontramos los porcentajes de incidencia, considerando TR y AG dentro de aquellos que son discriminatorios}
    \label{tab:hateval_dataset}
\end{table}

Utilizamos en este capítulo el dataset provisto por \citet{hateval2019semeval}, presentado en SemEval 2019 y orientado a la detección de discurso de odio contra mujeres e inmigrantes en Twitter. Los autores recopilaron comentarios en inglés y en español de dicha red social mediante tres estrategias combinadas: monitoreando a las posibles víctimas de cuentas de odio; chequeando el historial de usuarios creadores de contenido discriminatorio; y filtrando contenido mediante palabras clave. A su vez, este trabajo distingue entre el discurso de odio dirigido a individuos y el discurso de odio genérico, y entre mensajes agresivos y no agresivos. En el Capítulo \ref{chap:05_dataset_creation} construiremos un conjunto de datos contextualizado de discurso de odio en base a algunas de las limitaciones observadas sobre en este capítulo.

Las instancias del dataset poseen las siguientes etiquetas:

\begin{itemize}
    \item \textbf{HS}: una etiqueta binaria que marca si el tweet tiene contenido discriminatorio contra mujeres o inmigrantes (0 si no lo tiene, 1 si hay discurso de odio)
    \item \textbf{TR}: Si hay HS, una etiqueta binaria que marca si el objetivo del discurso de odio es un objetivo genérico (0) o si se refiere a un individuo específico (1)
    \item \textbf{AG}: Si hay HS, una etiqueta binaria que marca si el tweet es agresivo
\end{itemize}

\begin{table}
    \centering
    \small
    \begin{tabularx}{\textwidth}{X c c c}
        Texto & HS & TR & AG \\
        \hline
        Los tomas asi puro como si fuera jugo y cuando te querés rescatar estas hablando en árabe URL & 0 & 0 & 0 \\
        \rule{0pt}{4ex}Como son españoles nada... sin fueran refugiados...GLORIA \#migrates \#refugiados \#EspañaLoPrimero URL & 1 & 0 & 0 \\
        \rule{0pt}{4ex}@OmarPrietoGob ``Extranjero sin identificación será puesto en la frontera'' ENVÍA AL EJERCITO A TOMAR CONTROL DE LAS PULGAS PLATANEROS Y CURVA DE AHÍ PARA QUE VEAS COMO HAY COLOMBIANOS INDOCUMENTADOS COMO MONTE AHÍ DE BUHONERS PORQUE LA POLICÍA & 1 & 0 & 1 \\
        \rule{0pt}{4ex}Inmigrante da una brutal paliza a una joven por no dejarse besar en Ciudad Real.\#stopinvasion \#YoSiTeCreo \#NoesNo lo peor que no han salido a la calle las feminas del Twitter que tanto se indignaron con la salida de La Manda a la calle. & 1 & 1 & 0 \\
        \rule{0pt}{4ex}@elisacarrio Callate hija de puta gorda falopera & 1 & 1 & 1 \\
        \hline
    \end{tabularx}
    \caption{Ejemplos del dataset de SemEval 2019 Task 5: \hateval{}. HS indice la presencia de discurso de odio, TR la presencia de discriminación individualizada, y AG la presencia de discriminación agresiva}
    \label{tab:hateval_dataset_examples}
\end{table}



La Tabla \ref{tab:hateval_dataset} muestra los números para cada partición, cada idioma, y cada una de las etiquetas. Podemos observar entre los dos idiomas que, si bien la proporción de discurso de odio se mantiene muy similar (58\% vs 42\% aproximadamente), la proporción de discurso de odio individualizado (TR) y agresivo (AG) es notoriamente más alto para el español que para el inglés. Esto puede deberse, entre otras cosas, a distintas estrategias de recolección de los tweets. La tabla \ref{tab:hateval_dataset_examples} posee algunos ejemplos para cada una de las características en cuestión para la porción en español, que es la de nuestro interés.

\section{Tareas de clasificación}

Sobre los datos mencionados en la anterior sección, los autores propusieron dos tareas de clasificación:

\newcommand{\subtaska}[0]{\textbf{Tarea A}}
\newcommand{\subtaskb}[0]{\textbf{Tarea B}}

\begin{itemize}
    \item \subtaska{}: Dado un tweet predecir si contiene discurso de odio contra mujeres o inmigrantes (HS)
    \item \subtaskb{}: Dado un tweet, predecir si contiene discurso de odio (HS), si está dirigido contra un individuo o un grupo (TR), y si es agresivo o no (AG)
\end{itemize}


La primer tarea es la versión más básica de la detección de discurso de odio, donde predecimos una etiqueta binaria que marca la presencia de contenido de esta índole. La segunda es una versión más rica, de grano fino, donde predecimos varias características de particular interés para distinguir algunas formas potencialmente más peligrosas de este fenómeno: por ejemplo, si es agresivo y si es individualizado, lo que puede indicar alguna incitación a un ataque de un individuo o miembros de algún grupo protegido.

\citet{hateval2019semeval} propusieron para medir el desempeño en la \subtaska{} la Macro F1 de las clases positiva y negativa. Para el caso de la \subtaskb{}, utilizaron dos métricas: Macro F1 de las 3 clases (HS, TR, AG) y también la medida Exact Match Ratio:

\begin{equation*}
    EMR = \frac{1}{n} \sum\limits_{i=1}^{n} I(Y_i, Y_i^*)
\end{equation*}

\noindent siendo $Y_i$ las etiquetas respectivas $(HS, TR, AG)$, $Y_i^*$ las etiquetas que predice nuestro sistema, e  $I$ la función indicadora ($I(x, x) = 1$; $0$ en cualquier otro caso). Observado más de cerca, esto puede entenderse la accuracy sobre la 3-upla de la salida de los clasificadores, pero para evitar confusiones usamos el nombre de Exact Match Ratio (EMR). \cite{zhang-2014-multilabel}

\section{Método}

En esta sección describimos los distintos modelos planteados para abordar las dos tareas de clasificación de discurso de odio, así como detalles de preprocesamiento introducidos en \citet{perez-2019-atalaya} para optimizar las representaciones de modelos lineales de clasificación.

\subsection{Preprocesamiento}
\label{sec:04_preprocessing}

Definimos dos niveles de preprocesamiento: básico y orientado a sentimientos, dependiendo del modelo a utilizar. El preprocesamiento básico de tweets es el mismo que describimos en la Sección \ref{sec:03_preprocessing}, y es el usado con los modelos pre-entrenados o modelos neuronales.

El preprocesamiento orientado a sentimientos incluye además lematización (usando TreeTagger \cite{schmid95}) y manejo de negación. Para el manejo de la negación, seguimos un enfoque simple:
% \cite {das01, pang02}:
Buscamos palabras de negación y agregamos el prefijo 'NOT \_' a los siguientes tokens. Se niegan hasta tres tokens, o menos si se encuentra un token que no sea una palabra.

\subsection{Modelos de clasificación}
\label{sec:04_classifiers}

Para las tareas propuestas, analizamos el desempeño de diversos modelos de clasificación. Algunos de ellos son los presentados para la shared-task \hateval{} en \citet{perez-2019-atalaya}, a las cuales agregamos modelos basados en transformers. \footnote{Estos modelos no estaban disponibles al momento de presentar dicho trabajo. El trabajo de \bert{} \cite{devlin2018bert} es de finales de 2018, y hasta finales de 2019 no fue publicada una versión entrenada en español, \beto{}} Para la tarea de detección binaria (\subtaska{}) planteamos 3 tipos de clasificadores:

\begin{enumerate}
    \item Modelos lineales: regresiones logísticas y SVM con kernel lineales, consumiendo como entrada bolsas de palabras, bolsas de caracteres, y tweet embeddings
    \item Redes neuronales recurrentes: usando como entrada representaciones no contextualizadas (\fasttext{}) y  contextualizadas (\elmo{})
    \item Modelos pre-entrenados de lenguaje.
\end{enumerate}

Para los modelos lineales, utilizamos representaciones de cada tweet calculadas con Smooth Inverse Frequency (ver Sección \ref{sec:02_tweet_embeddings} para más detalles), usando como base los vectores de \fasttext{} entrenados sobre tweets con preprocesamiento orientado a sentimientos. Los modelos recurrentes consumieron como entrada la concatenación de representaciones de \fasttext{} que describimos en la Sección \ref{sec:03_classification} a las cuales adosamos vectores contextualizados basados en \elmo{}. \cite{peters2018} Para esta última técnica, usamos la versión en español entrenada por \citet{che-EtAl:2018:K18-2}. Finalmente, consideramos los siguientes modelos pre-entrenados: para el español \beto{} \cite{canete2020spanish}, y para el inglés \bert{} \cite{devlin2018bert}, \roberta{} \cite{liu2019roberta} y \bertweet{}. \cite{dat2020bertweet}

La tarea de multidetección de discurso de odio (\subtaskb{}) podemos pensarla de dos maneras:

\begin{enumerate}
    \item Un problema de clasificación múltiple
    \item Un problema de clasificación de 5 clases
\end{enumerate}

En el primer caso, el enfoque es el de predecir por separado cada una de las variables HS, AG, y TR. La segunda formulación se basa en observar que no tenemos 8 combinaciones permitidas sino sólo cinco: si no hay HS no nos interesa observar las otras dos variables. Con esta observación, convertimos cada combinación en una clase de un problema de clasificación estándar. En \citet{perez-2019-atalaya} propusimos un modelo basado en Support Vector Machines que consume la misma entrada que detallamos anteriormente y con una salida de cinco clases. No evaluamos en dicho trabajo un modelo recurrente con este esquema de clasificación ni tampoco lo haremos aquí, considerando que evaluamos opciones que han demostrado tener mejor desempeño para numerosas tareas de clasificación de texto.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{img/05/bert_model_hateval.pdf}
    \caption{Modelo basado en BERT para la tarea de clasificación múltiple. Cada variable (HS, TR, AG) representa un problema de clasificación en sí mismo}
    \label{fig:bert_hateval_classifier}
\end{figure}


Asimismo, considerando la opción de multiclasificación, proponemos para la \subtaskb{} modelos de lenguaje pre-entrenados con tres salidas. Recordemos que la arquitectura usual de clasificación basada en \bert{} consta de poner como última capa una softmax que consume como entrada la salida del token \verb|[CLS]|. Esto, además, agregando como parámetro una matriz de proyección $W \in \mathbb{R}^{m \times 768}$ donde $m$ es la cantidad de clases de nuestro problema y 768 corresponde a la dimensión de cada vector del modelo de transformers.

Para construir un modelo de multiclasificación, mantenemos la misma arquitectura pero, en lugar de usar como activación la función softmax, utilizamos la función sigmoidea elemento a elemento. En el caso de clasificación de $n$ clases, interpretamos a $\text{softmax}(W x + b)_i$ como la probabilidad\footnote{Estrictamente hablando, más bien sería un puntaje entre 0 y 1} instancia pertenezca a la clase $i$. Por otro lado, en el caso de multiclasificación de $n$ variables, $\sigma(W x + b)_i$ \footnote{Esta expresión es elemento a elemento} nos da la probabilidad de predecir la etiqueta positiva para la variable i-ésima: en nuestro caso, $\sigma(W x + b)_1$ nos da $P(HS = 1\mid x)$,  $\sigma(W x + b)_2$ nos da  $P(TR = 1 \mid x)$ y finalmente el tercer subíndice nos da  $P(AG = 1\mid x)$. La Figura \ref{fig:bert_hateval_classifier} ilustra el modelo utilizado.

Para entrenar el modelo de clasificación, evaluamos dos tipos de funciones de costo. En primer lugar, utilizamos la suma de las entropías cruzadas binarias. Concretamente, si $y = (y_{HS}, y_{TR}, y_{AG})$ son las etiquetas de una instancia e $\widehat{y}$ la predicción del modelo, la función de costo es:

\begin{equation}
\label{eq:multi_loss}
L(y, \widehat{y}) = \sum\limits_{\mathclap{k \in \{HS, TR, AG\}}} J(y_k, \widehat{y_k})
\end{equation}

\noindent donde $J$ es la entropía cruzada binaria. Esta función de costo, sin embargo, ignora cualquier tipo de jerarquía entre las variables; por ejemplo, si para una instancia tenemos $HS = 0$, calcula el costo también de las variables $TR$ y $AG$. Contemplamos entonces una variante de la función descripta en la ecuación \ref{eq:multi_loss} para tener en cuenta esto:

\begin{equation}
    \label{eq:hierarchical_loss}
    L(y, \widehat{y}) =  J(y_{HS}, \widehat{y_{HS}}) + \beta(y_{HS})\sum\limits_{\mathclap{k \in \{TR, AG\}}} J(y_k, \widehat{y_k})
\end{equation}

\noindent donde $\beta(y_{HS})$ pondera la pérdida de las variables del segundo nivel de nuestra jerarquía. Una opción puede ser considerar $\beta(1) = 1, \beta(0) = 0$, donde ignoramos las pérdidas de las variables $TR$ y $AG$ cuando no hay discurso discriminatorio. Análogamente, $\beta(y) = 1$ sería el caso descripto en la ecuación \ref{eq:multi_loss}. Una forma de generalizar esto es agregando un hiperparámetro $\gamma \in [0, 1]$ para escribir $\beta(y) = (1-y) \gamma + y$. Optimizamos este hiperparámetro realizando una búsqueda lineal entre $0$ y $1$ utilizando pasos de $0.1$.

Al momento de realizar inferencia, realizamos las evaluaciones de los modelos realizamos poniendo una máscara por encima de estos modelos de clasificación múltiple de manera de evitar salidas incoherentes (por ejemplo, $HS = 0, TR = 1, AG= 0$).


\section{Resultados}

\newcommand{\esrow}[1]{\multirow{#1}{*}{es}}
\newcommand{\enrow}[1]{\multirow{#1}{*}{en}}

\begin{table}[ht!]
    \centering
    \begin{tabular}{l c c c c c}
        Modelo       & Idioma      & Precision    & Recall       & F1            & Macro F1 \\
        \hline
        SVM$^*$      & \mr{3}{es}  & $63.9$       & $80.0$       & $71.1$       & $73.0$  \\
        ELMO-RNN     &             & $66.1$       & $75.3$       & $70.4$       & $73.5$  \\
        BETO         &             & $\mbf{67.4}$ & $\mbf{83.9}$ & $\mbf{74.7}$ & $\mbf{76.4}$ \\
\rule{0pt}{4ex}BERT  & \mr{3}{en}  & $47.4$       & $\mbf{96.8}$ & $63.7$       & $49.6$ \\
        RoBERTa      &             & $47.0$       & $96.7$       & $63.2$       & $48.6$ \\
        BERTweet     &             & $\mbf{49.5}$ & $95.9$       & $\mbf{65.3}$& $\mbf{54.6}$ \\
        \hline
    \end{tabular}
    \caption{Resultados de la evaluación para la detección de discurso de odio en el dataset de test, medidas por \% de precisión, sensitividad y F1 sobre la clase positiva (discurso de odio) y por la métrica Macro F1. Con $^*$ están marcados los resultados presentados en \citet{perez-2019-atalaya}. En negrita, el mejor resultado.}
    \label{tab:hateval_task_a}
\end{table}



La Tabla \ref{tab:hateval_task_a} muestra los resultados de la evaluación para la detección de discurso de odio binaria (\subtaska{}), marcando con un asterisco aquellos modelos presentados en \citet{perez-2019-atalaya}. Respecto a los resultados en español, el clasificador basado en SVMs obtiene una buena performance, aún comparado con aquel basado en embeddings contextualizados. Este algoritmo basado en SVMs obtuvo el mejor desempeño en la competencia con $0.730$ de Macro F1 \cite{hateval2019semeval}. El pobre desempeño de \elmo{} contra un algoritmo mucho más simple puede deberse a un mal pre-entrenamiento del modelo base en español\footnote{No queda claro que en entrenar este modelo sobre 20M palabras sea suficiente, ni que sea un dataset suficientemente general} y también debido al cambio de dominio, a los cuales los modelos pre-entrenados previos a BERT son sumamente sensibles \cite{hendrycks-etal-2020-pretrained}.

Para ambos idiomas, los modelos basados en Transformers \cite{vaswani2017attention} obtienen la mejor performance, con considerables mejoras respecto a los modelos basados en ELMo y a los SVMs \footnote{Un modelo que no evaluamos en el presente trabajo es la versión en español de RoBERTa, recientemente entrenada. En el capítulo 7 evaluaremos su rendimiento en esta tarea}. Particularmente, en el caso del inglés, \bertweet{} \cite{dat2020bertweet} obtiene la mejor Macro F1, algo esperable considerando que está particularmente diseñado para Twitter.


\begin{table}[ht!]
    \centering
    \begin{tabular}{lll ccc cc}
        Modelo            &        & Idioma      &  HS F1       & TR F1        &  AG F1      &  Macro F1    &   EMR           \\
        \thline{2.5}
        \mr{3}{BETO}      & multi  & \mr{3}{es}  & $74.1$       & $\mbf{76.5}$&$\mbf{68.8}$  & $\mbf{73.1}$ & $68.5$          \\
                          & hier   &             & $73.5$       & $75.8$       & $67.4$      & $72.2$       & $\mbf{70.3}$    \\
                          & combi  &             & $\mbf{74.2}$ & $76.3$       & $66.8$      & $72.4$       & $69.8$          \\

        \hline
              \mr{3}{BERT}& multi & \mr{3}{en}  & $63.8$       & $60.0$       & $44.3$      & $56.0$       & $38.0$          \\
                          & hier   &             & $64.2$       & $59.2$       & $45.1$      & $56.2$       & $38.8$          \\
                          & combi  &             & $64.4$       & $59.3$       & $44.2$      & $56.0$       & $39.8$          \\
        \hline
        \mr{3}{RoBERTa}   & multi  & \mr{3}{en}  & $63.4$       & $57.8$       & $45.4$      & $55.5$       & $36.5$          \\
                          & hier   &             & $63.7$       & $57.2$       & $45.6$      & $55.5$       & $37.0$          \\
                          & combi  &             & $63.6$       & $57.6$       & $44.2$      & $55.1$       & $37.7$          \\
        \hline
        \mr{3}{BERTweet}  & multi  & \mr{3}{en}  & $65.8$       & $62.9$       &$\mbf{46.2}$ &$\mbf{58.3}$  & $42.6$          \\
                          & hier   &             & $65.6$       & $61.7$       & $45.0$      & $57.4$       & $42.3$          \\
                          & combi  &             & $\mbf{66.6}$ &$\mbf{63.7}$   & $44.4$     & $58.2$       &$\mbf{44.9}$     \\
        \thline{2.5}
    \end{tabular}

    \caption{Resultados de la evaluación para para \subtaskb{} en términos de las F1 de las clases HS (Hate Speech), TR (Targeted), AG (Aggressive), el Exact Match Ratio (EMR), las Macro F1 de las clases en cuestión, y la Macro F1 de la clase HS. Las 3 variaciones de los modelos son: \emph{multi} es la salida de multiclasificación estándar, \emph{hier} es la salida de multiclasificación con una jerarquía de clasificación, y \emph{combi} es la salida de multiclasificación con una combinación de clasificaciones. Los resultados están expresados como las medias de 10 corridas independientes.}
    \label{tab:hateval_task_b}
\end{table}


La tabla \ref{tab:hateval_task_b} muestra los resultados de la \subtaskb{}, reportado por las F1 de cada variable predicha (HS, TR, AG), así como por la Macro F1 de las 3 variables mencionadas y el Exact Match Ratio. Los resultados están expresados como la media de 10 corridas independientes del experimento para cada configuración distinta. Consideramos las 3 versiones: \emph{multi} refiere a clasificación múltiple, \emph{hier} a clasificación múltiple con la función de costo jerárquica, y \emph{combi} a la conversión del problema en una clasificación de cinco clases.

Podemos observar que para español, la mejor performance en términos de EMR (la métrica más estricta) es el clasificador entrenado con la función de costo definida en \ref{eq:hierarchical_loss} (con el hiperparámetro $\gamma = 0.1$); sin embargo, la diferencia entre las performances no es significativa al correr un test de Kruskal-Wallis ($H(9) = 3.492, p = 0.174$). En términos de Macro F1, la mejor performance es de \beto{} con la salida múltiple y sin la función de costo jerárquica (\emph{multi}) pero de nuevo esta diferencia no es significativa ($H(9) = 3.656, p=0.16$).

Respecto al inglés, los mejores resultados pueden observarse en el modelo entrenado con \bertweet{} con la salida de cinco clases en el caso del EMR, y con la salida múltiple (sin pérdida jerárquica) para la Macro-F1. Este resultado, sin embargo, queda en términos de EMR por debajo del baseline propuesto por los autores del dataset \cite{hateval2019semeval}, aunque cercano en términos de Macro F1 a los mejores resultados de la competencia. En \citet{gertner-etal-2019-mitre}, se basaron en un ensemble de modelos entrenados con BERT y usando también un ajuste de dominio sobre tweets. Esta baja performance de nuestros modelos (y de los modelos en general sobre ese dataset) puede deberse a problemas de anotación y a que las particiones de train y test no son idénticamente distribuidas. \footnote{En \citet{gertner-etal-2019-mitre} dan evidencia de esto, algo que perjudica el desempeño de estos modelos}



\begin{table}[t]
    \centering
    \begin{tabular}{l  l l  c c c c}
        Modelo              & Idioma        &  Tarea  &     Precision  &   Recall        &          F1    &  Macro F1       \\
        \thline{2}
        \mr{2}{BERTweet}    & \mr{2}{en}    &  A      & $49.5 \pm 1.2$ &  $95.9 \pm 1.2$ & $65.3 \pm 0.9$ &  $54.6 \pm 2.7$ \\
                            &               &  B      & $50.5 \pm 1.1$ &  $94.8 \pm 1.8$ & $65.8 \pm 0.5$ &  $56.7 \pm 2.2$ \\
        \hline
        \mr{2}{BETO}        & \mr{2}{es}    &  A      & $67.4 \pm 2.1$ &  $83.9 \pm 2.6$ & $74.7 \pm 0.7$ &  $76.4 \pm 1.1$ \\
                            &               &  B      & $71.3 \pm 4.2$ &  $77.8 \pm 5.4$ & $74.1 \pm 1.3$ &  $77.1 \pm 1.5$ \\
        \thline{2}
    \end{tabular}
    \caption{Comparación de la performance sobre la detección de discurso de odio para los clasificadores entrenados sobre \subtaska{} y \subtaskb{}. Resultados expresados como la media de 10 corridas independientes del experimento junto a sus desviaciones estándar. Ambos clasificadores de la \subtaskb{} están entrenados sobre el problema de multi-clasificación}
    \label{tab:hateval_task_a_vs_b}
\end{table}

La Tabla \ref{tab:hateval_task_a_vs_b} muestra la comparativa para la detección de discurso de odio (HS) para aquellos clasificadores que obtuvieron mejores resultados para \subtaska{} y \subtaskb{} (\beto{} y \bertweet{}). Consideramos para la \subtaskb{} al clasificador \emph{multi} de cada modelo de lenguaje. Lejos de dañarse la performance de la detección de lenguaje discriminatorio (lo que analizamos en la \subtaska{}), predecir más de una variable pareciera mantener el desempeño general; más aún, podemos observar que en términos de Macro F1, incluso parecieran tener una ligera mejora al ser entrenados sobre una tarea más compleja.






\subsection{Análisis de Error}
\label{sec:hateval_error_analysis}
\input{src/04_error_analysis.tex}

\section{Discusión}
\label{sec:04_discussion}

Respecto a la performance de los modelos presentados, los modelos basados en Transformers son notoriamente superiores a los demás modelos, en ambas tareas e idiomas. Particularmente, en inglés podemos observar que aquellos pre-entrenados sobre tweets como \bertweet{} tienen mejor performance que aquellos que son entrenados sobre Wikipedia como \bert{} o \roberta{}. El discurso de odio está muchas veces basado en la utilización de jergas e insultos raciales o misóginos, con lo cual es esperable el mejor desempeño de un modelo que tiene en sus datos de entrenamiento este tipo de expresiones.

Sobre la tarea más difícil de detección múltiple de discurso de odio (\subtaskb{}), propusimos varios enfoques: uno basado en predecir cada variable por separado y otro en predecir una variable que indique la combinación en cuestión. El modelo de predicción múltiple entrenado con la función de costo jerárquica obtuvo la mejor performance en términos de EMR, y la de multi-clasificación obtuvo la mejor en términos de Macro F1. En el caso de inglés, el modelo entrenado sobre cinco clases obtuvo la mejor performance en EMR y de nuevo el de multi-clasificación sobre Macro F1; sin embargo, esta queda por debajo de la mejor performance de la competencia (obtenida por el equipo MITRE \cite{gertner-etal-2019-mitre}) que usa una compleja combinación de técnicas, algunas de las cuales veremos en el Capítulo \ref{chap:07_domain_adaptation}. De estos dos casos, el modelo de multi-clasificación corre con la ventaja de calcular cada variable de manera independiente y tener un hiperparámetro menos.

Algo que merece cierta atención es que, lejos de empeorar el desempeño de nuestros modelos, agregar nuevas variables a predecir (además de la existencia de discurso de odio) pareciera mejorar levemente la performance de la detección de este fenómeno, a la vez que obteniendo salidas más ricas e interpretables. Más aún, observamos que otros trabajos \cite{gertner-etal-2019-mitre} utilizando el mismo conjunto de datos mejoraron la performance con una capa adaptadora que modela las dos variables latentes codificadas conjuntamente en la salida binaria: la misoginia y el racismo. Teniendo esto en cuenta, una pregunta a explorar es si contar con esta información (las características agredidas) puede mejorar la performance de los clasificadores o tener salidas más interpretables que sólo una etiqueta binaria.

Hacemos a continuación una disquisición no sólo sobre este trabajo y el dataset en el que se basa sino en líneas generales sobre los recursos y enfoques actuales en el área de detección de discurso de odio. Continuando con la idea del párrafo anterior, una limitación que puede verse es que la mayoría de los trabajos atacan una, dos, o a como mucho tres características protegidas. Por ejemplo, los trabajos de \citet{waseem2016hateful} y \citet{hateval2019semeval} sólo consideran racismo y sexismo, mientras que el de \citet{Davidson2017AutomatedHS} agrega homofobia a esta consideración. Sería deseable poder contar con un dataset que como mínimo cuente con estas tres características en conjunto a otras quizás menos utilizadas: odio de clase (a veces conocida como \emph{aporofobia}), discriminación por aspecto físico, por discapacidad, entre otras. A su vez, contar con la información de la característica atacada (algo que no ocurre en los datos utilizados en este capítulo) puede ser interesante para tener una mejor interpretabilidad de las salidas de nuestros algoritmos, y posiblemente para mejorar su rendimiento.

Un problema particular que se puede observar en los datos de \citet{hateval2019semeval} (pero que atraviesa a muchos otros) es el proceso de recolección de los datos: los tweets son recolectados mayormente a través de keywords. Como mencionamos en la Sección \ref{sec:hateval_dataset}, el proceso de recolección consta de varias estrategias combinadas; sin embargo (ver Apéndice \ref{app:04}), hay una altísima incidencia de algunas palabras (como \emph{sudaca} o \emph{inmigrante}) que sesgan fuertemente el dataset. Esto (entre otras cuestiones) puede ser un problema para los modelos que se entrenan sobre estos datos, haciendo que aprendan correlaciones espurias generadas por estas distribuciones. De todas formas, esto es una limitación general para estos tipos de aprendizaje sobre datos crudos y etiquetas, donde es difícil establecer e interpretar cómo un clasificador termina encontrando patrones para detectar el fenómeno medido.

La \textbf{anotación}, la etapa subsiguiente a la recolección de datos, pareciera presentar en este dataset algunos problemas. Hemos visto en la anterior sección una lista no extensiva de varios errores de etiquetado, aún cuando este dataset fue realizado con un complejo sistema combinando crowdsourcing,  etiquetado con expertos y desempate. Si bien es difícil trazar las razones detrás de estos problemas, observando las instancias incorrectamente etiquetadas puede hipotetizarse que esto es producto de un no entendimiento de las expresiones en los distintos dialectos del español y diferentes realidades socioculturales. \citet{waseem-2016-racist} mostró que las anotaciones ``amateurs'' (producto del uso de crowdsourcing) tienden a tener mayores instancias de Hate Speech (algo que daría la impresión de ocurrir aquí) y que datasets anotados por expertos mejoran la performance de los modelos. Este problema podría profundizarse dado que no queda claro si los anotadores son hablantes nativos de español, al no tener información detallada de quienes realizaron la tarea de etiquetado.

%%
%% Falta de contexto
%%

Un problema del dataset estudiado en este capítulo (pero que aplica a muchos otros también) es la \textbf{falta de contexto}: los mensajes carecen de información adicional sobre la noticia o el tema del que se está hablando. Cuando leemos un mensaje de un tweet, casi siempre lo leemos en el contexto de una noticia, o un trending topic. Muy rara vez leemos un mensaje en total aislamiento. De hecho, gran parte de los comentarios de este dataset tiene un contexto implícito: la noticia de conflicto migratorio en Ceuta. Otros comentarios, por otro lado, no se entienden bien ya que son respuestas a un tweet y que según el hilo de conversación pueden entenderse o no como discriminatorios.

Sobre esta falta de contexto, hay muchos mensajes aislados que pueden requerir información adicional para entender su significado. Por ejemplo, un comentario que dice ``hay que matarlos'' puede o no entenderse como discurso de odio. Si el objeto del mensaje se refiere a mosquitos, ese mensaje no es odioso; si, por otro lado, está hablando sobre migrantes chinos en el contexto del COVID-19, entonces ese mensaje es discriminatorio (y además llama a tomar una medida violenta). Podemos preguntarnos sobre este punto si el acceso a información contextual nos puede auxiliar en la detección de discurso de odio, siendo este contexto un hilo de conversación, una noticia a la que se refiere, o alguna otra forma de de información adicional.

Finalmente, otro problema que suele ocurrir relacionado al anterior es que no tenemos \textbf{información granular} de los datos anotados. Si bien algunos trabajos agregan información de la característica vulnerada, la mayoría simplemente agrega una etiqueta binaria sobre la existencia o no de discurso de odio (o bien algún nivel intermedio como si hay o no discurso ofensivo, como el caso de \citet{Davidson2017AutomatedHS}). Teniendo en cuenta lo observado en este capítulo, agregar información más detallada sobre cada caso puede ayudar a mejorar la detección del discurso de odio mediante una señal más rica a nuestros clasificadores sobre las diferentes fronteras de cada característica ofendida.


\section{Conclusiones}

En este capítulo hemos hecho un primer acercamiento a la tarea de detección de lenguaje discriminatorio, repasando de su definición desde un marco legal y desde el usado en la literatura de Procesamiento de Lenguaje Natural. Analizamos técnicas de clasificación del estado del arte sobre el dataset presentado en la shared task multilingual \cite{hateval2019semeval}. En base a este dataset, analizamos dos tareas: detección binaria de discurso de odio, y detección de múltiples variables (si es discurso de odio, si es dirigido, si es agresivo).

Para estas tareas, presentamos técnicas de clasificación basadas en modelos lineales que consumen distintos tipos de entrada como ser tweet embeddings y bolsas de caracteres; modelos basados en redes recurrentes que consumen embeddings contextualizados; y finalmente, utilizamos modelos de lenguaje pre-entrenados usando la arquitectura de Transformers. Para ambas, los modelos de Transformers obtuvieron el mejor desempeño, superando ampliamente a las demás técnicas.

En el caso de la tarea de detección múltiple, propusimos dos formas de atacar el problema: como clasificación múltiple (prediciendo simultáneamente las tres variables), y convirtiendo a un problema de clasificación simple sobre cinco clases posibles. Observamos, a su vez, que lejos de dañar la performance de la detección de discurso de odio, predecir más de una variable mejora la performance de nuestros clasificadores.

Analizando este dataset y algunos otros de la bibliografía, marcamos algunas oportunidades de mejora y observaciones en la detección de discurso de odio. En primer lugar, la posibilidad de agregar información contextual a los mensajes a analizar, sea sobre el tópico del que se está hablando o contexto conversacional previo. En segundo lugar, agregar \tbf{información granular} sobre las características ofendidas. Y finalmente, un punto no menor a la hora de la creación de recursos para un fenómeno tan complejo y social es indispensable tener muchos recaudos a la hora de la anotación --algo que ya ha sido observado en otros trabajos-- teniendo particular cuidado sobre el trasfondo sociocultural de quienes tomen esa tarea.

En los siguientes capítulos, exploraremos algunas de estas oportunidades de mejora. Particularmente, nos centraremos en la incorporación de contexto en la detección de discurso discriminatorio, construyendo un conjunto de datos que incorpore esta información a los mensajes anotados, y explorando cómo mejorar los algoritmos del estado del arte que aprovechen esa información.
