\label{chap:04_hate_speech}

El discurso de odio contra mujeres, inmigrantes y otros grupos protegidos es un fenómeno generalizado en la Internet, y que resulta importante monitorear dada su potencial relación con actos violentos como hemos comentado en la introducción de esta tesis. En los primeros días de la World Wide Web, algunos académicos se aventuraron a decir a que los prejuicios y el odio serían removidos en este espacio mediante la disolución de identidades en el ámbito virtual \cite{levy2001cyberculture, rheingold1993virtual,calderon2020topic}. Veinte años después de esta hipótesis, podemos decir que no ha sido el caso. La prevalencia del racismo en la ``World White Web''  y la explosión de esta en las redes sociales ha sido estudiada en numerosos trabajos \cite{adams2005white, kettrey2014staking}, como así también la misoginia en el mundo virtual \cite{filipovic2007blogging, mantilla2013gendertrolling}, entre otros ataques discriminatorios.

El discurso racista y sexista es una constante en las redes sociales, pero muchos picos se documentan después de eventos detonantes como asesinatos con motivos religiosos o políticos \cite{burnap2015cyber}. Debido a esto, algunos estados y organizaciones supranacionales han tomado cartas en el asunto instando a las empresas de redes sociales a que tomen medidas para bajar la incidencia de este fenómeno. Debido a la enorme cantidad de contenido generado por usuarios en las redes sociales, es necesario desarrollar herramientas que faciliten la labor humana en la detección y prevención del discurso de odio, con particular foco de aquel que incita a la violencia física.


En este capítulo haremos una introducción a este problema desde varias ópticas. Analizaremos las diversas definiciones de discurso de odio y haremos una breve reseña de este fenómeno desde un marco legal y de tratados internacionales para luego centrarnos en este problema desde una perspectiva del procesamiento de lenguaje natural. En base al dataset de la competencia hatEval \cite{hateval2019semeval}, propondremos técnicas de detección de discurso de odio para las tareas propuestas, algunas de ellas presentadas en \citet{atalaya_tass2018}. Finalmente, marcaremos algunos problemas actuales en los enfoques actuales de la detección de discurso discriminatorio y algunas oportunidades de mejora que abordaremos en capítulos subsiguientes.


\section{¿Qué es el discurso de odio?}
\label{sec:hate_speech_definitions}

\input{src/04_hate_speech_definitions.tex}

\section{Trabajo previo}

En esta sección haremos una reseña de la literatura de la detección de discurso de odio y otros fenómenos similares. Un análisis extensivo de esta disciplina escapa totalmente al alcance de esta tesis debido a la enorme cantidad de trabajo del área, que ha explotado . Referimos a quien esté interesado a \citet{schmidt2017survey} y a \citet{fortuna2018survey}. Más recientemente, \citet{poletto2021resources} hace un análisis pormenorizado y actualizado de los recursos para la tarea de detección de discurso de odio.

La detección del discurso del odio es una tarea de clasificación de oraciones bastante relacionada con el análisis de sentimientos y ha sido estudiada para varias redes sociales \cite{thelwall2008social, pak2010twitter, saleem2017web}. Uno de los primeros trabajos al respecto es \citet{greevy2004classifying} usando bolsas de palabras y SMVs para detectar contenido racista en páginas web. Construyeron su dataset de manera semi-supervisada buscando sitios mediante keywords y sus links en motores de búsqueda. Siguiendo un enfoque similar, \citet{warner2012detecting} usó unigrams y clusters Brown con SVM para detectar mensajes antisemitas en Twitter.

\citet{waseem2016hateful} anotó un corpus y usó n-gramas de caracteres para detectar comentarios de odio, y \citet{badjatiya2017deep} usó el mismo conjunto de datos para entrenar modelos de aprendizaje profundo e incrustaciones ajustadas junto con Gradient Boosted Trees. \citet {zhang2018detecting} entrenó una red neuronal profunda que combina CNN con unidades recurrentes cerradas \cite{cho2014learning}, superando a los sistemas anteriores en varios conjuntos de datos.

\citet{anzovino2018automatic} recopiló un corpus de tweets misóginos y propuso una taxonomía para distinguirlos en diferentes categorías. Los autores propusieron una serie de técnicas diferentes para clasificarlos, mostrando que enfoques simples (como el uso de modelos lineales junto con n-gramas de token) logran un rendimiento competitivo en conjuntos de datos de pequeño tamaño.

En cuanto a las tareas compartidas, \citet{fersini2018overview} presentó un shared task en la detección de misoginia en Twitter, tanto en español como en inglés, mientras que \citet{fersini2018evalitaoverview} planteó un desafío similar pero en italiano e inglés. \citet{bosco2018overview} propuso un concurso de detección automática sobre publicaciones de Twitter y comentarios de Facebook, que incluía discursos de odio en general.

Dentro de los trabajos en español, \citet{plaza2021pretrained} evalua distintos modelos pre-entrenados de lenguaje sobre la tarea de detección de discriminación usando dos datasets: el primero, \citet{pereira2019detecting} que consta de 6000 tweets, recolectado por el Estado Español para monitorear el discurso de odio en redes sociales; y el segundo, el dataset de SemEval 2019 Task 5 (hatEval) \cite{hateval2019semeval}, presentado en contexto de una shared-task y que comprende ataques contra inmigrantes y mujeres.


\section{Descripción del dataset}
\begin{table}[t]
    \centering
    \small
    \begin{tabular}{l|l l l | l l l}
        Categoría  &    \mc{3}{Español}                          & \mc{3}{Inglés}                                \\
                   &    Train      & Dev          & Test         &    Train      & Dev          & Test           \\
        No HS      &2,643 (58.7\%) & 278 (55.6\%) & 940 (58.8\%) &5,217 (58.0\%) & 573 (57.3\%) & 1,740 (58.0\%) \\
        HS         &1,857 (41.3\%) & 222 (44.4\%) & 660 (41.2\%) &3,783 (42.0\%) & 427 (42.7\%) & 1,260 (42.0\%) \\
        TR         &1,129 (60.8\%) & 137 (61.7\%) & 423 (64.1\%) &1,341 (35.4\%) & 219 (51.3\%) & 529 (42.0\%)   \\
        AG         &1,502 (80.9\%) & 176 (79.3\%) & 474 (71.8\%) &1,559 (41.2\%) & 204 (47.8\%) & 594 (47.1\%)   \\
        Total      &4,500          & 500          & 1,600        &9,000          & 1,000        & 3,000          \\
    \end{tabular}
    \caption{Números del dataset de \citet{hateval2019semeval}, por idioma y por partición. No HS representa los tweets que no tienen contenido odioso, HS aquellos que sí, TR aquellos que son individualizados, y AG aquellos que son agresivos. Entre paréntesis encontramos los porcentajes de incidencia, considerando TR y AG dentro de aquellos que son discriminatorios}
    \label{tab:hateval_dataset}
\end{table}

El dataset que utilizamos en este capítulo es el provisto por \citet{hateval2019semeval}. Este dataset está orientado a la detección de discurso de odio contra mujeres e inmigrantes en Twitter, tanto en inglés como en español. Nuestro trabajo estará centrado en el dataset en español, aunque también propondremos modelos de clasificación para el problema en inglés.

Las instancias del dataset poseen las siguientes etiquetas:

\begin{itemize}
    \item \textbf{HS}: una etiqueta binaria que marca si el tweet tiene contenido discriminatorio (0 si no lo tiene, 1 si hay discurso de odio)
    \item \textbf{Target}: Si hay HS, una etiqueta binaria que marca si el objetivo del discurso de odio es un objetivo genérico (0) o si se refiere a un individuo específico (1)
    \item \textbf{Agresividad}: Si hay HS, una etiqueta binaria que marca si el tweet es agresivo
\end{itemize}

\begin{table}
    \centering
    \footnotesize
    \begin{tabularx}{\textwidth}{X c c c}
        Texto & HS & TR & AG \\
        \hline
        Los tomas asi puro como si fuera jugo y cuando te querés rescatar estas hablando en árabe URL & 0 & 0 & 0 \\
        Como son españoles nada... sin fueran refugiados...GLORIA \# migrates \# refugiados \# EspañaLoPrimero URL & 1 & 0 & 0 \\
        @OmarPrietoGob ``Extranjero sin identificación será puesto en la frontera'' ENVÍA AL EJERCITO A TOMAR CONTROL DE LAS PULGAS PLATANEROS Y CURVA DE AHÍ PARA QUE VEAS COMO HAY COLOMBIANOS INDOCUMENTADOS COMO MONTE AHÍ DE BUHONERS PORQUE LA POLICÍA & 1 & 0 & 1 \\
        Inmigrante da una brutal paliza a una joven por no dejarse besar en Ciudad Real.\#stopinvasion \#YoSiTeCreo \#NoesNo lo peor que no han salido a la calle las feminas del Twitter que tanto se indignaron con la salida de La Manda a la calle. & 1 & 1 & 0 \\
        @elisacarrio Callate hija de puta gorda falopera & 1 & 1 & 1 \\
        \hline
    \end{tabularx}
    \caption{Ejemplos del dataset de SemEval 2019 Task 5: HatEval. HS indice la presencia de discurso de odio, TR la presencia de discriminación individualizada, y AG la presencia de discriminación agresiva}
    \label{tab:hateval_dataset_examples}
\end{table}


La tabla \ref{tab:hateval_dataset} posee los números del dataset. Podemos observar entre los dos idiomas que, si bien la proporción de discurso de odio se mantiene muy similar en ambos idiomas (58\% vs 42\% aproximadamente), la proporción de discurso de odio individualizado (TR) y agresivo (AG) es notoriamente más alto para el español. Esto puede deberse, entre otras cosas, a distintas estrategias de recolección de los datasets. La tabla \ref{tab:hateval_dataset_examples} posee algunos ejemplos para cada una de las características en cuestión.

\section{Tareas de clasificación propuestas}

Sobre el dataset de hatEval, los autores propusieron dos tareas:

\newcommand{\subtaska}[0]{\textbf{Tarea A}}
\newcommand{\subtaskb}[0]{\textbf{Tarea B}}

\begin{itemize}
    \item \subtaska{}: Dado un tweet predecir si contiene discurso de odio contra mujeres o inmigrantes (HS)
    \item \subtaskb{}: Dado un tweet, predecir si contiene discurso de odio (HS), si está dirigido contra un individuo o un grupo (TR), y si es agresivo o no (AG)
\end{itemize}


La primer tarea es la versión clásica de la detección de discurso de odio, donde predecimos una etiqueta binaria. La segunda es una versión más rica, de grano fino, donde predecimos varias características de particular interés para distinguir algunas formas potencialmente más peligrosas de este fenómeno: por ejemplo, si es agresivo y si es individualizado, lo que puede indicar alguna incitación a un ataque.

La performance de \subtaska{} es medida mediante la Macro F1 de la clase positiva y negativa. En el caso de \subtaskb{}, se mide por la Macro F1 de las 3 clases (HS, TR, AG) y también por la medida Exact Match Ratio:

\begin{equation*}
    EMR = \frac{1}{n} \sum\limits_{i=1}^{n} I(Y_i, Y_i^*)
\end{equation*}

siendo $Y_i$ las etiquetas respectivas $(HS, TR, AG)$, $Y_i^*$ las etiquetas que predice nuestro sistema, e  $I$ la función indicadora ($I(x, x) = 1$; $0$ en cualquier otro caso). Observado más de cerca, esto puede entenderse la accuracy sobre la 3-upla de la salida de los clasificadores, pero usamos la terminología de Exact Match Ratio como \citet{zhang-2014-multilabel} para referirse a esta métrica.

\section{Método}

\subsection{Preprocesamiento}
\label{sec:04_preprocessing}

Definimos dos niveles de preprocesamiento: preprocesamiento básico y orientado a sentimientos, dependiendo el modelo a utilizar. El preprocesamiento básico de tweets es el mismo que describimos en la sección \ref{sec:03_preprocessing}, y es el que utilizaremos con los modelos pre-entrenados o modelos neuronales.

El preprocesamiento orientado a sentimientos incluye además lematización (usando TreeTagger \cite{schmid95}) y manejo de negación.
Para el manejo de la negación, seguimos un enfoque simple:
% \cite {das01, pang02}:
Buscamos palabras de negación y agregamos el prefijo 'NOT \_' a los siguientes tokens. Se niegan hasta tres tokens, o menos si se encuentra un token que no sea una palabra.

\subsection{Modelos}

Para las tareas propuestas, analizamos el desempeño de diversos modelos de clasificación. Algunos de ellos son los presentados para la shared-task \hateval{} en \citet{atalaya_tass2018}, a las cuales agregamos modelos basados en transformers. \footnote{Estos modelos no estaban disponibles al momento de presentar dicho trabajo. El trabajo de BERT\cite{devlin2018bert} es de finales de 2018, y hasta finales de 2019 no fue publicada una versión entrenada en español, \beto{}}. Para la tarea de detección binaria (\subtaska{}) planteamos 3 tipos de clasificadores:

\begin{itemize}
    \item Modelos lineales: regresiones logísticas y SVM con kernel lineales, consumiendo como entrada bolsas de palabras, bolsas de caracteres, y tweet embeddings (
    \item Redes neuronales recurrentes: usando como entrada word-embeddings y embeddings contextualizados (\elmo{})
    \item Modelos pre-entrenados basados en LM: ídem sección \ref{sec:03_classification}.
\end{itemize}

Para los modelos lineales, utilizamos tweets embeddings calculados con SIF (ver sección \ref{sec:02_tweet_embeddings}) para más detalles). Utilizamos como base embeddings de \fasttext{} entrenados sobre tweets utilizando el preprocesado orientado a sentimientos descripto en la sección \ref{sec:04_preprocessing}. Para el resto de los modelos, utilizamos el preprocesado básico.

Para el caso de la tarea de multidetección (\subtaskb{}), podemos pensar este problema de dos maneras:

\begin{enumerate}
    \item Un problema de clasificación múltiple
    \item Un problema de clasificación de 5 clases
\end{enumerate}

En el primer caso, el enfoque es el de predecir por separado HS, AG, y TR. La segunda formulación se basa en observar que no todas las 8 combinaciones son permitidas, sino sólo 5: si no hay HS no nos interesa observar las otras dos variables. Tenemos entonces 5 clases a predecir.

Con esta última observación, propusimos en \citet{atalaya_tass2018} un modelo basado en SVM (consumiendo la misma entrada que detallamos anteriormente). No evaluamos en dicho trabajo un modelo recurrente con este esquema de clasificación, ni tampoco lo haremos aquí, considerando que evaluaremos modelos que han demostrado tener mejor performance para numerosas tareas de clasificación de texto.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{img/bert_model_hateval.pdf}
    \caption{Modelo basado en BERT para la tarea de clasificación múltiple. Cada variable (HS, TR, AG) representa un problema de clasificación en sí mismo}
    \label{fig:bert_hateval_classifier}
\end{figure}


Así mismo, proponemos para esta subtarea modelos basados en transformers basado en multi-clasificación. La arquitectura usual de modelos basados en BERT para clasificación constan de poner como última capa una  ``cabeza'' que consume la salida del token \verb|[CLS]|. Esto agrega una matriz de parámetros $W \in \mathbb{R}^{m \times 768}$ donde $m$ es la cantidad de clases de nuestro problema y 768 corresponde a la dimensión de cada vector del modelo de transformers, y usando softmax como función de activación.

Para construir un modelo de multi-clasificación, mantenemos la misma arquitectura pero, en lugar de usar como activación la función softmax, utilizamos la función sigmoidea elemento a elemento. En el caso de clasificación de $n$ clases, $softmax(W x + b)$ nos da para el elemento i-ésimo el score de que la instancia pertenezca a la clase $i$. Por otro lado, en el caso de multiclasificación de $n$ variables, $\sigma(W x + b)$\footnote{Esta expresión es elemento a elemento} nos da el score de predecir la etiqueta positiva para el (en nuestro caso,  $\sigma(W x + b)_1$ nos da $P( HS = 1 \mid x)$,  $\sigma(W x + b)_2$ nos da  $P(TR = 1 \mid x)$, etc). La figura \ref{fig:bert_hateval_classifier} ilustra el modelo utilizado.

Para entrenar el modelo de clasificación, evaluamos dos tipos de funciones de costo. En primer lugar, utilizamos la suma o promedio\footnote{Es equivalente optimizar una u otra} de las entropías cruzadas binarias. Concretamente, si $y = (y_{HS}, y_{TR}, y_{AG})$ son las etiquetas de una instancia e $\widehat{y}$ la predicción del modelo, la función de costo es:

\begin{equation}
\label{eq:multi_loss}
L(y, \widehat{y}) = \sum\limits_{k \in \{HS, TR, AG\}} J(y_k, \widehat{y_k})
\end{equation}

donde $J$ es la entropía cruzada. Esta función de costo, sin embargo, ignora cualquier tipo de jerarquía entre las variables; por ejemplo, si para una instancia tenemos $HS = 0$, calcula el costo también de las variables $TR$ y $AG$. Contemplamos entonces una variante de esta función para tener en cuenta esta jerarquía:

\begin{equation}
    \label{eq:hierarchical_loss}
    L(y, \widehat{y}) =  J(y_{HS}, \widehat{y_{HS}}) + \beta(y_{HS})\sum\limits_{k \in \{TR, AG\}} J(y_k, \widehat{y_k})
\end{equation}

donde $\beta(y_{HS})$ pondera la pérdida de las variables del segundo nivel de nuestra jerarquía. Una opción puede ser considerar $\beta(1) = 1, \beta(0) = 0$, donde ignoramos las pérdidas de las variables $TR, AG$ cuando no hay discurso discriminatorio. Análogamente, $\beta(y) = 1$ sería el caso descripto en la ecuación \ref{eq:multi_loss}. Una forma de generalizar esto es agregando un hiperparámetro $\gamma \in [0, 1]$ para escribir $\beta(y) = (1-y) \gamma + y$.

Las evaluaciones de los modelos las realizamos poniendo una máscara por encima de estos modelos de clasificación múltiple de manera de evitar salidas incoherentes (por ejemplo, $HS = 0, TR = 1, AG= 0$).


\section{Resultados}

\newcommand{\esrow}[1]{\multirow{#1}{*}{es}}
\newcommand{\enrow}[1]{\multirow{#1}{*}{en}}

\begin{table}[ht]
    \centering
    \begin{tabular}{l l| l l l l}
        Modelo          & Idioma      & Precision  & Recall      & F1            & Macro-F1 \\
        \hline
        SVM$^*$         & \mr{3}{es}  & 0.639       & 0.800       & 0.711       & 0.730  \\
        ELMO-RNN        &             & 0.661       & 0.753       & 0.704       & 0.735  \\
        \beto{}         &             & \tbf{0.674} & \tbf{0.839} & \tbf{0.747} & \tbf{0.764} \\
        \hline
        bert            & \mr{3}{en}  & 0.474       & \tbf{0.968} & 0.637       &  0.496 \\
        roberta         &             & 0.470       & 0.967       & 0.632       &  0.486 \\
        bertweet        &             & \tbf{0.495} & 0.959       & \tbf{0.653} & \tbf{0.546} \\
        \hline
    \end{tabular}
    \caption{Resultados de la evaluación para la detección de discurso de odio en datasets de desarrollo y test, medidas por precisión y sensitividad sobre la clase positiva (discurso de odio) y por la métrica macro F1. Con $^*$ están marcados los resultados presentados en \citet{atalaya_tass2018}. En negrita, el mejor resultado.}
    \label{tab:hateval_task_a}
\end{table}



La tabla \ref{tab:hateval_task_a} muestra los resultados de evaluación para los modelos propuestos para la detección de discurso de odio binaria (\subtaska{}). Marcamos con un asterisco aquellos modelos presentados en \citet{atalaya_tass2018}. Respecto a los resultados en español, el modelo basado en SVMs obtiene una buena performance, aún contra aquel basado en embeddings contextualizados, habiendo obtenido el mejor desempeño en la competencia con $0.730$ de Macro F1. La pobre performance de ELMo contra un modelo mucho más simple puede deberse a un mal pre-entrenamiento del modelo base\footnote{No queda claro que en entrenar este modelo sobre 20M palabras sea suficiente, ni que sea un dataset suficientemente general} y también debido al cambio de dominio, a los cuales modelos pre-entrenados previos a BERT son sumamente sensibles \cite{hendrycks-etal-2020-pretrained}.

Para ambos idiomas, los modelos basados en transformers \cite{vaswani2017attention} obtienen la mejor performance, con considerables mejoras respecto a los modelos basados en ELMo y a los SVMs. Particularmente, en el caso del inglés, \bertweet{} \cite{dat2020bertweet} obtiene la mejor Macro-F1. En el capítulo 7 presentaremos un modelo similar a \bertweet{} para español que mejora la performance sobre \beto{}, a la vez que evaluaremos sobre versiones de \beto{} ajustadas al dominio social\footnote{Un modelo que no evaluamos en el presente trabajo es la versión en español de RoBERTa, recientemente entrenada. En el capítulo 7 evaluaremos su performance}.


\begin{table}[t]
    \centering
    \small
    \begin{tabular}{lll rrr rr}
        Modelo            &        & Idioma      &  HS F1     & TR F1        &  AG F1        &   EMR       &  Macro F1       \\
        \hline
        \mr{3}{\beto{}}      & multi  & \mr{3}{es}  &  0.741     &  \tbf{0.765} &  \tbf{0.688}  & 0.685       &     \tbf{0.731} \\
                          & hier   &             &  0.735     &  0.758       &  0.674        & \tbf{0.703} &     0.722          \\
                          & combi  &             &  \tbf{0.742} &  0.763       &  0.668        & 0.698       &     0.724          \\
        \hline
        \hline
        \mr{3}{BERT}      & multi  & \mr{3}{en}  &  0.638     &  0.600       &  0.443        & 0.380       &     0.560       \\
                          & hier   &             &  0.642     &  0.592       &  0.451        & 0.388       &     0.562       \\
                          & combi  &             &  0.644     &  0.593       &  0.442        & 0.398       &     0.560       \\
        \hline
        \mr{3}{RoBERTa}   & multi  & \mr{3}{en}  &  0.634     &  0.578       &  0.454        & 0.365       &     0.555       \\
                          & hier   &             &  0.637     &  0.572       &  0.456        & 0.370       &     0.555       \\
                          & combi  &             &  0.636     &  0.576       &  0.442        & 0.377       &     0.551       \\
        \hline
        \mr{3}{\bertweet{}}  & multi  &\mr{3}{en}   &  0.658     &  0.629       &\tbf{0.462}    & 0.426       & \tbf{0.583}     \\
                          & hier   &             &  0.656     &  0.617       &  0.450        & 0.423       &     0.574       \\
                          & combi  &             & \tbf{0.666}&\tbf{0.637}   &  0.444        & \tbf{0.449} &     0.582       \\
        \hline
    \end{tabular}

    \caption{Resultados de la evaluación para para \subtaskb{} en términos de las F1 de las clases HS (Hate Speech), TR (Targeted), AG (Aggressive), el Exact Match Ratio (EMR), las Macro F1 de las clases en cuestión, y la Macro F1 de la clase HS. Las 3 variaciones de los modelos son: \emph{multi} es la salida de multiclasificación estándar, \emph{hier} es la salida de multiclasificación con una jerarquía de clasificación, y \emph{combi} es la salida de multiclasificación con una combinación de clasificaciones. Los resultados están expresados como las medias de 10 corridas independientes.}
    \label{tab:hateval_task_b}
\end{table}


La tabla \ref{tab:hateval_task_b} muestra los resultados de la \subtaskb{}, reportado por las F1 de cada variable predicha (HS, TR, AG), así como por el Macro F1 de HS y el Macro F1 de las 3 variables mencionadas. Los resultados están expresados como la media de 10 corridas independientes del experimento para cada configuración distinta. Consideramos las 3 versiones: \emph{multi} refiere a clasificación múltiple, \emph{hier} a clasificación múltiple con la función de costo jerárquica, y \emph{combi} a la conversión del problema en una clasificación de 5 clases.

Podemos observar que para español, la mejor performance en términos de EMR (la métrica más estricta) es el clasificador entrenada con la función de costo definida en \ref{eq:hierarchical_loss} (con el hiperparámetro $\gamma = 0.1$); sin embargo, esta diferencia la diferencia entre las performances no es significativa al correr un test de Kruskal-Wallis ($H(9) = 3.492, p = 0.174$). En términos de Macro-F1, la mejor performance es de \beto{} con el problema de clasificación múltiple y sin la función de costo jerárquica, pero de nuevo esta diferencia no es significativa ($H(9) = 3.656, p=0.16$).

Respecto al inglés, los mejores resultados pueden observarse en el modelo entrenado con \bertweet{} con la salida de 5 clases en el caso del EMR, y con la salida múltiple (sin pérdida jerárquica) para la Macro-F1. Este resultado, sin embargo, queda en términos de EMR por debajo del baseline, y cercano en términos de Macro F1 a los mejores resultados de la competencia. En \citet{gertner-etal-2019-mitre}, se basaron en un ensemble de modelos entrenados con BERT y usando también un ajuste de dominio sobre tweets. Esta baja performance de nuestros modelos (y de los modelos en general sobre ese dataset) puede deberse a problemas de anotación y de que las particiones de train y test no son idénticamente distribuidas. \footnote{En\citet{gertner-etal-2019-mitre} dan evidencia de esto en su trabajo algo que perjudica el desempeño de estos modelos}



\begin{table}[t]
    \centering
    \small
    \begin{tabular}{l  l l | c c c c}
        \toprule
        Modelo              & Idioma        &  Tarea  &     Precision     &          Recall    &              F1    &        Macro F1    \\
        \hline
        \mr{2}{\bertweet{}} & \mr{2}{en}    &  A      & 0.495 $\pm$ 0.012 &  0.959 $\pm$ 0.012 &  0.653 $\pm$ 0.009 &  0.546 $\pm$ 0.027 \\
                            &               &  B      & 0.505 $\pm$ 0.011 &  0.948 $\pm$ 0.018 &  0.658 $\pm$ 0.005 &  0.567 $\pm$ 0.022 \\
        \hline
        \mr{2}{\beto{}}     & \mr{2}{es}    &  A      & 0.674 $\pm$ 0.021 &  0.839 $\pm$ 0.026 &  0.747 $\pm$ 0.007 &  0.764 $\pm$ 0.011 \\
                            &               &  B      & 0.713 $\pm$ 0.042 &  0.778 $\pm$ 0.054 &  0.741 $\pm$ 0.013 &  0.771 $\pm$ 0.015 \\
        \bottomrule
    \end{tabular}
    \caption{Comparación de la performance sobre la detección de discurso de odio para los clasificadores entrenados sobre \subtaska{} y \subtaskb{}. Resultados expresados como la media de 10 corridas independientes del experimento junto a sus desviaciones estándar. Ambos clasificadores de la \subtaskb{} están entrenados sobre el problema de multi-clasificación}
    \label{tab:hateval_task_a_vs_b}
\end{table}

Lejos de dañarse la performance de la detección de lenguaje discriminatorio (lo que analizamos en la \subtaska{}), predecir más de una variable pareciera mantener el desempeño e incluso mejorarlo levemente. La tabla \ref{tab:hateval_task_a_vs_b} muestra la comparativa para la detección de discurso de odio (HS) para aquellos clasificadores que obtuvieron mejores resultados para \subtaska{} y \subtaskb{} (en ambos casos, \beto{} y \bertweet{}). En ambos casos, hay ligeras diferencias para a favor de uno u otro clasificador pero no parecieran ser significativas.






\subsection{Análisis de Error}
\label{sec:hateval_error_analysis}
\input{src/04_error_analysis.tex}

\section{Discusión}
\label{sec:04_discussion}

Respecto a lo relativo a la performance de los modelos presentados, los modelos basados en transformers son notoriamente superiores a los demás modelos, en ambas tareas en idiomas. Particularmente, en inglés, podemos observar que aquellos pre-entrenados sobre tweets (\bertweet{}) tienen mejor performance que aquellos que son pre-entrenados sobre wikipedia como BERT o RoBERTa.

Sobre la tarea más difícil de detección múltiple de discurso de odio (\subtaskb{}), propusimos varios enfoques: uno basado en predecir cada variable por separado y otro en predecir una variable que indique la combinación en cuestión. El modelo de predicción múltiple entrenado con la función de costo jerárquica \ref{eq:hierarchical_loss} obtuvo la mejor performance en términos de EMR, y la de multi-clasificación obtuvo la mejor en términos de Macro F1. En el caso de inglés, el modelo entrenado sobre 5 clases obtuvo la mejor performance en EMR y de nuevo el de multi-clasificación sobre Macro F1; sin embargo, esta dista de la mejor performance de la competencia (obtenida por el equipo MITRE \cite{gertner-etal-2019-mitre}) que usa una combinación de técnicas, algunas de las cuales veremos en el capítulo \ref{chap:07_domain_adaptation} \footnote{En ese trabajo hacen un ensemble de clasificadores BERT adaptados a dominio}. De estos dos casos, el modelo de multi-clasificación corre con la ventaja de calcular cada variable de manera independiente y tener un hiperparámetro menos.

Algo que merece cierta atención es que, lejos de empeorar el desempeño de nuestros modelos, agregar nuevas variables a predecir (además de la existencia de discurso de odio) pareciera mejorar levemente la performance de la detección de este fenómeno, a la vez que obteniendo salidas más ricas e interpretables. Más aún, observamos que modelos como el del equipo MITRE \citet{gertner-etal-2019-mitre} en dicha competencia mejoraron la performance con una capa adaptadora que modela las dos variables latentes que el dataset no brinda: la misoginia y el racismo. Teniendo esto en cuenta, una pregunta a explorar es si contar con esta información (las características agredidas) puede mejorar la performance de los clasificadores o tener salidas más interpretables que sólo una etiqueta binaria.

Hacemos a continuación una disquisición no sólo sobre este trabajo y el dataset en el que se basa sino en líneas generales sobre los recursos y enfoques actuales en el área de detección de discurso de odio. Continuando con la idea del párrafo anterior, una limitación que puede verse es que la mayoría de los trabajos atacan una, dos, o a como mucho tres características protegidas. Por ejemplo, los datasets de \cite{waseem2016hateful} y \cite{hateval2019semeval} (el utilizado en esta tarea) sólo consideran racismo y sexismo, mientras que \citet{Davidson2017AutomatedHS} agrega homofobia a esta consideración. Sería deseable poder contar con un dataset que como mínimo cuente con estas tres características en conjunto a otras quizás menos utilizadas: odio de clase (a veces conocida como ``aporofobia''), discriminación por aspecto físico, o por discapacidad. Esto, desde ya, con un framework unificado de anotación y no recolectando datasets anotados individualmente.

Un problema particular que se puede observar en este dataset (pero que atraviesa a muchos otros) es el proceso de recolección de los datos: los tweets son recolectados mayormente a través de keywords. Como está explicado en el overview de esta shared-task \cite{hateval2019semeval}, se usó para su recolección una combinación de estrategias. Sin embargo (ver sección XXX), hay una altísima incidencia de algunas palabras (como \emph{sudaca} o \emph{inmigrante}) que sesgan fuertemente el dataset. Esto (entre otras cuestiones) puede ser un problema para los modelos que se entrenan sobre estos datos, haciendo que aprendan correlaciones espurias. Si bien pueden aplicarse técnicas de regularización (como \emph{data augmentation}) para mitigar
esto, no deja de ser un problema. De todas formas, esto es una limitación general para estos tipos de aprendizaje sobre datos crudos y etiquetas, donde es difícil establecer e interpretar cómo un clasificador termina encontrando patrones para detectar el fenómeno medido.

La \textbf{anotación}, la etapa subsiguiente a la recolección de datos, pareciera presentar en este dataset algunos problemas. Hemos visto en la anterior sección una lista no extensiva de numerosos errores de etiquetado, aún cuando este dataset fue realizado con un etiquetado de 2 + desempate. Si bien es difícil trazar las razones detrás de estos problemas, observando las instancias podría uno imaginarse que esto es producto de un no entendimiento de las expresiones en los distintos dialectos del español. \citet{waseem-2016-racist} mostró que las anotaciones ``amateurs'' (producto del uso de crowdsourcing) tienden a tener mayores instancias de Hate Speech (algo que daría la impresión de ocurrir aquí) y que datasets anotados por expertos mejoran la performance de los modelos. Este problema podría profundizarse dado que no queda claro si los anotadores son hablantes nativos de español.

%%
%% Falta de contexto
%%

Un problema que observamos en el dataset estudiado en este capítulo (pero que aplica a otros también) es la \textbf{falta de contexto}: muchos de los mensajes carecen de información adicional sobre la noticia o el tema del que se está hablando. Cuando leemos un mensaje de un tweet, casi siempre los leemos en el contexto de una noticia, o un trending topic. Muy rara vez leemos un mensaje en total aislamiento. De hecho, gran parte de los comentarios de este dataset tiene un contexto implícito: la noticia de conflicto migratorio en Ceuta. Otros comentarios, por otro lado, no se entienden bien ya que son respuestas a un tweet y que según el hilo de conversación pueden entenderse o no como discriminatorios.

Sobre esta falta de contexto, hay muchos mensajes aislados que pueden requerir información adicional para entender su significado. Por ejemplo, un comentario que dice ``hay que matarlos'' puede o no entenderse como discurso de odio. Si el objeto del mensaje se refiere a mosquitos, ese mensaje no es odioso; si, por otro lado, está hablando sobre chinos en el contexto del COVID-19, entonces ese mensaje es discriminatorio (y además llama a tomar una medida violenta). En ese sentido, podemos preguntarnos sobre este punto es si el acceso a información contextual nos puede auxiliar en la detección de discurso de odio: siendo este un hilo de conversación, una noticia a la que se refiere, o alguna otra forma de de información.

Otro problema que suele ocurrir relacionado al anterior es que no tenemos \textbf{información granular} de los datos anotados. Si bien algunos trabajos agregan información de la característica vulnerada, la mayoría simplemente agrega una etiqueta binaria sobre la existencia o no de discurso de odio (o bien algún nivel intermedio como si hay o no discurso ofensivo, como el caso de \citet{Davidson2017AutomatedHS}). Teniendo en cuenta lo observado en este capítulo, agregar información más detallada sobre cada caso puede ayudar a mejorar la detección del discurso de odio mediante una señal más rica a nuestros clasificadores sobre las diferentes fronteras de cada característica.


\section{Conclusiones}

En este capítulo hemos hecho una primer acercamiento a la tarea de detección de lenguaje discriminatorio, haciendo un repaso de su definición desde un marco legal y desde el usado en la literatura de Procesamiento de Lenguaje Natural. Analizamos técnicas de clasificación del estado del arte sobre el dataset presentado en la shared task multilingual \emph{hatEval}\cite{hateval2019semeval}. En base a este dataset, analizamos dos tareas: detección binaria de discurso de odio, y detección de múltiples variables (si es discurso de odio, si es dirigido, si es agresivo).

Para estas tareas, presentamos varios modelos de clasificación. Por un lado, clasificadores lineales que consumen distintos tipos de entrada como ser tweet embeddings y bolsas de caracteres; modelos recurrentes que consumen embeddings contextualizados; y finalmente, modelos del estado del arte basados en modelos de lenguaje pre-entrenados usando la arquitectura de transformers. Para ambas, los modelos de transformers

En el caso de la tarea de detección múltiple, propusimos dos formas de atacar el problema: como clasificación múltiple (prediciendo simultáneamente las 3 variables), y convirtiendo a un problema de clasificación simple sobre 5 clases posibles. Observamos, a su vez, que lejos de dañar la performance de la detección de discurso de odio, predecir más de una variable mejora la performance de nuestros clasificadores.

Analizando este dataset y algunos otros de la bibliografía, marcamos algunas puntos de mejora en la detección de discurso de odio: principalmente, la falta de información contextual. La mayoría de los datasets no tienen mayor información sobre los mensajes de los usuarios, algo que usualmente no pasa en las redes sociales. Por otro lado, y teniendo en cuenta la observación hecha en el párrafo anterior, nos preguntamos si tener información más granular acerca de las características. Otro punto no menor es que para la creación de datasets de un fenómeno tan complejo y social es indispensable tener muchos recaudos a la hora de la anotación, algo que ya ha sido observado en otros trabajos.

En los siguientes capítulos, exploraremos algunas de estas observaciones. Particularmente, nos centraremos en la incorporación de contexto en la detección de discurso discriminatorio, construyendo un dataset que incorpore esta información a los mensajes anotados, y explorando cómo mejorar los algoritmos del estado del arte que aprovechen esa información.
