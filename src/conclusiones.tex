En esta tesis, hemos abordado la tarea de la detección de discurso de odio, intentando hacer avanzar el estado del arte basado mayormente en la clasificación binaria de este fenómeno sobre comentarios de usuarios en redes sociales. En ese sentido, propusimos una extensión de la tarea agregándole un marco contextual a cada instancia analizada. Esta información es usualmente descartada en la literatura del tema, que ha estado centrada en el análisis de comentarios aislados. Para estudiar esta extensión, construimos un conjunto de datos de respuestas de usuarios a artículos periodísticos argentinos en la red social Twitter. Luego de hacer una reseña del trabajo previo, tuvimos el cuidado de construir este recurso de manera interdisciplinaria --en la frontera del derecho, la sociología y el procesamiento del lenguaje natural-- y teniendo en cuenta el componente cultural del discurso de odio, evitando recaer en el etiquetado mediante plataformas de terceros que limitan esta posibilidad.


En base a los experimentos de clasificación realizados sobre este conjunto de datos, hemos podido brindar cierta evidencia de que el contexto --en este caso, en forma de tweet de medio periodístico-- puede aprovecharse para identificar discursos de odio mejorando el rendimiento de clasificadores basados en técnicas del estado del arte. Si bien no se observó lo mismo al utilizar un contexto más largo --el artículo periodístico completo-- trabajo futuro debería explorar si existen formas útiles de incorporar esta información al clasificador. De manera heurística, podríamos argumentar que los humanos tenemos acceso a contextos muchos más ricos, accediendo por otras vías a información sobre la noticia y el comentario en cuestión, incorporando conocimiento del mundo real --como por ejemplo, si la persona sobre la que habla la noticia posee cierta característica protegida no mencionada. Los clasificadores del estado del arte carecen de esta información, con lo cual una posible línea de investigación puede ser la de incorporar este conocimiento dentro de los algoritmos.

Este resultado --sobre los beneficios de utilizar información contextual-- va en línea con algunos trabajos recientes en NLP que muestran que la utilización de más de una fuente de información puede ser beneficiosa para ciertas tareas. Esto es algo esperable ya que nuestro entendimiento dista de ser descontextualizado --sólo sobre un comentario o un texto aislado-- sino que incorpora diversos elementos: desde el tópico de la conversación, quiénes son los interlocutores, conocimiento externo, entre otras cuestiones.

Un punto adicional que observamos es que la predicción de múltiples características --además de la mera existencia del discurso de odio-- no sólo no empeora el rendimiento de los algoritmos sino que lo mejora parcialmente para las técnicas del estado del arte. Es decir, si en vez de sólo predecir que existe o no discurso de odio predecimos más características --como ser la o las características ofendidas, si existe un llamado a la acción violenta, si está dirigido a un grupo o un individuo-- podemos, por un lado, mejorar la interpretabilidad y la riqueza de la salida de los algoritmos de detección; y por otro, mejorar su rendimiento al entrenarlos sobre una señal más rica. De esto, se desprende que es conveniente generar recursos que contengan anotaciones más detalladas --no sólo la etiqueta binaria-- y en lo posible marcando las características protegidas que se estén vulnerando en cada instancia.


Respecto a las limitaciones de este trabajo y sus conclusiones, una cuestión particular es que el conjunto de datos mencionado fue etiquetado considerando en todo momento el contexto del comentario. Una comparación justa entre algoritmos que incorporen contexto contra algoritmos que no lo hagan debería incluir un conjunto de entrenamiento etiquetado de manera descontextualizada, para así poder ajustar los clasificadores de acuerdo a estos dos tipos de anotaciones.

Una limitación más general --ya no sólo de los resultados de esta tesis sino del área del procesamiento del lenguaje natural-- versa sobre el actual estado del arte, basado en modelos de lenguajes neuronales como BERT, GPT, y compañía. Si bien es innegable el avance que han supuesto estos modelos pre-entrenados, habiendo logrado resultados superadores en casi toda tarea de NLP, no podemos dejar de observar que, en términos de lo mencionado por Judea Pearl \cite{pearl2018book}, estos sistemas aún están en una etapa meramente asociacional. Muy a pesar de que muchos trabajos hablen sobre cierto entendimiento por parte de estos algoritmos (por ejemplo, el benchmark General Language Understanding Evaluation, GLUE), estos sólo detectan regularidades en los datos, sin efectuar ningún razonamiento sobre ellos.

Para nuestro problema concreto, un clasificador puede detectar que decirle ``sos hombre'' a un artículo relacionado a una mujer (quizás trans) conlleva discurso de odio contra la comunidad LGBTI. Sin embargo, este mismo mensaje ofuscado de alguna manera (por ejemplo, preguntándole el nombre, pidiéndole el documento, o alguna otra forma que no hayamos observado en los datos) logra burlar a nuestros sistemas ya que están exclusivamente basados en detectar regularidades y no pueden efectuar ningún razonamiento simbólico entre la equivalencia de estos mensajes. Ligado a este ejemplo, algunos trabajos han puesto en duda los avances recientes en el área, indicando que los actuales algoritmos basados en modelos de lenguaje --aún en sus formas más complejas y sobreparametrizadas con miles de millones de parámetros-- no son más que ``loros estocásticos'', muy hábiles en detectar regularidades y hacernos creer que llevan adentro algún tipo de entendimiento \cite{bender-koller-2020-climbing,bender2021dangers}. Sin embargo, el entrenamiento sobre la mera forma del lenguaje --Terabytes de texto no etiquetado-- no conlleva ningún tipo de comprensión sino sólo un aprendizaje sobre su distribución.

Volviendo al fenómeno estudiado en esta tesis, aún cuando gran parte del discurso de odio se expresa en forma de insultos o expresiones ofensivas muy características que son detectables por los algoritmos actuales, hay un subconjunto de estos mensajes que necesitan conocimiento del mundo real, de la relación entre interlocutores, y muchas veces de realizar algún tipo de razonamiento. Para los enfoques actuales de NLP, este tipo de deducciones están fuera de alcance, y hemos observado en el análisis de error algunos casos donde los clasificadores fallaban en la detección de comentarios abiertamente discriminatorios pero que guardaban algún tipo de razonamiento, metáfora, o dificultad adicional. Parafraseando a \citet{mitchell2021ai}, podemos decir que \textbf{la detección de discurso discriminatorio es más difícil de lo que creemos}.

¿Significa esto que los algoritmos actuales no son de ningún uso? En absoluto. Los actuales algoritmos de detección, aún con sus defectos y siendo bastante rudimentarios, logran captar parte del lenguaje discriminatorio que observamos en redes sociales. Sin embargo, es necesario entender sus limitaciones: a medida que estos sistemas puedan encontrar regularidades con más detalle --y potencialmente sean usados para moderar este tipo de agresiones-- muchos usuarios expresarán este tipo de ofensas de manera más sofisticada para lograr esquivar su escrutinio, apelando a metáforas, mensajes indirectos, multimodalidad, etc. Asimismo, un punto no explorado en este trabajo y que limita la aplicación para casos graves de discursos de odio es la limitada interpretabilidad de los algoritmos basados en redes neuronales, un mal endémico a toda el área de Inteligencia Artificial. Teniendo estas cuestiones en mente, planteamos que agregar más contexto e información del mundo real a nuestros algoritmos puede ayudarlos a mitigar parcialmente sus limitaciones.

Para cerrar, un eje que atraviesa este trabajo es que fue realizado íntegramente en español y atendiendo la realidad sociocultural de Argentina. La inmensa mayoría de la literatura sobre este tema es en inglés, algo que está muy alejado no sólo desde lo lingüístico sino también desde el plano cultural. Los discursos de odio están situados dentro de las realidades sociales de cada región, por lo que es necesario estudiarlos atendiendo las distintas problemáticas características y no sólo considerando la variable idioma. Como consecuencia de esto, esta tesis intenta aportar a balancear la asimetría de recursos tanto en el área particular y específica de detección de discurso de odio como así también en la de NLP en general.