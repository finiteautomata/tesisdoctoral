En esta tesis hemos abordado la tarea de la detección de discurso de odio, intentando empujar el estado del arte y proponiendo enfoques superadores a lo que es gran parte de lo que se hace hoy día, basado en la detección binaria de este fenómeno. En ese sentido, propusimos la tarea de detección de odio contextualizada, algo que había sido poco estudiando en la literatura. Construímos un dataset etiquetado para discurso de odio sobre comentarios de usuarios de Twitter sobre artículos periodísticos en español rioplatense. Teniendo en cuenta una breve reseña de algunos trabajos previos, decidimos realizar este trabajo con anotadores nativos de esta variedad dialectal, y no con terceros a través de plataformas para poder obtener etiquetas de mayor calidad.

En base a los experimentos de clasificación realizados, hemos podido brindar cierta evidencia que el contexto --en este caso, en forma de tweet de medio periodístico-- puede aprovecharse para identificar discursos de odio mejorando la performance de clasificadores basados en técnicas del estado del arte. Si bien los experimentos que realizamos no mostraron una mejora en el rendimiento utilizando un contexto más largo --en forma de artículo periodístico completo-- trabajo futuro debería explorar cómo incorporar esta información al clasificador. De manera heurística, podríamos argumentar que los humanos muchas veces utilizamos este contexto ``largo'' implícitamente, accediendo por otras vías a información sobre la noticia en cuestión -- por ejemplo, si la persona sobre la que habla la noticia posee cierta característica protegida que está implícita en la nota.

Este resultado --sobre la posibilidad de utilizar información contextual-- va en línea con varios trabajos recientes del área de extracción de opiniones en redes sociales que cuales muestran que la utilización de varias fuentes de información es beneficiosa para los algoritmos de detección. Esto es algo esperable ya que nuestra percepción de la realidad dista de ser unimodal --sólo percibiendo un comentario o un texto en aislamiento-- sino que incorpora diversos elementos: desde el tópico de la conversación, quiénes son los interlocutores, entre otras cuestiones. Dentro de estas diversas .

Un punto que observamos es que la predicción de múltiples características además de la mera existencia del discurso de odio no sólo no empeora la performance de los clasificadores sino que la mejora marginalmente para las técnicas del estado del arte basadas en modelos pre-entrenados de lenguaje. Es decir, si en vez de sólo predecir que existe o no discurso de odio predecimos más características --como ser la o las características ofendidas, si existe un llamado a la acción violenta, si está dirigido a un grupo o un individuo-- podemos, por un lado, mejorar la interpretabilidad y la riqueza de la salida de los algoritmos de detección; y por otro, mejorar su rendimiento al brindarles una señal más poderosa en su entrenamiento. De esto, podemos fomentar que los próximos datasets de discurso de odio se anoten con más variables a predecir --no sólo la etiqueta binaria-- y en lo posible marcando las características protegidas que se estén vulnerando en cada instancia.


Respecto a las limitaciones puntuales de este trabajo, puede marcarse por un lado el hecho de que sólo realizamos el etiquetado viendo el contexto de cada comentario. Algunos trabajos previos (como \cite{pavlopoulos2020toxicity}) han usado ambos tipos de etiquetado --contextualizado y no contextualizado-- para entrenar los clasificadores contextualizados sobre datos anotados sobre etiquetadores que vieron el contexto, y lo mismo para aquellos no contextualizados.

Debemos ser, sin embargo, cautelosos acerca de los resultados obtenidos en este trabajo y, en líneas generales, de la mayoría de los avances en la detección de discurso discriminatorio. El estado del arte actual en NLP está basado en modelos de lenguajes neuronales pre-entrenados de lenguaje, altamente potentes. En términos de lo mencionado en \emph{The Book of Why} de Judea Pearl \cite{pearl2018book}, estos sistemas están en una etapa meramente ``asociacional''. Es decir, nuestros modelos de lenguaje (muy a pesar de que muchos artículos hablen sobre cierto ``entendimiento'' por parte de estos) tan sólo detectan regularidades en los datos, como si fueran sólamente el ajuste de una curva que un estadístico realiza hace más de un siglo, sin realizar ningún tipo de razonamiento de lo que se está haciendo.

En nuestro problema concreto, un clasificador puede detectar que decirle ``sos hombre'' a un artículo relacionado a una mujer (quizás trans) conlleva discurso de odio contra la comunidad LGBTI. Sin embargo, este mismo mensaje ofuscado de alguna manera (por ejemplo, preguntándole el nombre, o alguna otra forma que no hayamos observado en los datos) logra burlar a nuestros sistemas.Ligado a este ejemplo, \cite{bender-koller-2020-climbing} ilustran este punto: nuestros actuales sistemas, basados en modelos de lenguaje, aún en sus formas más complejas y sobreparametrizadas con miles de millones de parámetros, no son más que ``loros estadísticos'', muy hábiles en detectar regularidades y hacernos creer que llevan adentro algún tipo de razonamiento. Sin embargo, la realidad es que no lo tienen, ya que el entrenamiento sobre la mera ``forma'' del lenguaje --es decir, los gigabytes de texto de entrenamiento de BERT, GPT y amigos-- no conlleva ningún entendimiento ni razonamiento. Podemos decir de todo esto --y parafraseando a \citet{mitchell2021ai}-- que \textbf{la detección de discurso discriminatorio es más difícil de lo que creemos}.

¿Significa esto que los sistemas actuales no sirven para nada? En absoluto. Los actuales algoritmos de detección, aún con sus defectos y siendo bastante rudimentarios, logran detectar parte del lenguaje discriminatorio que observamos en redes sociales. Sin embargo, es necesario entender sus limitaciones: a medida que estos sistemas puedan encontrar regularidades con más detalle, muchos usuarios ocultarán este discurso de manera más sofisticada para lograr saltear su escrutinio (en caso de que estemos hablando de sistemas que se usen con fines de moderación). Teniendo estas cuestiones en cuenta, planteamos que agregar más información y contexto a nuestros algoritmos puede ayudarlos a mitigar parcialmente sus limitaciones.

Para cerrar, un eje que atraviesa este trabajo es que lo realizamos íntegramente en español. La mayor parte de la literatura sobre este tema es en inglés, y entendiendo que el discurso de odio es un fenómeno social y cultural, es necesario estudiarlo en otros idiomas. Por eso, este trabajo intenta aportar a balancear la asimetría de recursos tanto en el área particular y específica de detección de discurso de odio como así también en la de NLP en general.