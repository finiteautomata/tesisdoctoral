\label{chap:05_dataset_creation}

Por lo marcado en la discusi√≥n de la anterior secci√≥n, consideramos interesante el problema de analizar el impacto del contexto en la detecci√≥n de lenguaje discriminatorio. Antes de proseguir, podemos preguntarnos: ¬øa qu√© nos referimos con el t√©rmino ``contexto''? La contextualizaci√≥n, seg√∫n John Cook-Gumperz, es:

\begin{displayquote}[\citet{gumperz1992contextualization}]
    (el) uso que hacen hablantes y oyentes de se√±ales verbales y no verbales para poder conectar lo que se dice en un momento con el conocimiento adquirido a trav√©s de la experiencia para poder mantener la participaci√≥n en la conversaci√≥n y entender lo que se pretende decir.
\end{displayquote}

En este sentido, cualquier se√±al que pueda ayudar a entender las intenciones del interlocutor en una red social es informaci√≥n que ayuda a situar los mensajes: desde el hilo de una conversaci√≥n, la noticia a la que hace referencia, el historial de conversaciones previas entre los interactores, informaci√≥n sociocultural de los interlocutores, entre otras \cite{sheth2021defining}. Para poner un ejemplo de por qu√© es necesario disponer de informaci√≥n adicional al comentario analizado, el mensaje ``sos un hombre'' en solitario puede parecer inofensivo; ahora, si ese mismo mensaje est√° dirigido hacia una mujer trans, su sentido es claramente discriminatorio. El comentario --con claro tono agresivo-- ``hay que tirar una bomba ah√≠'' puede tener car√°cter discriminatorio si lo consideramos en el contexto de una nota que habla sobre China y el COVID-19; sin embargo, es distinto si estamos hablando de un partido de f√∫tbol, donde el remitente de un club manifiesta su enemistad contra otro equipo.


Vimos en el anterior cap√≠tulo que muchos mensajes analizados no se entend√≠an bien al carecer de informaci√≥n contextual, tanto conversacional o del t√≥pico al que hace cuesti√≥n. En l√≠neas generales, la mayor√≠a de los problemas de NLP sobre textos sociales suelen plantearse sobre comentarios sin ning√∫n otro tipo de dato de qui√©n lo emite \todo{chequear tilde}, a qui√©n se lo dirige, ni sobre qu√© tema est√° hablando. Para analizar esto desde el problema de la detecci√≥n de discurso de odio, nos abocamos en primer lugar a la tarea de crear un conjunto de datos que no s√≥lo contenga un mensaje/comentario, sino que provea un contexto para √©ste. Un √°mbito natural para esta tarea son las notas period√≠sticas, donde disponemos de un art√≠culo y comentarios realizados sobre la nota. En este escenario, el comentario es el texto a analizar, mientras que el contexto est√° dado por la nota.

Muchos sitios de noticias disponen de sistemas embebidos de comentarios, pero vista la dificultad para la recolecci√≥n y los limitados datos provistos por estos sitios acerca de sus usuarios nos llevaron a buscar otro medio: Twitter. Esta red social provee una sencilla API para descargar datos, a la vez que tiene t√©rminos y condiciones amigables para poder publicarlos. As√≠ mismo, podemos pensar que algunas secciones de Twitter operan de una manera similar a un foro de comentarios de un sitio de noticias. Este dominio (comentarios sobre art√≠culos period√≠sticos) tiene una naturaleza particular ya que las agresiones discriminatorias son usualmente a personajes p√∫blicos o colectivos de personas, y se dan de manera indirecta (a trav√©s del comentario en la noticia) y no directa (es decir, como respuesta al usuario de Twitter ofendido).

El trabajo realizado en este cap√≠tulo tuvo lugar en el contexto de un Proyecto Interdisciplinario de la UBA\footnote{\url{https://cyt.rec.uba.ar/vinculacion-transferencia/piuba/}} junto a soci√≥logos, abogados, ling√ºistas, y comput√≥logos. Particularmente, el trabajo de la construcci√≥n del manual de etiquetado fue discutido en conjunto, contemplando varias perspectivas a la hora de armar una definici√≥n propia (algunas de estas ya fueron vertidas en la discusi√≥n en la Secci√≥n \ref{sec:hate_speech_definitions}). Teniendo en cuenta que muchos trabajos del √°rea de detecci√≥n de discurso de odio mediante t√©cnicas de NLP no se realizan desde una mirada interdisciplinaria, es un aspecto a remarcar de la construcci√≥n de este recurso.

\section{Trabajo previo}
\label{sec:dataset_previous}

Pocos trabajos del √°rea de detecci√≥n de lenguaje abusivo o discurso de odio incorporan alg√∫n tipo de contexto a los comentarios recolectados para estas tareas. En esta secci√≥n haremos una revisi√≥n de los trabajos que han abordado la construcci√≥n de recursos que contengan alg√∫n tipo de informaci√≥n contextual. \citet{gao-huang-2017-detecting} construyeron un conjunto de datos de lenguaje discriminatorio sobre 1518 comentarios del sitio de Fox News, siendo estos anotados por etiquetadores que observaron tanto el comentario como el titular de la noticia en conjunto. Sobre estos datos, los autores efectuaron experimentos de clasificaci√≥n usando regresiones log√≠sticas y redes neuronales. En estos experimentos, observaron que un clasificador (tanto lineal como neuronal) mejora su performance al consumir el t√≠tulo de la noticia, dando indicios de que se puede aprovechar el contexto para mejorar la detecci√≥n de este fen√≥meno. Sin embargo, como marcan \citet{pavlopoulos2020toxicity}, este trabajo cuenta con algunos problemas: en primer lugar, el tama√±o del dataset es peque√±o, y est√° extra√≠do de s√≥lo 10 noticias, lo cual limita fuertemente los posibles contextos de los comentarios. A su vez, la anotaci√≥n fue realizada mayormente por una √∫nica persona, lo cual hace poco confiables las etiquetas obtenidas. Finalmente, algunos detalles menores debieran ser analizados con mayor detalle, como por ejemplo la utilizaci√≥n de los nombres de usuarios como variables predictivas.

\citet{mubarak-etal-2017-abusive} recolectaron comentarios en √°rabe con contenido abusivo del portal Al Jazeera para distintos art√≠culos period√≠sticos. Sin embargo, este dataset tiene un problema: los comentarios son presentados a los anotadores sobre noticias, ignorando todo el thread de la conversaci√≥n. Esto hace que el contexto sea presentado de manera parcial.

Paralelamente a nuestro trabajo, \citet{pavlopoulos2020toxicity} analizaron el impacto de agregar contexto a la tarea de detecci√≥n de toxicidad. En particular, plantearon dos preguntas:

\begin{itemize}
    \item ¬øQu√© tanto afecta el contexto a la toxicidad percibida por humanos en conversaciones online?
    \item ¬øPuede el contexto ayudar a mejorar la performance de clasificadores de toxicidad en comentarios?
\end{itemize}

Para responder estos dos puntos, los autores construyeron dos datasets en base a Wikipedia Talk Pages \cite{hua-etal-2018-wikiconv}, un conjunto de datos de discusiones del sitio de Wikipedia. En primer lugar, armaron un peque√±o conjunto de 250 comentarios anotados por dos grupos disjuntos de anotadores: uno de los grupos anot√≥ los comentarios de manera contextualizada, viendo tanto el comentario en cuesti√≥n como el t√≠tulo de la discusi√≥n; el otro grupo s√≥lo vio el comentario a anotar sin contexto alguno. En dicho experimento observaron que los anotadores que observaron el contexto percibieron 6.4\% de comentarios t√≥xicos versus un 4.4\% de quienes anotaron sin contexto, una diferencia significativa aplicando un test Mann-Whitney U. Desagregando estos resultados, observaron que 13 de los 250 comentarios (5.2\%) tuvieron diferencias de anotaci√≥n entre los dos grupos, con 9 (3.6\%) comentarios donde aument√≥ la toxicidad percibida y 4 comentarios donde baj√≥ la toxicidad al ser agregado el contexto.

Para responder la segunda pregunta, anotaron $20$ mil comentarios del mencionado foro, la mitad anotados por un grupo que etiquet√≥ viendo el contexto y la otra que no lo vio. Entre todos los comentarios recolectados, eligieron aquellos con profundidad entre dos (respuestas directas) a cinco, y que fuesen entre 10 y 400 caracteres de largo. Luego, entrenaron varios clasificadores con t√©cnicas del estado del arte, sobre los cuales pudieron observar que el contexto no pareciera mejorar significativamente la performance en la detecci√≥n de toxicidad en comentarios. En el pr√≥ximo cap√≠tulo nos extenderemos sobre las t√©cnicas utilizadas por este trabajo.

\citet{xenos-2021-context} continuaron el trabajo de \citet{pavlopoulos2020toxicity} desagregando el resultado de la segunda pregunta. Puntualmente, y observando que s√≥lo un porcentaje peque√±o de los comentarios parecen ser incididos por el contexto en el trabajo anterior, construyeron una nueva tarea: estimaci√≥n de sensibilidad al contexto. Para ello, y usando como base el conjunto de datos de Civil Comments \cite{borkan2019civil}, reanotan un subconjunto de sus comentarios usando informaci√≥n de contexto a trav√©s de crowdsourcing, y usando etiquetas de toxicidad en un estilo similar a una regresi√≥n ordinal: no t√≥xico, incierto, t√≥xico, y muy t√≥xico. Sobre las anotaciones originales (que fueron hechas sin contexto) y las nuevas anotaciones, definieron para cada comentario una sensibilidad al contexto, dada por:

\begin{equation}
    \delta(p) = s^{oc}(p) - s^{ic}(p)
\end{equation}

\noindent donde $s^{oc}$ es la fracci√≥n de anotadores sin contexto que marcaron toxicidad, y $s^{ic}$ los que no tienen contexto. En el siguiente cap√≠tulo haremos un repaso de los experimentos de clasificaci√≥n obtenidos en este trabajo.

\citet{sheth2021defining}, en un trabajo muy reciente, se√±alaron algunas oportunidades y desaf√≠os para incorporar fuentes de informaci√≥n m√°s ricas a la tarea de detecci√≥n de toxicidad. Por ejemplo, incorporar informaci√≥n como el background socio-cultural de los interactores puede ayudar a distinguir algunos tipos de reapropiaci√≥n de t√©rminos potencialmente catalogados como t√≥xicos -- por ejemplo, personas afroamericanas interpel√°ndose con t√©rminos racistas entre s√≠. As√≠ mismo, el historial de interacci√≥n entre los usuarios puede ayudar a distinguir interacciones abusivas de charlas amistosas entre amigos que usan vocabulario potencialmente t√≥xico. Finalmente, se promueve el uso de contenido externo para acercarse lo m√°s posible al conocimiento humano a trav√©s de conocimiento del contenido, el individuo (atacado) y la comunidad. Para ello, se promueve el uso de bases de conocimiento y knowledge-infusion learning \cite{gaur2020infusion} para combinar c√≥mputo neuronal sobre datos no estructurados y estructurados.



\citet{wiegand2021implicitly} mencionan formas impl√≠citas de abuso, mucho m√°s complejas que las basadas solamente en palabras ofensivas. Por ejemplo, deshumanizaciones (``los jud√≠os son una plaga que merece ser eliminada''), llamadas a la acci√≥n (``hay que tirar una bomba en ese pa√≠s''), acusaciones (``los chinos inventaron el coronavirus''), entre otros tipos sutiles de comportamiento t√≥xico. Tambi√©n menciona que la mayor√≠a de los datasets no consiguen capturar estos fen√≥menos debido a la forma de recolecci√≥n usualmente basada en keywords.

\citet{sap2020social} plantean un esquema bastante m√°s complejo dentro de la detecci√≥n de toxicidad o lenguaje abusivo. El conjunto de datos presentado en ese trabajo consta de comentarios recolectados de diversas redes sociales que son analizados con un formalismo al que denominan \emph{Social Bias Frames}. Cada instancia est√° etiquetada jer√°rquicamente de acuerdo a: toxicidad, intencionalidad, obscenidad, si est√° dirigido a un grupo, a qu√© grupo, qu√© implicancia tiene (\emph{``los XXX son todos YYY''}), y si el emisor es perteneciente al mismo grupo social que est√° siendo en teor√≠a atacado.

\section{Esquema del conjunto de datos}


%%
%%
%% Link a Draw
%% https://docs.google.com/drawings/d/1IcBITgNJN-tehmvnZqcSF9cUuWIpNKJg6yHI5yjNF9c/edit
%%
%%

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{img/idea_dataset.pdf}
    \caption{Boceto del conjunto de datos: art√≠culos period√≠sticos y sus respectivos comentarios en Twitter}
    \label{fig:idea_dataset}
\end{figure}

Para construir un conjunto de datos contextualizado analizamos algunas alternativas. Como vimos en otros trabajos, se puede entender el contexto de un mensaje de varias maneras: un contexto tem√°tico, donde sabemos que cierto comentario habla sobre un tema en particular; y un contexto conversacional, donde tenemos una secuencia de comentarios (un hilo o thread) y podemos extraer un comentario padre para cada uno salvo el ra√≠z. La primera opci√≥n es la explorada por \citet{gao-huang-2017-detecting,mubarak-etal-2017-abusive}, donde recolectan comentarios de Fox News y Al-Jazeera respectivamente. El contexto conversacional, como hemos relatado anteriormente, es explorado en \citet{pavlopoulos2020toxicity} y \citet{xenos-2021-context}; sin embargo, como es marcado en el primer trabajo, la recolecci√≥n de datos es no trivial, a√∫n en un caso m√°s amplio como el lenguaje abusivo, ya que la incidencia de comentarios de esta √≠ndole es relativamente baja. Es esperable que la tasa de ocurrencia de contenido discriminatorio sea a√∫n menor, dificultando la recolecci√≥n de datos interesantes para nuestro estudio.

Para analizar el impacto del contexto en la tarea de detecci√≥n de discurso de odio, decidimos entonces ir por la primera opci√≥n: comentarios sobre notas period√≠sticas. No vamos a considerar un hilo de respuestas (contexto conversacional) sino simplemente aquellos comentarios que sean directos sobre el art√≠culo period√≠stico. En ese punto, la idea ser√≠a similar a la de \citet{gao-huang-2017-detecting}, aunque una diferencia respecto a este conjunto de datos es la de incorporar dos modos de contexto: uno corto, donde s√≥lo tengamos el t√≠tulo de la noticia; y uno largo, donde tengamos el texto completo del art√≠culo.

Algo no menor a la hora de considerar la construcci√≥n del dataset es la posibilidad de publicar los datos. Por citar un ejemplo, el conjunto de datos recolectado por \citet{gao-huang-2017-detecting} es de libre acceso \footnote{\url{https://github.com/sjtuprog/fox-news-comments}} pero no queda claro que los t√©rminos y condiciones de la fuente permita esto. M√°s a√∫n, si hubi√©semos querido extraerlo de m√∫ltiples fuentes (por ejemplo, varios diarios), deber√≠amos chequear y/o acceder a permisos para cada sitio, a la vez que tendr√≠amos el problema de tener fuentes diversas de los datos: diferentes longitudes, formatos, metadatos, entre otros.

Para evitar muchos de estos inconvenientes y poder reutilizar parte del trabajo desarrollado en esta tesis, decidimos recolectar comentarios en Twitter. Concretamente, decidimos recolectar respuestas de usuarios a posteos hechos por cuentas de medios. De alguna manera, esto emula un foro de comentarios de medios, teniendo la ventaja de un formato √∫nico para comentarios y un acceso a una audiencia de usuarios mucho m√°s amplia que la de los microforos de cada sitio de noticias. La Figura \ref{fig:idea_dataset} ilustra un boceto de lo que queremos recolectar. A su vez, una ventaja de Twitter es que posee t√©rminos y condiciones de Twitter amigables para publicar los datos con fines de investigaci√≥n. Las notas period√≠sticas fueron tambi√©n descargadas pero debido a restricciones no tenemos a√∫n en claro si podr√°n ser publicadas.

Finalmente, la elecci√≥n del idioma. El conjunto de datos construido consta de comentarios realizados en idioma espa√±ol, m√°s precisamente en la variedad dialectal del R√≠o de la Plata (espa√±ol rioplatense). Una primera consideraci√≥n al respecto de esto es la de generar recursos por fuera del ingl√©s, un eje planteado para esta tesis. Por otro lado, tambi√©n es importante se√±alar que el discurso de odio es un fen√≥meno cultural, y es importante que quienes est√©n a cargo de la construcci√≥n de este recurso sean conscientes del trasfondo socioling√º√≠stico donde est√°n situados los discursos discriminatorios. Es por eso que a lo largo de este cap√≠tulo tuvimos particular cuidado en esta dimensi√≥n, tanto desde el proceso de recolecci√≥n hasta la selecci√≥n de los anotadores que est√©n inmersos en la realidad cultural local.


\section{Proceso de construcci√≥n}

Dividiremos la construcci√≥n del dataset en tres etapas:

\begin{enumerate}
    \item Recolecci√≥n: Proceso de recolecci√≥n de datos de Twitter y de los art√≠culos period√≠sticos
    \item Selecci√≥n: Proceso de selecci√≥n del conjunto de art√≠culos y comentarios recolectados a etiquetar
    \item Anotaci√≥n: Proceso de etiquetado de los art√≠culos seleccionados
\end{enumerate}

Si bien en muchos casos las dos primeras etapas suelen ser la misma o bien la selecci√≥n se limita a una muestra aleatoria de la recolecci√≥n, este procedimiento ser√≠a muy ineficiente para nuestro estudio. Esto se debe a que en el dominio de comentarios period√≠sticos y discurso de odio, encontramos este tipo de discurso distribuido de manera muy poco uniforme, usualmente concentrado alrededor de ciertos t√≥picos disparadores. Para poder recolectar datos con una proporci√≥n razonable del fen√≥meno estudiado, evaluamos algunas posibilidades de selecci√≥n de los art√≠culos y sus respectivos comentarios.

En algunos trabajos previos, la recolecci√≥n y selecci√≥n constan conjuntamente de la b√∫squeda en base a ciertas palabras clave, que son utilizadas para recolectar tweets o bien para preseleccionar usuarios productores de discurso de odio \cite{waseem2016hateful,hateval2019semeval}. En nuestro caso, la selecci√≥n de art√≠culos y comentarios presenta cierta novedad y complejidad, con lo cual separamos este procedimiento para explicarlo detalladamente en las siguientes secciones.

% COLECCI√ìN
\input{src/05_sec_coleccion.tex}
% SELECCI√ìN
\input{src/05_sec_selection.tex}
% ANOTACI√ìN
\input{src/05_sec_anotacion.tex}

\section{Resultados}

\begin{table}
    \centering
    % \begin{tabular}{lrr}
    %     \toprule
    %     Total articles & 1238    \\
    %     Total comments &  56869  \\
    %     Hateful Tweets &   8715  \\
    %     Ratio          &   0.153 \\
    % \end{tabular}
    \begin{tabular}{lrr}
        \toprule
        Caracter√≠stica &  N√∫mero &  Llamadas a acci√≥n \\
        \midrule
        RACISMO        &   \num{2469} & \num{ 674} \\
        APARIENCIA     &   \num{1803} & \num{  34} \\
        CRIMINAL       &   \num{1642} & \num{ 722} \\
        POLITICA       &   \num{1428} & \num{ 136} \\
        MUJER          &   \num{1332} & \num{  18} \\
        CLASE          &   \num{ 823} & \num{ 135} \\
        LGBTI          &   \num{ 818} & \num{  11} \\
        DISCAPACIDAD   &   \num{ 580} & \num{   4} \\
        TOTAL          &   \num{8715} & \num{1451} \\
        \bottomrule
    \end{tabular}
    \caption{Datos desagregados por caracter√≠stica de los comentarios discriminatorios del conjunto de datos resultante. Se listan adem√°s la cantidad de llamados a la acci√≥n dentro de cada una. Notar que el total no corresponde con la suma de las columnas ya que un mismo comentario puede estar asignado a m√°s de una caracter√≠stica.}
    \label{tab:dataset_figures}

\end{table}

El conjunto resultante consta de \num{1238} art√≠culos etiquetados, y \num{56869} comentarios respectivamente, de los cuales \num{8715} contienen contenido discriminatorio seg√∫n los criterios de asignaci√≥n antes referidos. Aproximadamente 1 de cada 6 comentarios es discriminatorio, aunque vale aclarar que esto no es representativo del universo de notas period√≠sticas ya que la selecci√≥n de los datos no fue aleatoria.

\todo{cambiar todos los n√∫meros al comando num}

La Tabla \ref{tab:dataset_figures} contiene los n√∫meros de los comentarios anotados y desagregados por las distintas caracter√≠sticas consideradas y los llamados a la acci√≥n. La categor√≠a con m√°s comentarios es RACISMO, seguido por APARIENCIA y CRIMINAL. Dentro de los tweets que llaman a alg√∫n tipo de acci√≥n, se coloca en primer lugar los dirigidos hacia la categor√≠a CRIMINAL, muchos en la forma de llamados a matar a criminales y delincuentes. La categor√≠a RACISMO acapara tambi√©n muchos llamados a la acci√≥n, mayormente contra poblaci√≥n china a la que se culpa de la pandemia del COVID-19 y conteniendo llamados a tomar distintas sanciones contra sus integrantes.


\begin{table}
    \centering
    \begin{tabular}{lc}
        \toprule
        Categor√≠a   & $\alpha$  \\
        \midrule
        Discurso de odio     &  \num{0.58} \\
        Llamados a la acci√≥n &  \num{0.64} \\
        \hline
        MUJER                &  \num{0.78} \\
        LGBTI                &  \num{0.92} \\
        RACISMO              &  \num{0.93} \\
        CLASE                &  \num{0.71} \\
        POLITICA             &  \num{0.81} \\
        DISCAPACIDAD         &  \num{0.85} \\
        APARIENCIA           &  \num{0.87} \\
        CRIMINAL             &  \num{0.93} \\
        \bottomrule
    \end{tabular}
    \caption{Tabla de acuerdo para la etiqueta de discurso de odio y diferentes caracter√≠sticas medido por $\alpha$ de Krippendorff. El acuerdo sobre \emph{discurso de odio} es reportado sobre todos los etiquetadores que hayan analizado cada comentario. Para el resto de las caracter√≠sticas, el acuerdo es calculado s√≥lo sobre aquellas anotaciones que marcaron discurso de odio.}
    \label{tab:annotation_agreement}
\end{table}

La Tabla \ref{tab:annotation_agreement} reporta el acuerdo entre anotadores usando la m√©trica \emph{alfa de Krippendorff} \cite{krippendorff2018content}. Esta m√©trica mide el acuerdo entre diversos anotadores, donde $1$ es acuerdo total, y 0 o valores negativos indican ning√∫n tipo de acuerdo. Puede entenderse como una generalizaci√≥n de la m√©trica \emph{kappa de Fleiss} para el caso en que los anotadores no etiqueten todas las instancias. Utilizamos para su c√°lculo la implementaci√≥n en Python de la librer√≠a \emph{krippendorff} \footnote{\url{https://github.com/pln-fing-udelar/fast-krippendorff}}.

Reportamos en primer lugar el acuerdo para HS sobre todas las etiquetas. En el caso de las etiquetas del segundo nivel del modelo jer√°rquico (caracter√≠sticas y llamado a la acci√≥n) calculamos el acuerdo s√≥lo sobre aquellas que hayan marcado que el comentario contiene discurso de odio. Esto es equivalente en t√©rminos del c√°lculo propuesto en \citet{krippendorff2018content} a calcular el acuerdo con una etiqueta faltante en el segundo nivel para aquellos anotadores que hayan marcado que no hay HS.

Si bien el acuerdo sobre cada caracter√≠stica tiende a ser alto, debe leerse como el acuerdo sobre la raz√≥n detr√°s del discurso de odio. La mayor penalizaci√≥n queda reservada a la etiqueta de discurso de odio que tiene $\alpha = 0.58$, algo que podr√≠a marcarse como un acuerdo razonable teniendo en cuenta valores observados en la literatura \cite{poletto2021resources}.



\subsection{Co-ocurrencia de caracter√≠sticas ofendidas}
%%
%%
%% Generar con
%% https://docs.google.com/drawings/d/1IcBITgNJN-tehmvnZqcSF9cUuWIpNKJg6yHI5yjNF9c/edit
%%
%%

\begin{figure}[t]
    \centering
    \begin{subfigure}[]{0.49\textwidth}
        \includegraphics[width=\textwidth]{img/05/heatmap_characteristics.pdf}
        \caption{Co-ocurrencia de las caracter√≠sticas ofendidas en un comentario}
        \label{subfig:heatmap_characteristics_comment}
    \end{subfigure}
    \begin{subfigure}[]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/05/heatmap_characteristics_article.pdf}
        \caption{Co-ocurrencia de las caracter√≠sticas ofendidas en un art√≠culo}
        \label{subfig:heatmap_characteristics_article}
    \end{subfigure}

    \caption{Matrices de co-ocurrencias de caracter√≠sticas ofendidas. La figura \ref{subfig:heatmap_characteristics_comment} muestra la co-ocurrencia dentro de un mismo comentario, y la figura \ref{subfig:heatmap_characteristics_article} muestra la co-ocurrencia dentro de los comentarios de un mismo art√≠culo. M√°s luminoso indica m√°s co-ocurrencia}
    \label{fig:heatmap_characteristics}
\end{figure}


De los \num{8715} comentarios odiosos, el 77\% de ellos (\num{6777}) contiene una sola caracter√≠stica ofendida de acuerdo al proceso de asignaci√≥n realizado. Cerca del 20\% tienen dos caracter√≠sticas ofendidas, y 220 comentarios tienen tres o m√°s. La Figura \ref{fig:heatmap_characteristics} ilustra la matriz de co-ocurrencia entre las distintas caracter√≠sticas para aquellos comentarios que tengan m√°s de una caracter√≠stica ofendida. En ella podemos ver que la m√°xima co-ocurrencia se da entre las caracter√≠sticas MUJER y APARIENCIA, seguidos por RACISMO y CLASE, POLITICA y CLASE, y RACISMO y POLITICA.



Otra forma de analizar la co-ocurrencia es agrupando por art√≠culos las distintas caracter√≠sticas de sus comentarios, para as√≠ observar como un mismo contexto puede suscitar distintos tipos discriminaci√≥n. La Figura \ref{subfig:heatmap_characteristics_article} ilustra las interacciones entre las distintas caracter√≠sticas por art√≠culo. Puede observarse en esta figura mayor dispersi√≥n en las co-ocurrencias que en la Figura \ref{subfig:heatmap_characteristics_comment}, apreci√°ndose algunas interacciones adicionales como por ejemplo entre RACISMO y POLITICA y --quiz√°s inesperadamente-- entre APARIENCIA y POLITICA. La caracter√≠stica que parece tener menos interacci√≥n con las dem√°s es LGBTI, indicando que est√° bien delimitada de las dem√°s posibles causas de discurso discriminatorio.

\begin{table}[t]
    \small
    \begin{tabular}{l p{0.3\textwidth} p{0.5\textwidth}}
        \toprule
        √çndice & Contexto        & Comentario \\
        \midrule
        \rule{0pt}{3ex}1 & Ofelia Fern√°ndez apoy√≥ al Gobierno en la pol√©mica por los presos y apunt√≥ a la Justicia que ``odia a las mujeres''  & Hijadept,, ojala pronto recibas la visita de alguno de esos gusanos. Te van a quedar. Ganas de apoyar al. Gobierno? Larva rastrera gorda. Decerebrada\\%& MUJER, POLITICA, APARIENCIA, DISCAPACIDAD \\
        \rule{0pt}{3ex}2 & ``Es hora de ponerle l√≠mites al odio'' | Por Victoria Donda &  Justo √©sta zurda mugrienta, ignorante y altanera... \\% & MUJER, POLITICA, APARIENCIA\\
        \rule{0pt}{3ex}3 &Coronavirus en la Argentina: un video pone en evidencia la violaci√≥n de la cuarentena en la Villa 1-11-14 & Cierren esa nido de negros y napalm. Hasta reduc√≠s el crimen y el gasto p√∫blico.\\%& CLASE, RACISMO\\
        \rule{0pt}{3ex}4 & Fabiola Y√°√±ez denunci√≥ a un periodista por publicaciones agraviantes & Claro si ofendel a la que se cuelga en el ca√±o xq ahora cree ser primera dama?üòÇ hay que ser peruka para dar asco y ser basuras bigote enseguida ordena como se meti√≥ en Facebook y en todo que culpa te.emos que saque la mujer del cabarute? \\ % RACISMO, MUJER
        \rule{0pt}{3ex}5 & Los infectados en villas porte√±as crecieron un 80\% en cuatro d√≠as & Ojal√° que el virus penetre m√°s en las villas y maten a todos esos delincuentes que viven ahi, hay paraguayos narcos, bolivianos que traen la droga de bolivia, y gente de mala vida. Tambi√©n hay travas que van a trabajar de noche a palermo.\\%& RACISMO, CLASE, LGBTI  \\
        \rule{0pt}{3ex}6 & El enojo de Moria Cas√°n contra Roc√≠o Oliva: ``Mucha agua oxigenada, le qued√≥ media neurona para jugar a la pelota'' & Y la vieja Moria, mucha cirug√≠a y estiramiento. de cara que parece un travesti \\
        \rule{0pt}{3ex}7 & Ricky Martin: ``Soy un hombre latino y homosexual viviendo en los Estados Unidos, soy una amenaza'' & Rid√≠culo perdiste t√∫ rumbo das n√°useas ü§Æ famosos eternos (v√≠ctimas) üôÑü§¶‚Äç‚ôÄÔ∏è √°ndate a Puerto Rico entonces ah√≠ no ser√°s una amenaza\\ %& LGBTI, RACISMO
        \hline
    \end{tabular}
    \caption{Ejemplos con m√°s de una caracter√≠stica ofendida marcada}
    \label{tab:multi_char_examples}
\end{table}




La Tabla \ref{tab:multi_char_examples} muestra algunos ejemplos con m√°s de una caracter√≠stica ofendida. Algunos de los comentarios se encuentran en la frontera de las caracter√≠sticas, como por ejemplo, APARIENCIA y MUJER en los ejemplos 1 y 2, o CLASE y RACISMO en el ejemplo 3. Otras instancias son conjunciones de expresiones discriminatorias, como en el caso del n√∫mero 5, donde tenemos una conjunci√≥n de RACISMO, CLASE, y LGBTI; o bien en el comentario 6, de APARIENCIA y LGBTI.



\subsection{An√°lisis por caracter√≠stica}

\todo{highlight a la tabla y sus palabras ofensivas}
\input{src/dataset_ejemplos.tex}


Las tablas \ref{tab:women_and_lgbti_examples}, \ref{tab:class_racism_examples} y \ref{tab:politics_and_calls_examples} ilustran ejemplos seleccionados de comentarios discriminatorios para las distintas caracter√≠sticas estudiadas. Hacemos a continuaci√≥n un an√°lisis cualitativo y observaciones generales sobre cada categor√≠a.

En primer lugar, en la Tabla \ref{tab:women_and_lgbti_examples} podemos apreciar que la caracter√≠stica MUJER revista cierta complejidad. En particular, algunos casos son de dif√≠cil interpretraci√≥n, como las acusaciones de mentirosa a una mujer v√≠ctima de una violaci√≥n\footnote{\url{https://www.lavanguardia.com/gente/20181212/453520382646/denuncia-actor-juan-darthes-violar-thelma-fardin-argentina-patito-feo.html}}, apreciaciones a su cuerpo, entre otros comentarios mis√≥ginos.

Una categor√≠a desafiante es la de los comentarios discriminatorios contra la comunidad LGBTI. M√°s all√° de algunos insultos expl√≠citos ( \emph{trolo, trabuco, maric√≥n}, etc), hay muchas instancias que tienen un contenido dif√≠cil de descifrar. Particularmente, aquellos comentarios contra personas transg√©nero. Muchos de estos mensajes discriminatorios hacen alusiones a su genitalidad o a su cuerpo en general, de manera metaf√≥rica o ir√≥nica, puntos que se presentan como desafiantes para algoritmos de detecci√≥n autom√°tica. A su vez, es claro que es sumamente necesaria la informaci√≥n contextual para poder comprender el caracter abusivo de estos comentarios, algo que muchas veces ni siquiera queda claro del art√≠culo ya que no todos mencionan --ni tienen por qu√© hacerlo-- el g√©nero de la persona atacada.

En el caso de la categor√≠a CRIMINAL ilustrada en la Tabla \ref{tab:class_racism_examples}, se puede observar por un lado comentarios muy violentos (\emph{``bala'', ``m√°tenlos'', ``plomo''}) que necesitan el contexto para entenderse como ofensivos en los t√©rminos planteados en nuestro trabajo (si un art√≠culo fuese sobre una plaga de osos o langostas no deber√≠amos considerarlos como tal). Por otro lado, algunos mensajes enumerados son m√°s dif√≠ciles de descifrar y dependientes del contexto, como las celebraciones ante el abatimiento de un preso o criminal (``bravo'', ``felicitaciones!'') que parecen inofensivas hasta que se lee el contexto de la noticia. Algo a remarcar de este tipo de comentarios es que tienen una polaridad positiva y contenido altamente ir√≥nico, este √∫ltimo punto indescifrable s√≥lo observando el texto del tweet.

En el caso de RACISMO (la caracter√≠stica m√°s marcada del conjunto de datos) hay una fuerte cantidad de comentarios discriminatorios contra la comunidad china. Estos mensajes son compatibles con el brote racista que tuvo lugar durante la pandemia del COVID-19, algo que tuvo su replica en las redes sociales y que ha sido ya marcado por \citet{he2021racism}. Muchos de estos tweets con contenido discriminatorio incitan a la acci√≥n, algunos con llamados a tomar medidas ``blandas'', (\emph{``no ir a comprarles a los supermercados''}) y otros directamente alentando al exterminio de este pueblo.

Las caracter√≠sticas listadas en la Tabla \ref{tab:politics_and_calls_examples} (POLITICA, DISCAPACIDAD, APARIENCIA) poseen caracter√≠sticas m√°s elementales y menos desafiantes, basadas en agravios directos y expl√≠citos. A priori, uno podr√≠a pensar que son las caracter√≠sticas que menos necesidad de contexto revisten, ya que --mayormente-- su carga de odio es notoria y centrada en insultos. Algunos de los ejemplos de dicha tabla ilustran t√©cnicas de camuflaje (\emph{tafaldegaver}, falta de verga, \emph{docer}, cerdo) que dificultan su detecci√≥n.


\section{Discusi√≥n}

%En este cap√≠tulo, describimos la construcci√≥n de un dataset contextualizado de lenguaje discriminatorio. Separamos la construcci√≥n de este dataset en tres etapas: recolecci√≥n, selecci√≥n, y anotaci√≥n. Con respecto a la recolecci√≥n, esta se bas√≥ en recolectar respuestas a noticias period√≠sticas posteadas en Twitter por los principales medios de noticias de Argentina.

De las tres etapas en las que separamos la tarea de la construcci√≥n del conjunto de datos, la recolecci√≥n fue la √∫nica que no present√≥ decisiones complejas. La posterior etapa de selecci√≥n, por el contrario, nos plante√≥ algunos obst√°culos no menores teniendo en cuenta que el discurso de odio no est√° distribuido uniformemente entre los distintos art√≠culos period√≠sticos. Exploramos distintas alternativas para poder escoger art√≠culos y comentarios a etiquetar, tanto observando el texto de los art√≠culos como sus comentarios. Decidimos seleccionar los art√≠culos en base a sus respuestas potencialmente discriminatorias usando un lexic√≥n de expresiones, luego de evaluaciones subjetivas que resultaron en una mejor calidad de art√≠culos seleccionados en base a este m√©todo. Utilizamos el lexic√≥n no para marcar los comentarios a etiquetar, sino los art√≠culos: los comentarios a etiquetar fueron elegidos --ahora s√≠-- de manera aleatoria entre los art√≠culos ya seleccionados. Trabajo futuro podr√≠a explorar alternativas para esta selecci√≥n, como por ejemplo utilizar las conexiones de amistad en Twitter entre los usuarios comentaristas.

Para realizar la tarea de etiquetado, definimos un modelo de anotaci√≥n jer√°rquico y granular de acuerdo a lo discutido en la Secci√≥n \ref{sec:04_discussion}. El hecho de anotar las caracter√≠sticas --y no s√≥lo la etiqueta binaria de presencia de discurso de odio-- es algo que pocos trabajos previos han explorado. Seis etiquetadores nativos de la variedad dialectal rioplatense realizaron la tarea bajo un esquema de dos anotaciones y desempate. Como producto, obtuvimos cerca de \num{57000} comentarios repartidos en \num{1238} art√≠culos, una cantidad de tama√±o considerable en t√©rminos de comentarios aunque no tengamos par√°metro de comparaci√≥n ya que no existen muchos conjuntos de datos similares. De los comentarios, alrededor de \num{8000} comentarios tienen contenido discriminatorio, obteniendo una tasa aproximada de un comentario discriminatorio cada seis.


Un an√°lisis exploratorio de los comentarios discriminatorios muestra ejemplos complejos y ricos, algunos de ellos altamente dependientes del contexto. Finalmente, un an√°lisis de la co-ocurrencia de las caracter√≠sticas ofendidas da muestra de que el conjunto de datos anotado posee diversidad en sus instancias, con m√∫ltiples tipos de discriminaci√≥n y art√≠culos que poseen comentarios odiosos de diversa naturaleza. Podemos especular que tanto el texto (el comentario en s√≠) como el contexto (el tweet del medio period√≠stico y su art√≠culo period√≠stico) contienen informaci√≥n valiosa para poder distinguir entre las distintas categor√≠as discriminatorias.


\section{Conclusi√≥n}


En este cap√≠tulo hemos desarrollado el proceso de construcci√≥n de un conjunto de datos contextualizado de discurso de odio en redes sociales. Para ello, recolectamos respuestas de usuarios a noticias period√≠sticas posteadas en Twitter por los principales medios de noticias de Argentina. Describimos detalladamente el proceso de su construcci√≥n --tanto en la recolecci√≥n, selecci√≥n y anotaci√≥n de los datos-- haciendo eje en las distintas dificultades que fuimos encontrando y posibilidades de mejora.

Como resultado, obtuvimos m√°s de \num{8000} comentarios discriminatorios anotados de manera granular de acuerdo a las diferentes caracter√≠sticas ofendidas. Mediante evaluaciones subjetivas y an√°lisis de las co-ocurrencias de las caracter√≠sticas, podemos afirmar que este conjunto de datos posee comentarios con notable complejidad, discurso de odio expl√≠cito e impl√≠cito, y art√≠culos que suscitan distintos tipos de reacciones discriminatorias, lo cual aporta a la riqueza de los datos.

Con este conjunto de datos como insumo, pasaremos ahora a analizar un punto que discutimos en Secci√≥n \ref{sec:04_discussion}: la contextualizaci√≥n de los mensajes para la detecci√≥n de discurso de odio. Este tema ha sido poco abordado en la literatura y es por ello que pasaremos ahora a analizar el impacto de poseer esta informaci√≥n adicional.

\section{Notas}


En el Ap√©ndice \ref{app:02} se encuentra el manual de etiquetado como as√≠ informaci√≥n adicional sobre la construcci√≥n del conjunto de datos. La herramienta de etiquetado puede encontrarse en \url{https://github.com/finiteautomata/news-labelling}.
