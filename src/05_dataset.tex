\label{chap:dataset_creation}

En este cap√≠tulo describiremos la construcci√≥n de un dataset contextualizado de discurso de odio. Describiremos en detalle el proceso de recolecci√≥n, selecci√≥n y anotaci√≥n de datos.

Por lo marcado en anteriores secciones, consideramos interesante el problema de analizar el impacto del contexto en la detecci√≥n de lenguaje discriminatorio. Para citar un ejemplo de por qu√© es necesario, el mensaje ``sos un hombre'' en solitario puede parecer inofensivo; ahora, si ese mismo mensaje est√° dirigido hacia una mujer trans, su contenido es claramente discriminatorio. Para analizar esto, nos abocamos a la decisi√≥n de crear un dataset que no s√≥lo contenga un mensaje/comentario, sino que provea un contexto en el cual se da este mensaje. Un √°mbito natural para esta tarea son las notas period√≠sticas, donde disponemos de una nota y comentarios realizados sobre √©sta.

Muchos sitios de noticias disponen de sistemas embebidos de comentarios, pero vista la dificultad para la recolecci√≥n a la vez que los limitados datos provistos por estos sitios nos llevaron a buscar otro medio: Twitter. Twitter provee una sencilla API para descargar datos, a la vez que tiene t√©rminos y condiciones amigables para poder publicar estos datos. As√≠ mismo, esta red social opera de una manera similar a un foro de comentarios de un sitio de noticias. Este tipo de datos (comentarios sobre art√≠culos period√≠sticos) tiene una naturaleza particular, ya que las agresiones discriminatorias son usualmente a personajes p√∫blicos o colectivos de personas, y se dan de manera indirecta (a trav√©s del comentario en la noticia) y no directa (es decir, como respuesta al usuario de Twitter ofendido).

El trabajo realizado en este cap√≠tulo tuvo lugar en el contexto de un Proyecto Interdisciplinario de la UBA\footnote{\url{https://cyt.rec.uba.ar/vinculacion-transferencia/piuba/}} junto a soci√≥logos, abogados, ling√ºistas, y comput√≥logos. Particularmente, el trabajo de la construcci√≥n del manual de etiquetado fue discutido en conjunto, contemplando varias perspectivas a la hora de armar una definici√≥n propia (algunas de estas ya fueron vertidas en la discusi√≥n en la secci√≥n \ref{sec:hate_speech_definitions}). Teniendo en cuenta que un alto porcentaje de trabajos del √°rea de detecci√≥n de discurso de odio (y de manera m√°s importante, en la construcci√≥n de sus recursos) mediante t√©cnicas de NLP no abordan una mirada interdisciplinaria, es un aspecto a remarcar de lo realizado en la construcci√≥n de este dataset.

\section{Trabajos previos}
\label{sec:dataset_previous}

Pocos trabajos del √°rea de detecci√≥n de lenguaje abusivo o discurso de odio incorporan alg√∫n tipo de contexto a los comentarios del usuario para estas tareas. En esta secci√≥n haremos un repaso de los trabajos que han abordado esto de alguna manera. \citet{gao-huang-2017-detecting} construy√≥ un dataset de lenguaje discriminatorio sobre 1518 comentarios del sitio de Fox News. A los anotadores les fue presentado tanto el comentario como la noticia a la hora de realizar el etiquetado. Sobre este dataset, los autores efectuaron experimentos de clasificaci√≥n usando modelos lineales (regresiones log√≠sticas) y modelos neuronales. En estos experimentos, observaron que un clasificador (tanto lineal como neuronal) mejora su performance al consumir el t√≠tulo de la noticia, dando indicios de que se puede aprovechar el contexto para mejorar la detecci√≥n de este fen√≥meno. Sin embargo, como marca \citet{pavlopoulos2020toxicity} este trabajo cuenta con algunos problemas: en primer lugar, el tama√±o del dataset es peque√±o, y est√° extra√≠do de s√≥lo 10 noticias, lo cual limita fuertemente los posibles contextos. A su vez, la anotaci√≥n fue realizada mayormente por una √∫nica persona, lo cual hace poco confiables las etiquetas obtenidas. Luego, algunos detalles menores debieran ser analizados con mayor detalle, como por ejemplo la utilizaci√≥n de los nombres de usuarios como features predictivas.

\citet{mubarak-etal-2017-abusive} construy√≥ un dataset en √°rabe sobre comentarios con contenido abusivo del portal Al Jazeera. Sin embargo, este daaset tiene un problema: los comentarios son s√≥lo presentados a los anotadores sobre noticias, ignorando todo el thread de la conversaci√≥n. Esto hace que el contexto sea parcial.


Paralelamente a nuestro trabajo, \citet{pavlopoulos2020toxicity} analiza el impacto de agregar contexto a la tarea de detecci√≥n de toxicidad. En particular, plantea dos preguntas

\begin{itemize}
    \item ¬øQu√© tanto afecta el contexto a la toxicidad percibida por humanos en conversaciones online?
    \item ¬øPuede el contexto ayudar a mejorar la performance de clasificadores de toxicidad en comentarios?
\end{itemize}

Para responder estas preguntas, los autores construyeron dos datasets en base a Wikipedia Talk Pages\cite{hua-etal-2018-wikiconv}, un dataset de discusiones del sitio de Wikipedia. En primer lugar, armaron un dataset de 250 comentarios anotados por dos grupos disjuntos de anotadores: uno de los grupos anot√≥ los comentarios de manera contextualizada, viendo tanto el comentario en cuesti√≥n como el t√≠tulo de la discusi√≥n; el otro grupo s√≥lo vio el comentario a anotar sin contexto alguno. En dicho experimento observaron que los anotadores contextualizados percibieron 6.4\% de comentarios t√≥xicos versus un 4.4\% de quienes anotaron sin contexto, una diferencia significativa aplicando un test Mann-Whitney. Desagregando estos resultados, observaron que 13 de los 250 comentarios (5.2\%) tuvieron diferencias de anotaci√≥n entre los dos grupos, con 9 (3.6\%) comentarios donde aument√≥ la toxicidad percibida y 4 comentarios donde baj√≥ la toxicidad al ser agregado el contexto.

Para responder la segunda pregunta, anotaron un dataset de 20k comentarios, 10k anotados por un grupo que etiquet√≥ viendo el contexto y otros 10k que no lo vio. Entre todos los comentarios del dataset original de Wikipedia Talk Pages, eligieron aquellos con profundidad entre 2 (respuestas directas) a 5, y con entre 10 y 400 caracteres de largo. Luego, entrenaron varios clasificadores usando este dataset y all√≠ pudieron observar que el contexto no pareciera mejorar la performance. En el pr√≥ximo cap√≠tulo nos extenderemos sobre las t√©cnicas utilizadas por este trabajo.

\citet{xenos-2021-context} contin√∫a el trabajo de \citet{pavlopoulos2020toxicity} desagregando el resultado de la segunda pregunta. Puntualmente, y observando que s√≥lo un porcentaje peque√±o de los comentarios parecen ser incididos por el contexto en el trabajo previo, construyen una nueva tarea: estimaci√≥n de sensibilidad al contexto. Para ello, toman el dataset de Civil Comments\cite{borkan2019civil}, y reanotan un subconjunto de este dataset usando informaci√≥n de contexto a trav√©s de crowdsourcing. Las etiquetas de este dataset son de toxicidad en un estilo similar a una regresi√≥n ordinal, entendiendo las categor√≠as no t√≥xico, incierto, t√≥xico, y muy t√≥xico. Ahora, teniendo las anotaciones originales del dataset (que fueron hechas sin contexto) y las nuevas anotaciones, pueden definir para cada comentario una sensibilidad al contexto, dada por

\begin{equation}
    \delta(p) = s^{oc}(p) - s^{ic}(p)
\end{equation}

donde $s^{oc}$ es la fracci√≥n de anotadores sin contexto que marcaron toxicidad, y $s^{ic}$ los que no tienen contexto.

\citet{sheth2021defining}, en un trabajo muy reciente, se√±ala algunas oportunidades y desaf√≠os  para incorporar fuentes de informaci√≥n m√°s ricas a la tarea de detecci√≥n de toxicidad. Por ejemplo, incorporar informaci√≥n como el background socio-cultural de los interactores puede ayudar a distinguir algunos tipos de reapropiaci√≥n de t√©rminos potencialmente catalogados como t√≥xicos. As√≠ mismo, el historial de interacci√≥n entre los usuarios puede ayudar a distinguir interacciones abusivas de charlas amistosas entre amigos que usan vocabulario potencialmente t√≥xico. Finalmente, se promueve el uso de contenido externo para acercarse lo m√°s posible al conocimiento humano a trav√©s de conocimiento del contenido, el individuo (atacado) y la comunidad. Para ello, se promueve el uso de bases de conocimiento y knowledge-infusion learning \cite{gaur2020infusion} para combinar el c√≥mputo neuronal y simb√≥lico.



\citet{wiegand2021implicitly} menciona formas impl√≠citas de abuso, mucho m√°s complejas que las basadas solamente en palabras ofensivas. Por ejemplo, deshumanizaciones (``los jud√≠os son una plaga que merece ser eliminada''), llamadas a la acci√≥n (``hay que tirar una bomba en ese pa√≠s''), acusaciones (``los chinos inventaron el coronavirus''), entre otros tipos sutiles de comportamiento t√≥xico. As√≠ mismo, menciona que la mayor√≠a de los datasets no consiguen capturar estos fen√≥menos debido a la forma de recolecci√≥n usualmente basada en keywords.

\section{Esquema del dataset}


%%
%%
%% Link a Draw
%% https://docs.google.com/drawings/d/1IcBITgNJN-tehmvnZqcSF9cUuWIpNKJg6yHI5yjNF9c/edit
%%
%%

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{img/idea_dataset.pdf}
    \caption{Muestra de la recolecci√≥n de datos}
    \label{fig:idea_dataset}
\end{figure}

Para construir un dataset contextualizado barajamos varias opciones. Como vimos en otros datasets, se puede entender el contexto de varias maneras: un contexto ``tem√°tico'', donde sabemos que cierto comentario habla sobre un tema en particular; y un contexto conversacional, donde tenemos una secuencia de comentarios (un hilo o thread) y podemos extraer un comentario padre para cada uno salvo el ra√≠z. La primer opci√≥n es la explorada por \cite{gao-huang-2017-detecting,mubarak-etal-2017-abusive}, donde construyen un dataset de comentarios de Fox News y Al-Jazeera respectivamente. El contexto conversacional, como hemos relatado anteriormente, es explorado en \citet{pavlopoulos2020toxicity,xenos-2021-context}; sin embargo, como es marcado en el primer trabajo, la recolecci√≥n de datos es no trivial, a√∫n en un caso m√°s amplio como el lenguaje abusivo, ya que la incidencia es relativamente baja. Puede esperarse que en el contexto de lenguaje odioso se dificulte a√∫n m√°s esto.

Para analizar el contexto, decidimos entonces usar la primera opci√≥n: comentarios sobre notas period√≠sticas. No vamos a considerar un hilo de respuestas, sino simplemente aquellos comentarios que sean directos sobre la nota. En ese punto, el dataset que queremos construir ser√≠a similar al de \cite{gao-huang-2017-detecting}. Una diferencia respecto a este dataset ser√≠a la de incorporar dos modos de contexto: uno corto, donde s√≥lo tengamos el t√≠tulo de la noticia; y uno largo, donde tengamos el texto completo de la noticia.

El dataset construido ser√° sobre comentarios realizados en idioma espa√±ol, m√°s precisamente en la variedad dialectal del R√≠o de la Plata. Como dice la ``Regla de Bender''\cite{bender2011achieving}

\begin{quote}
    Do state the name of the language that is being studied, even if it's English. Acknowledging that we are working on a particular language foregrounds the possibility that the techniques may in fact be language specific. Conversely, neglecting to state that the particular data used were in, say, English, gives [a] false veneer of language-independence to the work.
\end{quote}

Este punto es importante ya que, a pesar de ser el segundo idioma en hablantes nativos (por delante del ingl√©s), los recursos suelen ser escasos y siempre a la rastra y reproducci√≥n de resultados en ingl√©s. \todo{quiz√°s esto lo mandar√≠amos a otro lado}

Algo no menor a la hora de considerar la construcci√≥n del dataset es la posibilidad de publicar los datos. Por citar un ejemplo, el dataset de \citet{gao-huang-2017-detecting}, si bien tiene sus datos de acceso p√∫blico \footnote{\url{https://github.com/sjtuprog/fox-news-comments}}, no queda claro que los t√©rminos y condiciones de la fuente de donde se extrajeron permita esto. M√°s a√∫n, si hubi√©semos querido extraerlo de m√∫ltiples fuentes (por ejemplo, varios diarios), deber√≠amos chequear y/o acceder a permisos para cada sitio, a la vez que tendr√≠amos el problema de tener fuentes diversas de los datos (diferentes longitudes, metadatos distintos, entre otras).

Para evitar muchos de estos problemas, y reutilizar muchas cuestiones con las que venimos trabajando en esta tesis, decidimos trabajar sobre comentarios hechos por usuarios en Twitter. Concretamente, sobre respuestas de comentarios de usuarios a posteos hechos por cuentas de medios. De alguna manera, esto emular√≠a un foro de comentarios de medios, tendr√≠amos un formato √∫nico para comentarios mientras tenemos diferentes ``audiencias''. La Figura \ref{fig:idea_dataset} ilustra esta idea. A su vez, los t√©rminos y condiciones de Twitter nos permiten publicar los datos \todo{Agregar alg√∫n link a esto}. Las notas period√≠sticas las descargaremos pero debido a problemas de copyright no ser√°n publicados.



\subsection{Proceso de construcci√≥n}

Dividiremos la construcci√≥n del dataset en tres etapas:

\begin{enumerate}
    \item Recolecci√≥n: Proceso de recolecci√≥n de datos de Twitter y de los art√≠culos period√≠sticos
    \item Selecci√≥n: Dado el conjunto de art√≠culos y comentarios recolectados, tomar una muestra de art√≠culos y comentarios a etiquetar
    \item Anotaci√≥n: Proceso de etiquetado de los art√≠culos seleccionados
\end{enumerate}

Si bien en muchos casos las dos primeras etapas suelen ser la misma o bien la selecci√≥n se limita a una muestra aleatoria de la recolecci√≥n, este procedimiento ser√≠a muy ineficiente en el caso de discurso de odio. Esto se debe a que en nuestro dominio de comentarios period√≠sticos y discurso de odio, encontramos este tipo de discurso distribuido de manera muy poco uniforme, usualmente concentrada alrededor de ciertos t√≥picos. Para construir un dataset con una proporci√≥n no marginal del fen√≥meno estudiado, estudiamos algunas posibilidades para seleccionar los art√≠culos y sus respectivos comentarios.

En algunos trabajos previos (como por ejemplo \citet{waseem2016hateful,hateval2019semeval}) la recolecci√≥n y selecci√≥n constan conjuntamente de usar ciertos keywords y, o bien recolectar tweets que usen esas palabras, o bien sirven para preseleccionar usuarios de los cuales luego extraer tweets para ser etiquetados.

En nuestro caso, la selecci√≥n de art√≠culos y comentarios presenta cierta novedad y complejidad, con lo cual separamos este procedimiento para explicarlo detalladamente en las siguientes secciones.

\section{Definici√≥n de discurso de odio}
\label{sec:our_hate_speech_definition}
\begin{table}[t]
    \centering
    \begin{tabularx}{\textwidth}{l X}
        Caracter√≠stica & Descripci√≥n \\
        \hline
        MUJER        & Misoginia, agresiones basadas en ser mujer  \\
        LGBTI        & Homofobia, transfobia, y ofensas a la comunidad LGBTI \\
        RACISMO      & Racismo, Xenofobia, Judeofobia, etc \\
        POBREZA      & Basado en su condici√≥n de clase \\
        POLITICA     & En base a la filiaci√≥n pol√≠tica del agredido \\
        ASPECTO      & Gordofobia, gerontofobia \\
        CRIMINAL     & Criminales, presos, y personas en conflicto con la ley \\
        DISCAPACIDAD & Discapacidades y adictos a sustancias

    \end{tabularx}
    \label{tab:caracteristicas_protegidas}
    \caption{Caracter√≠sticas protegidas consideradas en este trabajo}
\end{table}


Teniendo en cuenta la discusi√≥n realizada en la secci√≥n \ref{sec:hate_speech_definitions} realizamos nuestra propia definici√≥n de discurso de odio. Entendemos que hay discurso de odio en un texto social si √©ste contiene declaraciones de car√°cter intenso e irracional de rechazo, enemistad y aborrecimiento contra un individuo o contra un grupo, siendo estos objetivos de estas expresiones por poseer (o aparentar poseer) una caracter√≠stica protegida. Esta expresi√≥n puede manifestarse de manera expl√≠cita como insultos directos, celebraciones de cr√≠menes, incitaciones a tomar medidas contra el individuo o grupo, o tambi√©n expresiones m√°s veladas. Siempre, considerando, que no es necesario solamente un insulto o una agresi√≥n: es necesario hacer una apelaci√≥n expl√≠cita o impl√≠cita a al menos una caracter√≠stica protegida.

A diferencia de otros trabajos, nuestra definici√≥n comprende varias caracter√≠sticas, incluso algunas que est√°n en la frontera de ser ``protegidas''. Mientras en otros trabajos se centran mayormente en racismo y misoginia, aqu√≠ agregaremos homofobia y transfobia, odio de clase (``aporofobia''), por su aspecto f√≠sico, y otras. En particular, hay dos caracter√≠sticas no convencionales que tuvimos en cuenta. En primer lugar, el discurso de odio ``pol√≠tico'', que de acuerdo a XXX \todo{citation needed}, es dif√≠cil considerar como protegida ya que puede dar lugar a censuras. Por otro lado, tambi√©n consideramos el discurso de odio contra criminales, presos, y otras personas en situaci√≥n de conflicto con la ley. Si bien este punto ni siquiera es considerado como una caracter√≠stica protegida en ninguno de los trabajos mencionados en la secci√≥n \ref{sec:hate_speech_definitions}, al haber tanto contenido que incita a la violencia contra criminales en las noticias de policiales, agregamos esta caracter√≠stica. As√≠ mismo, esta caracter√≠stica puede ser de utilidad ya que nos interesa recoger incitaciones a la violencia, y este rubro es prol√≠fico en ello en las redes.

Tenemos entonces 8 caracter√≠sticas que agrupan tipos de discurso de odio: contra las mujeres; racismo y xenofobia; contra la comunidad LGBTI; odio de clase; gordofobia, gerontofobia y dem√°s odio por aspecto; por su ideolog√≠a pol√≠tica; y finalmente contra discapacitados y adictos. Las caracter√≠sticas en cuesti√≥n son listadas en la tabla \ref{tab:caracteristicas_protegidas} junto a acr√≥nimos que usaremos en el resto del cap√≠tulo.





\input{src/05_sec_coleccion.tex}
\input{src/05_sec_selection.tex}
\input{src/05_sec_anotacion.tex}

\section{Resultados}

\begin{table}
    \centering
    % \begin{tabular}{lrr}
    %     \toprule
    %     Total articles & 1238    \\
    %     Total comments &  56869  \\
    %     Hateful Tweets &   8715  \\
    %     Ratio          &   0.153 \\
    % \end{tabular}
    \begin{tabular}{lrr}
        \toprule
        Caracter√≠stica &  N√∫mero &  Llamadas a acci√≥n \\
        \midrule
        RACISMO        &   2469 &              674 \\
        APARIENCIA     &   1803 &               34 \\
        CRIMINAL       &   1642 &              722 \\
        POLITICA       &   1428 &              136 \\
        MUJER          &   1332 &               18 \\
        CLASE          &    823 &              135 \\
        LGBTI          &    818 &               11 \\
        DISCAPACIDAD   &    580 &                4 \\
        \bottomrule
    \end{tabular}
    \caption{Cantidad de comentarios odiosos del dataset resultante, segmentados por caracter√≠stica. Se listan adem√°s la cantidad de llamados a la acci√≥n dentro de los comentarios odiosos para cada caracter√≠stica}
    \label{tab:dataset_figures}

\end{table}

El dataset resultante consta de 1238 art√≠culos etiquetados, y 56869 comentarios respectivamente, de los cuales 8715 contienen contenido discriminatorio seg√∫n los criterios de asignaci√≥n antes referidos. Podemos observar que aproximadamente 1 de cada 6 comentarios es discriminatorio; esto no es representativo del universo de notas period√≠sticas ya que recordemos que la selecci√≥n de los datos no fue aleatoria. La tabla \ref{tab:dataset_figures} contiene estos datos estad√≠sticos.

De todos los tweets discriminatorios, tenemos en particular los llamados a la acci√≥n. La inmensa mayor√≠a de estos est√° dirigido hacia la categor√≠a CRIMINAL, muchos en la forma de llamados a matar a criminales y otros delincuentes.



La tabla \ref{tab:annotation_agreement} reporta el acuerdo entre anotadores usando la m√©trica alpha de Krippendorff \todo{agregar cita}. Reportamos el valor de $\alpha$ para HS sobre todas las etiquetas, y luego todas las etiquetas del segundo nivel del modelo jer√°rquico (caracter√≠sticas y llamado a la acci√≥n) s√≥lo sobre aquellas que hayan marcado que el comentario contiene HS. Esto es equivalente a calcular el acuerdo con una etiqueta faltante en el segundo nivel para las caracter√≠sticas y el llamado a la acci√≥n. Si bien este acuerdo tiende a ser alto, debe leerse como el acuerdo sobre la raz√≥n detr√°s del hate speech; la mayor penalizaci√≥n queda reservada a HS, que tiene $\alpha = 0.59$, algo que podr√≠a marcarse como un buen acuerdo teniendo en cuenta los par√°metros vistos en las tablas de preliminares. \todo{linkear esto}




\begin{table}
    \centering
    \begin{tabular}{lc}
        \toprule
        Categor√≠a   & $\alpha$ de Krippendorff \\
        \midrule
        Hateful              &  0.579 \\
        Calls to Action      &  0.641 \\
        \midrule
        WOMEN                &  0.783 \\
        LGBTI                &  0.920 \\
        RACISM               &  0.929 \\
        CLASS                &  0.706 \\
        POLITICS             &  0.808 \\
        DISABLED             &  0.849 \\
        APPEARANCE           &  0.871 \\
        CRIMINAL             &  0.931 \\
        \bottomrule
    \end{tabular}
    \caption{Reported Agreements. \emph{Hateful} agreement is reported for the binary decision of a tweet assigned as hateful or not; for the other characteristics (and the calls to action) the agreement is calculated over those tweets with two or more hateful marks}
    \label{tab:annotation_agreement}
\end{table}

\subsection{Co-ocurrencia de caracter√≠sticas ofendidas}
%%
%%
%% Generar con
%% https://docs.google.com/drawings/d/1IcBITgNJN-tehmvnZqcSF9cUuWIpNKJg6yHI5yjNF9c/edit
%%
%%

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{img/heatmap_characteristics.pdf}
    \caption{Matriz de co-ocurrencia de las caracter√≠sticas ofendidas para comentarios con dos o m√°s caracter√≠sticas marcadas. M√°s luminoso indica m√°s co-ocurrencia}
    \label{fig:heatmap_characteristics}
\end{figure}


De los 8715 comentarios odiosos, el 77\% de ellos (6777) tiene una sola caracter√≠stica ofendida marcada. Del resto, cerca del 20\% de ellos tiene 2 caracter√≠sticas ofendidas, y 220 comentarios tienen 3 o m√°s. En la figura \ref{fig:heatmap_characteristics} podemos observar la matriz de co-ocurrencia entre las distintas caracter√≠sticas para aquellos comentarios que tengan m√°s de una marcada. En ella podemos ver que la m√°xima co-ocurrencia se da entre la caracter√≠stica MUJER y APARIENCIA, seguidos por RACISMO y CLASE, POLITICA y CLASE, y RACISMO y POLITICA.


La tabla \ref{tab:multi_char_examples} muestra algunos ejemplos de comentarios con m√°s de una caracter√≠stica ofendida marcada. Podemos ver que algunos son ejemplos muy ``border'', justo en la frontera de las caracter√≠sticas (por ejemplo, APARIENCIA y MUJER), algunas tienen caracter√≠sticas que los anotadores marcaron impl√≠citamente (por ejemplo, el ataque a Milagro Sala, que es un) mientras otros son directamente una clara conjunci√≥n de ofensas a las caracter√≠sticas.


\begin{table}[t]
    \small
    \begin{tabularx}{\textwidth}{XXX}
        \toprule
        Art√≠culo        & Comentario                 & Caracter√≠sticas\\
        \midrule
        Ofelia Fern√°ndez apoy√≥ al Gobierno en la pol√©mica por los presos y apunt√≥ a la Justicia que ``odia a las mujeres''  & Hijadept,, ojala pronto recibas la visita de alguno de esos gusanos. Te van a quedar. Ganas de apoyar al. Gobierno? Larva rastrera gorda. Decerebrada & MUJER, POLITICA, APARIENCIA, DISCAPACIDAD \\
        ``Es hora de ponerle l√≠mites al odio'' | Por Victoria Donda &  Justo √©sta zurda mugrienta, ignorante y altanera... & MUJER, POLITICA, APARIENCIA\\
        Coronavirus en la Argentina: un video pone en evidencia la violaci√≥n de la cuarentena en la Villa 1-11-14 & Cierren esa nido de negros y napalm. Hasta reduc√≠s el crimen y el gasto p√∫blico. & RACISMO\\

        Fabiola Y√°√±ez denunci√≥ a un periodista por publicaciones agraviantes & Claro si ofendel a la que se cuelga en el ca√±o xq ahora cree ser primera dama?üòÇ hay que ser peruka para dar asco y ser basuras bigote enseguida ordena como se meti√≥ en Facebook y en todo que culpa te.emos que saque la mujer del cabarute? \\

        Los infectados en villas porte√±as crecieron un 80\% en cuatro d√≠as & Ojal√° que el virus penetre m√°s en las villas y maten a todos esos delincuentes que viven ahi, hay paraguayos narcos, bolivianos que traen la droga de bolivia, y gente de mala vida. Tambi√©n hay travas que van a trabajar de noche a palermo. & RACISMO, CLASE, LGBTI  \\

        Ricky Martin: ‚ÄúSoy un hombre latino y homosexual viviendo en los Estados Unidos, soy una amenaza‚Äù & Rid√≠culo perdiste t√∫ rumbo das n√°useas ü§Æ famosos eternos (v√≠ctimas) üôÑü§¶‚Äç‚ôÄÔ∏è √°ndate a Puerto Rico entonces ah√≠ no ser√°s una amenaza & LGBTI, \\

        El enojo de Moria Cas√°n contra Roc√≠o Oliva: ``Mucha agua oxigenada, le qued√≥ media neurona para jugar a la pelota'' & Y la vieja Moria, mucha cirug√≠a y estiramiento. de cara que parece un travesti
    \end{tabularx}
    \label{tab:multi_char_examples}
    \caption{Ejemplos con m√°s de una caracter√≠stica ofendida marcada}
\end{table}




\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{img/heatmap_characteristics_article.pdf}
    \caption{Matriz de co-ocurrencia de las caracter√≠sticas ofendidas entre comentarios de un mismo art√≠culo. M√°s luminoso indica m√°s co-ocurrencia}
    \label{fig:heatmap_characteristics_article}
\end{figure}



Otra forma de analizar la co-ocurrencia de comentarios es agrupando por art√≠culos, para observar como un mismo contexto puede suscitar distintos tipos de comentarios discriminatorios. La figura \ref{fig:heatmap_characteristics_article} ilustra las interacciones entre las distintas caracter√≠sticas por art√≠culo. Podemos observar que tenemos en este mapa de calor que tenemos mayor dispersi√≥n en las co-ocurrencias que reduciendo al an√°lisis a s√≥lo observar comentarios. Por mencionar algunas que no aparecen en la figura agrupada √∫nicamente por comentarios, puede verse una mayor interacci√≥n entre discurso de odio RACISMO y POLITICA, y, quiz√°s inesperadamente entre APARIENCIA y POLITICA. Las interacciones de la caracter√≠stica LGBTI se mantienen muy bajas, indicando que este tema suele estar concentrado en este tipo de ataques.

Observando estas co-ocurrencias, podemos observar que el dataset anotado posee cierta diversidad en sus instancias, con comentarios conteniendo m√∫ltiples tipos de discriminaci√≥n, y art√≠culos que poseen comentarios odiosos de diversa naturaleza. Sobre esto, podemos especular que tanto el texto (el comentario en s√≠) como el contexto (el tweet del medio period√≠stico y su art√≠culo period√≠stico) contienen informaci√≥n valiosa para poder distinguir entre las distintas categor√≠as discriminatorias. \todo{esto es pol√©mico, reformular: que varios comentarios sean discriminatorios y de caracter√≠stica distinta no implica que el contexto necesariamente ayude}




\subsection{An√°lisis por caracter√≠stica}

En la tabla XXX podemos observar algunos ejemplos seleccionados de comentarios. Algunas observaciones que pueden realizarse es que los comentarios marcados contra las mujeres tienen en algunos casos ciertas complejidades, como las acusaciones de ``mentirosa'' a una mujer que sufri√≥ una violaci√≥n (caso Thelma Fardin \todo{Agregar nota de esto}), apreciaciones a su cuerpo, entre otras cosas.

Una categor√≠a desafiante pareciera ser los comentarios discriminatorios contra la comunidad LGBTI. M√°s all√° de algunos insultos expl√≠citamente ofensivos (mediante insultos del estilo trolo, trabuco, maric√≥n, etc), hay muchos que tienen un contenido dif√≠cil de descifrar; en particular, aquellos comentarios contra personas trans. Muchos de estos mensajes hacen alusiones a su genitalidad o a su cuerpo en general, de manera metaf√≥rica o ir√≥nica, lo cual hace verdaderamente dif√≠cil su detecci√≥n. A su vez, es claro que en muchos de estos comentarios es sumamente necesaria la informaci√≥n contextual para poder comprender el caracter abusivo de estos comentarios.

En el caso de la categor√≠a CRIMINAL, se puede observar por un lado comentarios muy violentos (``bala'', ``m√°tenlos'', ``plomo'') que necesitan el contexto para entenderse como ofensivos contra esa caracter√≠stica (por ejemplo, si la nota fuese sobre una plaga de mosquitos no deber√≠amos considerarlo como ``discriminatorio''). Por otro lado, algunos comentarios son m√°s dif√≠ciles de descifrar y dependientes del contexto, como las celebraciones ante el abatimiento de un preso o criminal (``bravo'', ``felicitaciones!'') que parecen inofensivas hasta que se lee el contexto de la noticia. De hecho, a diferencia de otros comentarios, parecen tener hasta una polaridad positiva.

En el caso de racismo (la categor√≠a m√°s marcada del dataset) hay una fuerte cantidad de comentarios discriminatorios contra la comunidad china. Esto es esperable por el brote racista debido a la pandemia del COVID-19, documentado en YYYY \todo{agregar cita}. As√≠ mismo, es de las categor√≠as que m√°s llamados a la acci√≥n tiene, muchos del estilo de tirar bombas, aniquilar, etc a China o a la comunidad de dicho pa√≠s, o llamados a tomar medidas ``blandas'', como ``no ir a comprarles a los supermercados''.

Algunas de las categor√≠as tienen caracter√≠sticas m√°s elementales, como pol√≠tica, apariencia, y discapacidad. En los comentarios ilustrados. Esto es esperable ya que

Algunas de las agresiones, a su vez, usan t√©cnicas de camuflaje (``tafaldegaver'', falta de verga, ``docer''), que dificultan su detecci√≥n por las t√©cnicas actuales.

\input{src/dataset_ejemplos.tex}



\section{Conclusi√≥n}

En este cap√≠tulo, describimos la construcci√≥n de un dataset contextualizado de lenguaje discriminatorio o hate speech. Para ello, recolectamos respuestas a noticias period√≠sticas posteadas en Twitter por los principales medios de noticias de Argentina. Exploramos distintas alternativas para la selecci√≥n de art√≠culos a etiquetar, tanto observando los t√≥picos de los art√≠culos como los comentarios a este. Decidimos elegir los art√≠culos en base a sus comentarios potencialmente discriminatorios, y luego seleccionar una muestra aleatoria y acotada de comentarios.

Para realizar la tarea de etiquetado, desarrollamos nuestra propia herramienta la cual hacemos p√∫blica. Definimos un modelo de anotaci√≥n jer√°rquico y granular para la tarea, siendo relativamente novedoso el hecho de anotar las caracter√≠sticas ofendidas en cada texto social. Seis etiquetadores nativos de la variedad dialectal rioplatense realizaron la tarea de anotaci√≥n bajo un esquema de 2 anotaciones + desempate.

Como producto, obtuvimos un dataset de cerca de 57k comentarios repartidos en 1.2k art√≠culos, una cantidad de tama√±o considerable aunque no tengamos par√°metro de comparaci√≥n ya que no existen muchos datasets similares. De los 57k comentarios, alrededor de 8k comentarios tienen contenido discriminatorio (una tasa de 1 cada 6). Un an√°lisis exploratorio de los comentarios discriminatorios muestra ejemplos complejos y ricos, algunos de ellos altamente dependientes del contexto.

En el siguiente cap√≠tulo, abordaremos nuestra pregunta original: ¬øpuede el contexto ayudar a los algoritmos de clasificaci√≥n a mejorar su performance?. Para responder esto, utilizaremos este dataset especialmente dise√±ado.
