\label{chap:05_dataset_creation}

Por lo marcado en la discusión de la anterior sección, consideramos interesante el problema de analizar el impacto del contexto en la detección de lenguaje discriminatorio. Antes de proseguir, podemos preguntarnos: ¿a qué nos referimos con el término ``contexto''? La contextualización, según John Cook-Gumperz, es:

\begin{displayquote}[\citet{gumperz1992contextualization}]
    (el) uso que hacen hablantes y oyentes de señales verbales y no verbales para poder conectar lo que se dice en un momento con el conocimiento adquirido a través de la experiencia para poder mantener la participación en la conversación y entender lo que se pretende decir.
\end{displayquote}

En este sentido, cualquier señal que pueda ayudar a entender las intenciones del interlocutor en una red social es información que ayuda a situar los mensajes: desde el hilo de una conversación, la noticia a la que hace referencia, el historial de conversaciones previas entre los interactores, información sociocultural de los interlocutores, entre otras \cite{sheth2021defining}. Para poner un ejemplo de por qué es necesario disponer de información adicional al comentario analizado, el mensaje ``sos un hombre'' en solitario puede parecer inofensivo; ahora, si ese mismo mensaje está dirigido hacia una mujer trans, su sentido es claramente discriminatorio. El comentario --con claro tono agresivo-- ``hay que tirar una bomba ahí'' puede tener carácter discriminatorio si lo consideramos en el contexto de una nota que habla sobre China y el COVID-19; sin embargo, es distinto si estamos hablando de un partido de fútbol, donde el remitente de un club manifiesta su enemistad contra otro equipo.


Vimos en el anterior capítulo que muchos mensajes analizados no se entendían bien al carecer de información contextual, tanto conversacional o del tópico al que hace cuestión. En líneas generales, la mayoría de los problemas de NLP sobre textos sociales suelen plantearse sobre comentarios sin ningún otro tipo de dato de quién lo emite \todo{chequear tilde}, a quién se lo dirige, ni sobre qué tema está hablando. Para analizar esto desde el problema de la detección de discurso de odio, nos abocamos en primer lugar a la tarea de crear un conjunto de datos que no sólo contenga un mensaje/comentario, sino que provea un contexto para éste. Un ámbito natural para esta tarea son las notas periodísticas, donde disponemos de un artículo y comentarios realizados sobre la nota. En este escenario, el comentario es el texto a analizar, mientras que el contexto está dado por la nota.

Muchos sitios de noticias disponen de sistemas embebidos de comentarios, pero vista la dificultad para la recolección y los limitados datos provistos por estos sitios acerca de sus usuarios nos llevaron a buscar otro medio: Twitter. Esta red social provee una sencilla API para descargar datos, a la vez que tiene términos y condiciones amigables para poder publicarlos. Así mismo, podemos pensar que algunas secciones de Twitter operan de una manera similar a un foro de comentarios de un sitio de noticias. Este dominio (comentarios sobre artículos periodísticos) tiene una naturaleza particular ya que las agresiones discriminatorias son usualmente a personajes públicos o colectivos de personas, y se dan de manera indirecta (a través del comentario en la noticia) y no directa (es decir, como respuesta al usuario de Twitter ofendido).

El trabajo realizado en este capítulo tuvo lugar en el contexto de un Proyecto Interdisciplinario de la UBA\footnote{\url{https://cyt.rec.uba.ar/vinculacion-transferencia/piuba/}} junto a sociólogos, abogados, lingüistas, y computólogos. Particularmente, el trabajo de la construcción del manual de etiquetado fue discutido en conjunto, contemplando varias perspectivas a la hora de armar una definición propia (algunas de estas ya fueron vertidas en la discusión en la Sección \ref{sec:hate_speech_definitions}). Teniendo en cuenta que muchos trabajos del área de detección de discurso de odio mediante técnicas de NLP no se realizan desde una mirada interdisciplinaria, es un aspecto a remarcar de la construcción de este recurso.

\section{Trabajo previo}
\label{sec:dataset_previous}

Pocos trabajos del área de detección de lenguaje abusivo o discurso de odio incorporan algún tipo de contexto a los comentarios recolectados para estas tareas. En esta sección haremos una revisión de los trabajos que han abordado la construcción de recursos que contengan algún tipo de información contextual. \citet{gao-huang-2017-detecting} construyeron un conjunto de datos de lenguaje discriminatorio sobre 1518 comentarios del sitio de Fox News, siendo estos anotados por etiquetadores que observaron tanto el comentario como el titular de la noticia en conjunto. Sobre estos datos, los autores efectuaron experimentos de clasificación usando regresiones logísticas y redes neuronales. En estos experimentos, observaron que un clasificador (tanto lineal como neuronal) mejora su performance al consumir el título de la noticia, dando indicios de que se puede aprovechar el contexto para mejorar la detección de este fenómeno. Sin embargo, como marcan \citet{pavlopoulos2020toxicity}, este trabajo cuenta con algunos problemas: en primer lugar, el tamaño del dataset es pequeño, y está extraído de sólo 10 noticias, lo cual limita fuertemente los posibles contextos de los comentarios. A su vez, la anotación fue realizada mayormente por una única persona, lo cual hace poco confiables las etiquetas obtenidas. Finalmente, algunos detalles menores debieran ser analizados con mayor detalle, como por ejemplo la utilización de los nombres de usuarios como variables predictivas.

\citet{mubarak-etal-2017-abusive} recolectaron comentarios en árabe con contenido abusivo del portal Al Jazeera para distintos artículos periodísticos. Sin embargo, este dataset tiene un problema: los comentarios son presentados a los anotadores sobre noticias, ignorando todo el thread de la conversación. Esto hace que el contexto sea presentado de manera parcial.

Paralelamente a nuestro trabajo, \citet{pavlopoulos2020toxicity} analizaron el impacto de agregar contexto a la tarea de detección de toxicidad. En particular, plantearon dos preguntas:

\begin{itemize}
    \item ¿Qué tanto afecta el contexto a la toxicidad percibida por humanos en conversaciones online?
    \item ¿Puede el contexto ayudar a mejorar la performance de clasificadores de toxicidad en comentarios?
\end{itemize}

Para responder estos dos puntos, los autores construyeron dos datasets en base a Wikipedia Talk Pages \cite{hua-etal-2018-wikiconv}, un conjunto de datos de discusiones del sitio de Wikipedia. En primer lugar, armaron un pequeño conjunto de 250 comentarios anotados por dos grupos disjuntos de anotadores: uno de los grupos anotó los comentarios de manera contextualizada, viendo tanto el comentario en cuestión como el título de la discusión; el otro grupo sólo vio el comentario a anotar sin contexto alguno. En dicho experimento observaron que los anotadores que observaron el contexto percibieron 6.4\% de comentarios tóxicos versus un 4.4\% de quienes anotaron sin contexto, una diferencia significativa aplicando un test Mann-Whitney U. Desagregando estos resultados, observaron que 13 de los 250 comentarios (5.2\%) tuvieron diferencias de anotación entre los dos grupos, con 9 (3.6\%) comentarios donde aumentó la toxicidad percibida y 4 comentarios donde bajó la toxicidad al ser agregado el contexto.

Para responder la segunda pregunta, anotaron $20$ mil comentarios del mencionado foro, la mitad anotados por un grupo que etiquetó viendo el contexto y la otra que no lo vio. Entre todos los comentarios recolectados, eligieron aquellos con profundidad entre dos (respuestas directas) a cinco, y que fuesen entre 10 y 400 caracteres de largo. Luego, entrenaron varios clasificadores con técnicas del estado del arte, sobre los cuales pudieron observar que el contexto no pareciera mejorar significativamente la performance en la detección de toxicidad en comentarios. En el próximo capítulo nos extenderemos sobre las técnicas utilizadas por este trabajo.

\citet{xenos-2021-context} continuaron el trabajo de \citet{pavlopoulos2020toxicity} desagregando el resultado de la segunda pregunta. Puntualmente, y observando que sólo un porcentaje pequeño de los comentarios parecen ser incididos por el contexto en el trabajo anterior, construyeron una nueva tarea: estimación de sensibilidad al contexto. Para ello, y usando como base el conjunto de datos de Civil Comments \cite{borkan2019civil}, reanotan un subconjunto de sus comentarios usando información de contexto a través de crowdsourcing, y usando etiquetas de toxicidad en un estilo similar a una regresión ordinal: no tóxico, incierto, tóxico, y muy tóxico. Sobre las anotaciones originales (que fueron hechas sin contexto) y las nuevas anotaciones, definieron para cada comentario una sensibilidad al contexto, dada por:

\begin{equation}
    \delta(p) = s^{oc}(p) - s^{ic}(p)
\end{equation}

\noindent donde $s^{oc}$ es la fracción de anotadores sin contexto que marcaron toxicidad, y $s^{ic}$ los que no tienen contexto. En el siguiente capítulo haremos un repaso de los experimentos de clasificación obtenidos en este trabajo.

\citet{sheth2021defining}, en un trabajo muy reciente, señalaron algunas oportunidades y desafíos para incorporar fuentes de información más ricas a la tarea de detección de toxicidad. Por ejemplo, incorporar información como el background socio-cultural de los interactores puede ayudar a distinguir algunos tipos de reapropiación de términos potencialmente catalogados como tóxicos -- por ejemplo, personas afroamericanas interpelándose con términos racistas entre sí. Así mismo, el historial de interacción entre los usuarios puede ayudar a distinguir interacciones abusivas de charlas amistosas entre amigos que usan vocabulario potencialmente tóxico. Finalmente, se promueve el uso de contenido externo para acercarse lo más posible al conocimiento humano a través de conocimiento del contenido, el individuo (atacado) y la comunidad. Para ello, se promueve el uso de bases de conocimiento y knowledge-infusion learning \cite{gaur2020infusion} para combinar cómputo neuronal sobre datos no estructurados y estructurados.



\citet{wiegand2021implicitly} mencionan formas implícitas de abuso, mucho más complejas que las basadas solamente en palabras ofensivas. Por ejemplo, deshumanizaciones (``los judíos son una plaga que merece ser eliminada''), llamadas a la acción (``hay que tirar una bomba en ese país''), acusaciones (``los chinos inventaron el coronavirus''), entre otros tipos sutiles de comportamiento tóxico. También menciona que la mayoría de los datasets no consiguen capturar estos fenómenos debido a la forma de recolección usualmente basada en keywords.

\citet{sap2020social} plantean un esquema bastante más complejo dentro de la detección de toxicidad o lenguaje abusivo. El conjunto de datos presentado en ese trabajo consta de comentarios recolectados de diversas redes sociales que son analizados con un formalismo al que denominan \emph{Social Bias Frames}. Cada instancia está etiquetada jerárquicamente de acuerdo a: toxicidad, intencionalidad, obscenidad, si está dirigido a un grupo, a qué grupo, qué implicancia tiene (\emph{``los XXX son todos YYY''}), y si el emisor es perteneciente al mismo grupo social que está siendo en teoría atacado.

\section{Esquema del conjunto de datos}


%%
%%
%% Link a Draw
%% https://docs.google.com/drawings/d/1IcBITgNJN-tehmvnZqcSF9cUuWIpNKJg6yHI5yjNF9c/edit
%%
%%

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{img/idea_dataset.pdf}
    \caption{Boceto del conjunto de datos: artículos periodísticos y sus respectivos comentarios en Twitter}
    \label{fig:idea_dataset}
\end{figure}

Para construir un conjunto de datos contextualizado analizamos algunas alternativas. Como vimos en otros trabajos, se puede entender el contexto de un mensaje de varias maneras: un contexto temático, donde sabemos que cierto comentario habla sobre un tema en particular; y un contexto conversacional, donde tenemos una secuencia de comentarios (un hilo o thread) y podemos extraer un comentario padre para cada uno salvo el raíz. La primera opción es la explorada por \citet{gao-huang-2017-detecting,mubarak-etal-2017-abusive}, donde recolectan comentarios de Fox News y Al-Jazeera respectivamente. El contexto conversacional, como hemos relatado anteriormente, es explorado en \citet{pavlopoulos2020toxicity} y \citet{xenos-2021-context}; sin embargo, como es marcado en el primer trabajo, la recolección de datos es no trivial, aún en un caso más amplio como el lenguaje abusivo, ya que la incidencia de comentarios de esta índole es relativamente baja. Es esperable que la tasa de ocurrencia de contenido discriminatorio sea aún menor, dificultando la recolección de datos interesantes para nuestro estudio.

Para analizar el impacto del contexto en la tarea de detección de discurso de odio, decidimos entonces ir por la primera opción: comentarios sobre notas periodísticas. No vamos a considerar un hilo de respuestas (contexto conversacional) sino simplemente aquellos comentarios que sean directos sobre el artículo periodístico. En ese punto, la idea sería similar a la de \citet{gao-huang-2017-detecting}, aunque una diferencia respecto a este conjunto de datos es la de incorporar dos modos de contexto: uno corto, donde sólo tengamos el título de la noticia; y uno largo, donde tengamos el texto completo del artículo.

Algo no menor a la hora de considerar la construcción del dataset es la posibilidad de publicar los datos. Por citar un ejemplo, el conjunto de datos recolectado por \citet{gao-huang-2017-detecting} es de libre acceso \footnote{\url{https://github.com/sjtuprog/fox-news-comments}} pero no queda claro que los términos y condiciones de la fuente permita esto. Más aún, si hubiésemos querido extraerlo de múltiples fuentes (por ejemplo, varios diarios), deberíamos chequear y/o acceder a permisos para cada sitio, a la vez que tendríamos el problema de tener fuentes diversas de los datos: diferentes longitudes, formatos, metadatos, entre otros.

Para evitar muchos de estos inconvenientes y poder reutilizar parte del trabajo desarrollado en esta tesis, decidimos recolectar comentarios en Twitter. Concretamente, decidimos recolectar respuestas de usuarios a posteos hechos por cuentas de medios. De alguna manera, esto emula un foro de comentarios de medios, teniendo la ventaja de un formato único para comentarios y un acceso a una audiencia de usuarios mucho más amplia que la de los microforos de cada sitio de noticias. La Figura \ref{fig:idea_dataset} ilustra un boceto de lo que queremos recolectar. A su vez, una ventaja de Twitter es que posee términos y condiciones de Twitter amigables para publicar los datos con fines de investigación. Las notas periodísticas fueron también descargadas pero debido a restricciones no tenemos aún en claro si podrán ser publicadas.

Finalmente, la elección del idioma. El conjunto de datos construido consta de comentarios realizados en idioma español, más precisamente en la variedad dialectal del Río de la Plata (español rioplatense). Una primera consideración al respecto de esto es la de generar recursos por fuera del inglés, un eje planteado para esta tesis. Por otro lado, también es importante señalar que el discurso de odio es un fenómeno cultural, y es importante que quienes estén a cargo de la construcción de este recurso sean conscientes del trasfondo sociolingüístico donde están situados los discursos discriminatorios. Es por eso que a lo largo de este capítulo tuvimos particular cuidado en esta dimensión, tanto desde el proceso de recolección hasta la selección de los anotadores que estén inmersos en la realidad cultural local.


\section{Proceso de construcción}

Dividiremos la construcción del dataset en tres etapas:

\begin{enumerate}
    \item Recolección: Proceso de recolección de datos de Twitter y de los artículos periodísticos
    \item Selección: Proceso de selección del conjunto de artículos y comentarios recolectados a etiquetar
    \item Anotación: Proceso de etiquetado de los artículos seleccionados
\end{enumerate}

Si bien en muchos casos las dos primeras etapas suelen ser la misma o bien la selección se limita a una muestra aleatoria de la recolección, este procedimiento sería muy ineficiente para nuestro estudio. Esto se debe a que en el dominio de comentarios periodísticos y discurso de odio, encontramos este tipo de discurso distribuido de manera muy poco uniforme, usualmente concentrado alrededor de ciertos tópicos disparadores. Para poder recolectar datos con una proporción razonable del fenómeno estudiado, evaluamos algunas posibilidades de selección de los artículos y sus respectivos comentarios.

En algunos trabajos previos, la recolección y selección constan conjuntamente de la búsqueda en base a ciertas palabras clave, que son utilizadas para recolectar tweets o bien para preseleccionar usuarios productores de discurso de odio \cite{waseem2016hateful,hateval2019semeval}. En nuestro caso, la selección de artículos y comentarios presenta cierta novedad y complejidad, con lo cual separamos este procedimiento para explicarlo detalladamente en las siguientes secciones.

% COLECCIÓN
\input{src/05_sec_coleccion.tex}
% SELECCIÓN
\input{src/05_sec_selection.tex}
% ANOTACIÓN
\input{src/05_sec_anotacion.tex}

\section{Resultados}

\begin{table}
    \centering
    % \begin{tabular}{lrr}
    %     \toprule
    %     Total articles & 1238    \\
    %     Total comments &  56869  \\
    %     Hateful Tweets &   8715  \\
    %     Ratio          &   0.153 \\
    % \end{tabular}
    \begin{tabular}{lrr}
        \toprule
        Característica &  Número &  Llamadas a acción \\
        \midrule
        RACISMO        &   \num{2469} & \num{ 674} \\
        APARIENCIA     &   \num{1803} & \num{  34} \\
        CRIMINAL       &   \num{1642} & \num{ 722} \\
        POLITICA       &   \num{1428} & \num{ 136} \\
        MUJER          &   \num{1332} & \num{  18} \\
        CLASE          &   \num{ 823} & \num{ 135} \\
        LGBTI          &   \num{ 818} & \num{  11} \\
        DISCAPACIDAD   &   \num{ 580} & \num{   4} \\
        TOTAL          &   \num{8715} & \num{1451} \\
        \bottomrule
    \end{tabular}
    \caption{Datos desagregados por característica de los comentarios discriminatorios del conjunto de datos resultante. Se listan además la cantidad de llamados a la acción dentro de cada una. Notar que el total no corresponde con la suma de las columnas ya que un mismo comentario puede estar asignado a más de una característica.}
    \label{tab:dataset_figures}

\end{table}

El conjunto resultante consta de \num{1238} artículos etiquetados, y \num{56869} comentarios respectivamente, de los cuales \num{8715} contienen contenido discriminatorio según los criterios de asignación antes referidos. Aproximadamente 1 de cada 6 comentarios es discriminatorio, aunque vale aclarar que esto no es representativo del universo de notas periodísticas ya que la selección de los datos no fue aleatoria.

\todo{cambiar todos los números al comando num}

La Tabla \ref{tab:dataset_figures} contiene los números de los comentarios anotados y desagregados por las distintas características consideradas y los llamados a la acción. La categoría con más comentarios es RACISMO, seguido por APARIENCIA y CRIMINAL. Dentro de los tweets que llaman a algún tipo de acción, se coloca en primer lugar los dirigidos hacia la categoría CRIMINAL, muchos en la forma de llamados a matar a criminales y delincuentes. La categoría RACISMO acapara también muchos llamados a la acción, mayormente contra población china a la que se culpa de la pandemia del COVID-19 y conteniendo llamados a tomar distintas sanciones contra sus integrantes.


\begin{table}
    \centering
    \begin{tabular}{lc}
        \toprule
        Categoría   & $\alpha$  \\
        \midrule
        Discurso de odio     &  \num{0.58} \\
        Llamados a la acción &  \num{0.64} \\
        \hline
        MUJER                &  \num{0.78} \\
        LGBTI                &  \num{0.92} \\
        RACISMO              &  \num{0.93} \\
        CLASE                &  \num{0.71} \\
        POLITICA             &  \num{0.81} \\
        DISCAPACIDAD         &  \num{0.85} \\
        APARIENCIA           &  \num{0.87} \\
        CRIMINAL             &  \num{0.93} \\
        \bottomrule
    \end{tabular}
    \caption{Tabla de acuerdo para la etiqueta de discurso de odio y diferentes características medido por $\alpha$ de Krippendorff. El acuerdo sobre \emph{discurso de odio} es reportado sobre todos los etiquetadores que hayan analizado cada comentario. Para el resto de las características, el acuerdo es calculado sólo sobre aquellas anotaciones que marcaron discurso de odio.}
    \label{tab:annotation_agreement}
\end{table}

La Tabla \ref{tab:annotation_agreement} reporta el acuerdo entre anotadores usando la métrica \emph{alfa de Krippendorff} \cite{krippendorff2018content}. Esta métrica mide el acuerdo entre diversos anotadores, donde $1$ es acuerdo total, y 0 o valores negativos indican ningún tipo de acuerdo. Puede entenderse como una generalización de la métrica \emph{kappa de Fleiss} para el caso en que los anotadores no etiqueten todas las instancias. Utilizamos para su cálculo la implementación en Python de la librería \emph{krippendorff} \footnote{\url{https://github.com/pln-fing-udelar/fast-krippendorff}}.

Reportamos en primer lugar el acuerdo para HS sobre todas las etiquetas. En el caso de las etiquetas del segundo nivel del modelo jerárquico (características y llamado a la acción) calculamos el acuerdo sólo sobre aquellas que hayan marcado que el comentario contiene discurso de odio. Esto es equivalente en términos del cálculo propuesto en \citet{krippendorff2018content} a calcular el acuerdo con una etiqueta faltante en el segundo nivel para aquellos anotadores que hayan marcado que no hay HS.

Si bien el acuerdo sobre cada característica tiende a ser alto, debe leerse como el acuerdo sobre la razón detrás del discurso de odio. La mayor penalización queda reservada a la etiqueta de discurso de odio que tiene $\alpha = 0.58$, algo que podría marcarse como un acuerdo razonable teniendo en cuenta valores observados en la literatura \cite{poletto2021resources}.



\subsection{Co-ocurrencia de características ofendidas}
%%
%%
%% Generar con
%% https://docs.google.com/drawings/d/1IcBITgNJN-tehmvnZqcSF9cUuWIpNKJg6yHI5yjNF9c/edit
%%
%%

\begin{figure}[t]
    \centering
    \begin{subfigure}[]{0.49\textwidth}
        \includegraphics[width=\textwidth]{img/05/heatmap_characteristics.pdf}
        \caption{Co-ocurrencia de las características ofendidas en un comentario}
        \label{subfig:heatmap_characteristics_comment}
    \end{subfigure}
    \begin{subfigure}[]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/05/heatmap_characteristics_article.pdf}
        \caption{Co-ocurrencia de las características ofendidas en un artículo}
        \label{subfig:heatmap_characteristics_article}
    \end{subfigure}

    \caption{Matrices de co-ocurrencias de características ofendidas. La figura \ref{subfig:heatmap_characteristics_comment} muestra la co-ocurrencia dentro de un mismo comentario, y la figura \ref{subfig:heatmap_characteristics_article} muestra la co-ocurrencia dentro de los comentarios de un mismo artículo. Más luminoso indica más co-ocurrencia}
    \label{fig:heatmap_characteristics}
\end{figure}


De los \num{8715} comentarios odiosos, el 77\% de ellos (\num{6777}) contiene una sola característica ofendida de acuerdo al proceso de asignación realizado. Cerca del 20\% tienen dos características ofendidas, y 220 comentarios tienen tres o más. La Figura \ref{fig:heatmap_characteristics} ilustra la matriz de co-ocurrencia entre las distintas características para aquellos comentarios que tengan más de una característica ofendida. En ella podemos ver que la máxima co-ocurrencia se da entre las características MUJER y APARIENCIA, seguidos por RACISMO y CLASE, POLITICA y CLASE, y RACISMO y POLITICA.



Otra forma de analizar la co-ocurrencia es agrupando por artículos las distintas características de sus comentarios, para así observar como un mismo contexto puede suscitar distintos tipos discriminación. La Figura \ref{subfig:heatmap_characteristics_article} ilustra las interacciones entre las distintas características por artículo. Puede observarse en esta figura mayor dispersión en las co-ocurrencias que en la Figura \ref{subfig:heatmap_characteristics_comment}, apreciándose algunas interacciones adicionales como por ejemplo entre RACISMO y POLITICA y --quizás inesperadamente-- entre APARIENCIA y POLITICA. La característica que parece tener menos interacción con las demás es LGBTI, indicando que está bien delimitada de las demás posibles causas de discurso discriminatorio.

\begin{table}[t]
    \small
    \begin{tabular}{l p{0.3\textwidth} p{0.5\textwidth}}
        \toprule
        Índice & Contexto        & Comentario \\
        \midrule
        \rule{0pt}{3ex}1 & Ofelia Fernández apoyó al Gobierno en la polémica por los presos y apuntó a la Justicia que ``odia a las mujeres''  & Hijadept,, ojala pronto recibas la visita de alguno de esos gusanos. Te van a quedar. Ganas de apoyar al. Gobierno? Larva rastrera gorda. Decerebrada\\%& MUJER, POLITICA, APARIENCIA, DISCAPACIDAD \\
        \rule{0pt}{3ex}2 & ``Es hora de ponerle límites al odio'' | Por Victoria Donda &  Justo ésta zurda mugrienta, ignorante y altanera... \\% & MUJER, POLITICA, APARIENCIA\\
        \rule{0pt}{3ex}3 &Coronavirus en la Argentina: un video pone en evidencia la violación de la cuarentena en la Villa 1-11-14 & Cierren esa nido de negros y napalm. Hasta reducís el crimen y el gasto público.\\%& CLASE, RACISMO\\
        \rule{0pt}{3ex}4 & Fabiola Yáñez denunció a un periodista por publicaciones agraviantes & Claro si ofendel a la que se cuelga en el caño xq ahora cree ser primera dama?😂 hay que ser peruka para dar asco y ser basuras bigote enseguida ordena como se metió en Facebook y en todo que culpa te.emos que saque la mujer del cabarute? \\ % RACISMO, MUJER
        \rule{0pt}{3ex}5 & Los infectados en villas porteñas crecieron un 80\% en cuatro días & Ojalá que el virus penetre más en las villas y maten a todos esos delincuentes que viven ahi, hay paraguayos narcos, bolivianos que traen la droga de bolivia, y gente de mala vida. También hay travas que van a trabajar de noche a palermo.\\%& RACISMO, CLASE, LGBTI  \\
        \rule{0pt}{3ex}6 & El enojo de Moria Casán contra Rocío Oliva: ``Mucha agua oxigenada, le quedó media neurona para jugar a la pelota'' & Y la vieja Moria, mucha cirugía y estiramiento. de cara que parece un travesti \\
        \rule{0pt}{3ex}7 & Ricky Martin: ``Soy un hombre latino y homosexual viviendo en los Estados Unidos, soy una amenaza'' & Ridículo perdiste tú rumbo das náuseas 🤮 famosos eternos (víctimas) 🙄🤦‍♀️ ándate a Puerto Rico entonces ahí no serás una amenaza\\ %& LGBTI, RACISMO
        \hline
    \end{tabular}
    \caption{Ejemplos con más de una característica ofendida marcada}
    \label{tab:multi_char_examples}
\end{table}




La Tabla \ref{tab:multi_char_examples} muestra algunos ejemplos con más de una característica ofendida. Algunos de los comentarios se encuentran en la frontera de las características, como por ejemplo, APARIENCIA y MUJER en los ejemplos 1 y 2, o CLASE y RACISMO en el ejemplo 3. Otras instancias son conjunciones de expresiones discriminatorias, como en el caso del número 5, donde tenemos una conjunción de RACISMO, CLASE, y LGBTI; o bien en el comentario 6, de APARIENCIA y LGBTI.



\subsection{Análisis por característica}

\todo{highlight a la tabla y sus palabras ofensivas}
\input{src/dataset_ejemplos.tex}


Las tablas \ref{tab:women_and_lgbti_examples}, \ref{tab:class_racism_examples} y \ref{tab:politics_and_calls_examples} ilustran ejemplos seleccionados de comentarios discriminatorios para las distintas características estudiadas. Hacemos a continuación un análisis cualitativo y observaciones generales sobre cada categoría.

En primer lugar, en la Tabla \ref{tab:women_and_lgbti_examples} podemos apreciar que la característica MUJER revista cierta complejidad. En particular, algunos casos son de difícil interpretración, como las acusaciones de mentirosa a una mujer víctima de una violación\footnote{\url{https://www.lavanguardia.com/gente/20181212/453520382646/denuncia-actor-juan-darthes-violar-thelma-fardin-argentina-patito-feo.html}}, apreciaciones a su cuerpo, entre otros comentarios misóginos.

Una categoría desafiante es la de los comentarios discriminatorios contra la comunidad LGBTI. Más allá de algunos insultos explícitos ( \emph{trolo, trabuco, maricón}, etc), hay muchas instancias que tienen un contenido difícil de descifrar. Particularmente, aquellos comentarios contra personas transgénero. Muchos de estos mensajes discriminatorios hacen alusiones a su genitalidad o a su cuerpo en general, de manera metafórica o irónica, puntos que se presentan como desafiantes para algoritmos de detección automática. A su vez, es claro que es sumamente necesaria la información contextual para poder comprender el caracter abusivo de estos comentarios, algo que muchas veces ni siquiera queda claro del artículo ya que no todos mencionan --ni tienen por qué hacerlo-- el género de la persona atacada.

En el caso de la categoría CRIMINAL ilustrada en la Tabla \ref{tab:class_racism_examples}, se puede observar por un lado comentarios muy violentos (\emph{``bala'', ``mátenlos'', ``plomo''}) que necesitan el contexto para entenderse como ofensivos en los términos planteados en nuestro trabajo (si un artículo fuese sobre una plaga de osos o langostas no deberíamos considerarlos como tal). Por otro lado, algunos mensajes enumerados son más difíciles de descifrar y dependientes del contexto, como las celebraciones ante el abatimiento de un preso o criminal (``bravo'', ``felicitaciones!'') que parecen inofensivas hasta que se lee el contexto de la noticia. Algo a remarcar de este tipo de comentarios es que tienen una polaridad positiva y contenido altamente irónico, este último punto indescifrable sólo observando el texto del tweet.

En el caso de RACISMO (la característica más marcada del conjunto de datos) hay una fuerte cantidad de comentarios discriminatorios contra la comunidad china. Estos mensajes son compatibles con el brote racista que tuvo lugar durante la pandemia del COVID-19, algo que tuvo su replica en las redes sociales y que ha sido ya marcado por \citet{he2021racism}. Muchos de estos tweets con contenido discriminatorio incitan a la acción, algunos con llamados a tomar medidas ``blandas'', (\emph{``no ir a comprarles a los supermercados''}) y otros directamente alentando al exterminio de este pueblo.

Las características listadas en la Tabla \ref{tab:politics_and_calls_examples} (POLITICA, DISCAPACIDAD, APARIENCIA) poseen características más elementales y menos desafiantes, basadas en agravios directos y explícitos. A priori, uno podría pensar que son las características que menos necesidad de contexto revisten, ya que --mayormente-- su carga de odio es notoria y centrada en insultos. Algunos de los ejemplos de dicha tabla ilustran técnicas de camuflaje (\emph{tafaldegaver}, falta de verga, \emph{docer}, cerdo) que dificultan su detección.


\section{Discusión}

%En este capítulo, describimos la construcción de un dataset contextualizado de lenguaje discriminatorio. Separamos la construcción de este dataset en tres etapas: recolección, selección, y anotación. Con respecto a la recolección, esta se basó en recolectar respuestas a noticias periodísticas posteadas en Twitter por los principales medios de noticias de Argentina.

De las tres etapas en las que separamos la tarea de la construcción del conjunto de datos, la recolección fue la única que no presentó decisiones complejas. La posterior etapa de selección, por el contrario, nos planteó algunos obstáculos no menores teniendo en cuenta que el discurso de odio no está distribuido uniformemente entre los distintos artículos periodísticos. Exploramos distintas alternativas para poder escoger artículos y comentarios a etiquetar, tanto observando el texto de los artículos como sus comentarios. Decidimos seleccionar los artículos en base a sus respuestas potencialmente discriminatorias usando un lexicón de expresiones, luego de evaluaciones subjetivas que resultaron en una mejor calidad de artículos seleccionados en base a este método. Utilizamos el lexicón no para marcar los comentarios a etiquetar, sino los artículos: los comentarios a etiquetar fueron elegidos --ahora sí-- de manera aleatoria entre los artículos ya seleccionados. Trabajo futuro podría explorar alternativas para esta selección, como por ejemplo utilizar las conexiones de amistad en Twitter entre los usuarios comentaristas.

Para realizar la tarea de etiquetado, definimos un modelo de anotación jerárquico y granular de acuerdo a lo discutido en la Sección \ref{sec:04_discussion}. El hecho de anotar las características --y no sólo la etiqueta binaria de presencia de discurso de odio-- es algo que pocos trabajos previos han explorado. Seis etiquetadores nativos de la variedad dialectal rioplatense realizaron la tarea bajo un esquema de dos anotaciones y desempate. Como producto, obtuvimos cerca de \num{57000} comentarios repartidos en \num{1238} artículos, una cantidad de tamaño considerable en términos de comentarios aunque no tengamos parámetro de comparación ya que no existen muchos conjuntos de datos similares. De los comentarios, alrededor de \num{8000} comentarios tienen contenido discriminatorio, obteniendo una tasa aproximada de un comentario discriminatorio cada seis.


Un análisis exploratorio de los comentarios discriminatorios muestra ejemplos complejos y ricos, algunos de ellos altamente dependientes del contexto. Finalmente, un análisis de la co-ocurrencia de las características ofendidas da muestra de que el conjunto de datos anotado posee diversidad en sus instancias, con múltiples tipos de discriminación y artículos que poseen comentarios odiosos de diversa naturaleza. Podemos especular que tanto el texto (el comentario en sí) como el contexto (el tweet del medio periodístico y su artículo periodístico) contienen información valiosa para poder distinguir entre las distintas categorías discriminatorias.


\section{Conclusión}


En este capítulo hemos desarrollado el proceso de construcción de un conjunto de datos contextualizado de discurso de odio en redes sociales. Para ello, recolectamos respuestas de usuarios a noticias periodísticas posteadas en Twitter por los principales medios de noticias de Argentina. Describimos detalladamente el proceso de su construcción --tanto en la recolección, selección y anotación de los datos-- haciendo eje en las distintas dificultades que fuimos encontrando y posibilidades de mejora.

Como resultado, obtuvimos más de \num{8000} comentarios discriminatorios anotados de manera granular de acuerdo a las diferentes características ofendidas. Mediante evaluaciones subjetivas y análisis de las co-ocurrencias de las características, podemos afirmar que este conjunto de datos posee comentarios con notable complejidad, discurso de odio explícito e implícito, y artículos que suscitan distintos tipos de reacciones discriminatorias, lo cual aporta a la riqueza de los datos.

Con este conjunto de datos como insumo, pasaremos ahora a analizar un punto que discutimos en Sección \ref{sec:04_discussion}: la contextualización de los mensajes para la detección de discurso de odio. Este tema ha sido poco abordado en la literatura y es por ello que pasaremos ahora a analizar el impacto de poseer esta información adicional.

\section{Notas}


En el Apéndice \ref{app:02} se encuentra el manual de etiquetado como así información adicional sobre la construcción del conjunto de datos. La herramienta de etiquetado puede encontrarse en \url{https://github.com/finiteautomata/news-labelling}.
